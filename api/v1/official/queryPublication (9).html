{"state":"101","errorCode":null,"errorMessage":null,"result":{"total":11,"totalPage":2,"pageNum":1,"pageSize":9,"data":[{"id":32,"title":"Coded
Computing for Federated Learning at the Edge","description":"Federated Learning
(FL) is an exciting new paradigm that enables training a global model from data
generated locally at the client nodes, without moving client data to a
centralized server.
","link":"https://arxiv.org/abs/2007.03273","pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1677417776322%E6%88%AA%E5%B1%8F2023-02-26%20%E4%B8%8B%E5%8D%889.21.57.png","type":3,"deleted":0,"createTime":"2023-02-09
11:48:58","updateBy":null,"updateTime":"2023-02-26
13:22:59","iconUrl":null,"conferenceYear":"ICML
'2020"},{"id":24,"title":"Partial Model Averaging in Federated Learning:
Performance Guarantees and Benefits","description":" We propose a partial model
averaging framework that mitigates the model discrepancy issue in Federated
Learning.","link":"https://arxiv.org/abs/2201.03789","pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1677337023446%E6%88%AA%E5%B1%8F2023-02-25%20%E4%B8%8B%E5%8D%8810.41.03.png","type":3,"deleted":0,"createTime":"2023-02-09
11:41:12","updateBy":null,"updateTime":"2023-02-25
14:57:06","iconUrl":null,"conferenceYear":"FL-AAAI’2022"},{"id":25,"title":"SPIDER:
Searching Personalized Neural Architecture for Federated
Learning","description":" we introduce SPIDER, an algorithmic framework that
aims to Search Personalized neural architecture for federated learning.
","link":"https://arxiv.org/abs/2112.13939","pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1677336962572%E6%88%AA%E5%B1%8F2023-02-25%20%E4%B8%8B%E5%8D%8810.41.03.png","type":3,"deleted":0,"createTime":"2023-02-09
11:42:21","updateBy":null,"updateTime":"2023-02-25
14:56:06","iconUrl":null,"conferenceYear":"Arxiv’
2022"},{"id":27,"title":"Layer-wise Adaptive Model Aggregation for Scalable
Federated Learning","description":"We propose FedLAMA, a layer-wise model
aggregation scheme for scalable Federated Learning.
","link":"https://arxiv.org/abs/2110.10302","pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1677336873044%E6%88%AA%E5%B1%8F2023-02-25%20%E4%B8%8B%E5%8D%8810.30.55.png","type":3,"deleted":0,"createTime":"2023-02-09
11:44:32","updateBy":null,"updateTime":"2023-02-25
14:54:34","iconUrl":null,"conferenceYear":"Arxiv’2022"},{"id":8,"title":"FedNAS
(neural architecture search for FL personalization)","description":"We propose a
Federated NAS (FedNAS) algorithm to help scattered workers collaboratively
searching for a better architecture with higher accuracy.
","link":"https://arxiv.org/abs/2004.08546","pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1677336702611%E6%88%AA%E5%B1%8F2023-02-25%20%E4%B8%8B%E5%8D%8810.51.28.png","type":3,"deleted":0,"createTime":"2023-02-02
03:08:22","updateBy":null,"updateTime":"2023-02-25
14:51:45","iconUrl":"https://doc.fedml.ai/_static/image/cheetah.jpeg","conferenceYear":"CVPR’20
NAS Workshop"},{"id":9,"title":"SpreadGNN: Serverless Multi-task Federated
Learning for Graph Neural Networks","description":"This work proposes SpreadGNN,
a novel multi-task federated training framework capable of operating in the
presence of partial labels and absence of a central server for the first time in
the literature.
","link":"https://arxiv.org/abs/2106.02743","pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1677335985643%E6%88%AA%E5%B1%8F2023-02-25%20%E4%B8%8B%E5%8D%8810.39.17.png","type":3,"deleted":0,"createTime":"2023-02-02
03:08:33","updateBy":null,"updateTime":"2023-02-25
14:39:57","iconUrl":"https://doc.fedml.ai/_static/image/cheetah.jpeg","conferenceYear":"AAAI’21"},{"id":29,"title":"Coded
Computing for Low-Latency Federated Learning Over Wireless Edge
Networks","description":"Federated learning enables training a global model from
data located at the client nodes, without data sharing and moving client data to
a centralized
server.","link":"https://ieeexplore.ieee.org/abstract/document/9252954","pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1677312510495%E6%88%AA%E5%B1%8F2023-02-25%20%E4%B8%8B%E5%8D%884.06.59.png","type":3,"deleted":0,"createTime":"2023-02-09
11:46:47","updateBy":null,"updateTime":"2023-02-25
08:08:33","iconUrl":null,"conferenceYear":"IEEE Journal on Selected Areas in
Communications"},{"id":33,"title":"Straggler mitigation in distributed matrix
multiplication: Fundamental limits and optimal coding","description":"We
consider the problem of massive matrix multiplication, which underlies many data
analytic applications, in a large-scale distributed system comprising a group of
worker
nodes.","link":"https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Qhe5ua0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=Qhe5ua0AAAAJ:JoZmwDi-zQgC","pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1677060388477IT%20Soc%20Microsite%20logo%20300%20x%20350.002_1%20%281%29.jpeg","type":3,"deleted":0,"createTime":"2023-02-10
08:55:02","updateBy":null,"updateTime":"2023-02-23
02:49:05","iconUrl":null,"conferenceYear":"IEEE Transactions on Information
Theory '2020"},{"id":31,"title":"Hierarchical coded gradient aggregation for
learning at the edge","description":"Client devices at the edge are generating
increasingly large amounts of rich data suitable for learning powerful
statistical models.
","link":"https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Qhe5ua0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=Qhe5ua0AAAAJ:DUooU5lO8OsC","pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1677060530617images%20%282%29.jpeg","type":3,"deleted":0,"createTime":"2023-02-09
11:48:11","updateBy":null,"updateTime":"2023-02-23
02:41:21","iconUrl":null,"conferenceYear":"ISIT 2020"}]},"fail":false}
