{"state":"101","errorCode":null,"errorMessage":null,"result":{"total":12,"totalPage":2,"pageNum":1,"pageSize":9,"data":[{"id":5,"title":"FedML:
A Research Library and Benchmark for Federated Machine
Learning","description":"we introduce FedML, an open research library and
benchmark to facilitate FL algorithm development and fair performance
comparison","link":"https://arxiv.org/abs/2007.13518","pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1677335574861%E6%88%AA%E5%B1%8F2023-02-25%20%E4%B8%8B%E5%8D%884.10.34.png","type":2,"deleted":0,"createTime":"2023-02-02
03:07:49","updateBy":null,"updateTime":"2023-03-01
04:55:47","iconUrl":"https://doc.fedml.ai/_static/image/cheetah.jpeg","conferenceYear":"NeurIPS
2020 FL Workshop, Best Paper Award"},{"id":6,"title":"PipeTransformer: Automated
Elastic Pipelining for Distributed Training of Transformers","description":"In
this paper, we propose PipeTransformer, which leverages automated elastic
pipelining for efficient distributed training of Transformer
models.","link":"http://proceedings.mlr.press/v139/he21a/he21a.pdf","pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1677418645161%E6%88%AA%E5%B1%8F2023-02-26%20%E4%B8%8B%E5%8D%889.21.57.png","type":2,"deleted":0,"createTime":"2023-02-02
03:07:59","updateBy":null,"updateTime":"2023-02-26
13:37:27","iconUrl":"https://doc.fedml.ai/_static/image/cheetah.jpeg","conferenceYear":"ICML
'2021"},{"id":20,"title":"Communication-aware scheduling of serial tasks for
dispersed computing","description":"We consider the problem of task scheduling
for such networks, in a dynamic setting in which arriving computation jobs are
modeled as chains, with nodes representing tasks, and edges representing
precedence constraints among tasks.
I","link":"https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Qhe5ua0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=Qhe5ua0AAAAJ:eq2jaN3J8jMC","pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1677418616711%E6%88%AA%E5%B1%8F2023-02-26%20%E4%B8%8B%E5%8D%889.36.27.png","type":2,"deleted":0,"createTime":"2023-02-09
11:17:21","updateBy":null,"updateTime":"2023-02-26
13:37:01","iconUrl":null,"conferenceYear":"IEEE/ACM Transactions on Networking
'2019"},{"id":4,"title":"A fundamental tradeoff between computation and
communication in distributed computing","description":"How can we optimally
trade extra computing power to reduce the communication load in distributed
computing?","link":"https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Qhe5ua0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=Qhe5ua0AAAAJ:l7t_Zn2s7bgC","pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1677335551698%E6%88%AA%E5%B1%8F2023-02-25%20%E4%B8%8B%E5%8D%8810.32.16.png","type":2,"deleted":0,"createTime":"2023-02-02
03:07:36","updateBy":null,"updateTime":"2023-02-25
14:32:34","iconUrl":"https://doc.fedml.ai/_static/image/cheetah.jpeg","conferenceYear":"IEEE
Transactions on Information Theory '2017"},{"id":12,"title":"Pipe-SGD: A
decentralized pipelined SGD framework for distributed deep net
training","description":"we carefully analyze the AllReduce based setup, propose
timing models which include network latency, bandwidth, cluster size and compute
time, and demonstrate that a pipelined training with a width of two combines the
best of both synchronous and asynchronous training.
","link":"https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Qhe5ua0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=Qhe5ua0AAAAJ:yB1At4FlUx8C","pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1677313334689%E6%88%AA%E5%B1%8F2023-02-25%20%E4%B8%8B%E5%8D%884.10.34.png","type":2,"deleted":0,"createTime":"2023-02-09
11:08:17","updateBy":null,"updateTime":"2023-02-25
08:22:18","iconUrl":null,"conferenceYear":"NeurIPS
2018"},{"id":14,"title":"Gradiveq: Vector quantization for bandwidth-efficient
gradient aggregation in distributed cnn training","description":"In this paper,
we empirically demonstrate the strong linear correlations between CNN gradients,
and propose a gradient vector quantization technique, named GradiVeQ, to exploit
these correlations through principal component analysis (PCA) for substantial
gradient dimension reduction.
","link":"https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Qhe5ua0AAAAJ&cstart=100&pagesize=100&sortby=pubdate&citation_for_view=Qhe5ua0AAAAJ:NJ774b8OgUMC","pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1677313310874%E6%88%AA%E5%B1%8F2023-02-25%20%E4%B8%8B%E5%8D%884.10.34.png","type":2,"deleted":0,"createTime":"2023-02-09
11:10:51","updateBy":null,"updateTime":"2023-02-25
08:21:58","iconUrl":null,"conferenceYear":"NeurIPS
2018"},{"id":15,"title":"MEST: Accurate and Fast Memory-Economic Sparse Training
Framework on the Edge","description":"Our results suggest that unforgettable
examples can be identified in-situ even during the dynamic exploration of
sparsity masks in the sparse training process, and therefore can be removed for
further training speedup on edge devices.
","link":"https://proceedings.neurips.cc/paper/2021/hash/ae3f4c649fb55c2ee3ef4d1abdb79ce5-Abstract.html","pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1677312988612%E6%88%AA%E5%B1%8F2023-02-25%20%E4%B8%8B%E5%8D%884.10.34.png","type":2,"deleted":0,"createTime":"2023-02-09
11:11:22","updateBy":null,"updateTime":"2023-02-25
08:16:34","iconUrl":null,"conferenceYear":"NeurIPS
2021"},{"id":16,"title":"ApproxIFER: A Model-Agnostic Approach to Resilient and
Robust Prediction Serving Systems","description":" The common approach for
tackling this problem is replication which assigns the same prediction task to
multiple workers.
","link":"https://arxiv.org/abs/2109.09868","pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1677312967483%E6%88%AA%E5%B1%8F2023-02-25%20%E4%B8%8B%E5%8D%884.10.34.png","type":2,"deleted":0,"createTime":"2023-02-09
11:12:52","updateBy":null,"updateTime":"2023-02-25
08:16:11","iconUrl":null,"conferenceYear":"NeurIPS
2021"},{"id":21,"title":"Group Knowledge Transfer: Federated Learning of Large
CNNs at the Edge","description":"Scaling up the convolutional neural network
(CNN) size (e.g., width, depth, etc.) is known to effectively improve model
accuracy. However, the large model size impedes training on resource-constrained
edge
devices.","link":"https://arxiv.org/abs/2007.14513","pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1677312657133%E6%88%AA%E5%B1%8F2023-02-25%20%E4%B8%8B%E5%8D%884.10.34.png","type":2,"deleted":0,"createTime":"2023-02-09
11:18:11","updateBy":null,"updateTime":"2023-02-25
08:10:59","iconUrl":null,"conferenceYear":"NeurIPS â€™20"}]},"fail":false}
