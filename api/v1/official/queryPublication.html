{"state":"101","errorCode":null,"errorMessage":null,"result":{"total":48,"totalPage":6,"pageNum":1,"pageSize":9,"data":[{"id":57,"title":"Proof-of-Contribution-Based
Design for Collaborative Machine Learning on Blockchain","description":"Our goal
is to design a data marketplace for decentralized collaborative/federated
learning
applications","link":"https://arxiv.org/abs/2302.14031","pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1677646653797arXiv.png","type":6,"deleted":0,"createTime":"2023-03-01
04:42:11","updateBy":null,"updateTime":"2023-03-01
04:57:35","iconUrl":null,"conferenceYear":"arXiv"},{"id":5,"title":"FedML: A
Research Library and Benchmark for Federated Machine Learning","description":"we
introduce FedML, an open research library and benchmark to facilitate FL
algorithm development and fair performance
comparison","link":"https://arxiv.org/abs/2007.13518","pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1677335574861%E6%88%AA%E5%B1%8F2023-02-25%20%E4%B8%8B%E5%8D%884.10.34.png","type":2,"deleted":0,"createTime":"2023-02-02
03:07:49","updateBy":null,"updateTime":"2023-03-01
04:55:47","iconUrl":"https://doc.fedml.ai/_static/image/cheetah.jpeg","conferenceYear":"NeurIPS
2020 FL Workshop, Best Paper Award"},{"id":6,"title":"PipeTransformer: Automated
Elastic Pipelining for Distributed Training of Transformers","description":"In
this paper, we propose PipeTransformer, which leverages automated elastic
pipelining for efficient distributed training of Transformer
models.","link":"http://proceedings.mlr.press/v139/he21a/he21a.pdf","pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1677418645161%E6%88%AA%E5%B1%8F2023-02-26%20%E4%B8%8B%E5%8D%889.21.57.png","type":2,"deleted":0,"createTime":"2023-02-02
03:07:59","updateBy":null,"updateTime":"2023-02-26
13:37:27","iconUrl":"https://doc.fedml.ai/_static/image/cheetah.jpeg","conferenceYear":"ICML
'2021"},{"id":20,"title":"Communication-aware scheduling of serial tasks for
dispersed computing","description":"We consider the problem of task scheduling
for such networks, in a dynamic setting in which arriving computation jobs are
modeled as chains, with nodes representing tasks, and edges representing
precedence constraints among tasks.
I","link":"https://scholar.google.com/citations?view_op=view_citation&hl=en&user=Qhe5ua0AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=Qhe5ua0AAAAJ:eq2jaN3J8jMC","pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1677418616711%E6%88%AA%E5%B1%8F2023-02-26%20%E4%B8%8B%E5%8D%889.36.27.png","type":2,"deleted":0,"createTime":"2023-02-09
11:17:21","updateBy":null,"updateTime":"2023-02-26
13:37:01","iconUrl":null,"conferenceYear":"IEEE/ACM Transactions on Networking
'2019"},{"id":32,"title":"Coded Computing for Federated Learning at the
Edge","description":"Federated Learning (FL) is an exciting new paradigm that
enables training a global model from data generated locally at the client nodes,
without moving client data to a centralized server.
","link":"https://arxiv.org/abs/2007.03273","pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1677417776322%E6%88%AA%E5%B1%8F2023-02-26%20%E4%B8%8B%E5%8D%889.21.57.png","type":3,"deleted":0,"createTime":"2023-02-09
11:48:58","updateBy":null,"updateTime":"2023-02-26
13:22:59","iconUrl":null,"conferenceYear":"ICML
'2020"},{"id":24,"title":"Partial Model Averaging in Federated Learning:
Performance Guarantees and Benefits","description":" We propose a partial model
averaging framework that mitigates the model discrepancy issue in Federated
Learning.","link":"https://arxiv.org/abs/2201.03789","pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1677337023446%E6%88%AA%E5%B1%8F2023-02-25%20%E4%B8%8B%E5%8D%8810.41.03.png","type":3,"deleted":0,"createTime":"2023-02-09
11:41:12","updateBy":null,"updateTime":"2023-02-25
14:57:06","iconUrl":null,"conferenceYear":"FL-AAAI’2022"},{"id":25,"title":"SPIDER:
Searching Personalized Neural Architecture for Federated
Learning","description":" we introduce SPIDER, an algorithmic framework that
aims to Search Personalized neural architecture for federated learning.
","link":"https://arxiv.org/abs/2112.13939","pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1677336962572%E6%88%AA%E5%B1%8F2023-02-25%20%E4%B8%8B%E5%8D%8810.41.03.png","type":3,"deleted":0,"createTime":"2023-02-09
11:42:21","updateBy":null,"updateTime":"2023-02-25
14:56:06","iconUrl":null,"conferenceYear":"Arxiv’
2022"},{"id":27,"title":"Layer-wise Adaptive Model Aggregation for Scalable
Federated Learning","description":"We propose FedLAMA, a layer-wise model
aggregation scheme for scalable Federated Learning.
","link":"https://arxiv.org/abs/2110.10302","pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1677336873044%E6%88%AA%E5%B1%8F2023-02-25%20%E4%B8%8B%E5%8D%8810.30.55.png","type":3,"deleted":0,"createTime":"2023-02-09
11:44:32","updateBy":null,"updateTime":"2023-02-25
14:54:34","iconUrl":null,"conferenceYear":"Arxiv’2022"},{"id":50,"title":"MiLeNAS:
Efficient Neural Architecture Search via Mixed-Level
Reformulation","description":"In this paper, we demonstrate that gradient errors
caused by such approximations lead to suboptimality, in the sense that the
optimization procedure fails to converge to a (locally) optimal solution.
","link":"https://arxiv.org/abs/2003.12238","pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1677336837690%E6%88%AA%E5%B1%8F2023-02-25%20%E4%B8%8B%E5%8D%8810.51.28.png","type":5,"deleted":0,"createTime":"2023-02-10
09:19:31","updateBy":null,"updateTime":"2023-02-25
14:53:59","iconUrl":null,"conferenceYear":"CVPR 2020"}]},"fail":false}
