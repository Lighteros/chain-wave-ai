{"message":"Succeeded to process request","code":"SUCCESS","data":{"total":46,"totalPage":3,"pageNum":3,"pageSize":16,"data":[{"id":445,"versionId":4273,"owner":"meta","applicationName":"Llama-2-13b-chat","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1704851393407Llama.webp","description":"# **Llama 2**\n\nLlama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.\n\n## Model Details\n\n*Note: Use of this model is governed by the [Meta license](https://ai.meta.com/llama/use-policy/). By employing this model, you acknowledge and consent to adhere the terms and conditions outlined in the Meta license.*\n\nMeta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.\n\n**Model Developers** Meta\n\n**Variations** Llama 2 comes in a range of parameter sizes — 7B, 13B, and 70B — as well as pretrained and fine-tuned variations.\n\n**Input** Models input text only.\n\n**Output** Models generate text only.\n\n**Model Architecture** Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety.\n\n|         | Training Data                                 | Params | Content Length | GQA | Tokens | LR                    |\n|---------|-----------------------------------------------|--------|----------------|-----|--------|-----------------------|\n| Llama 2 | *A new mix of publicly available online data* | 7B     | 4k             | N   | 2.0T   | 3.0 x 10-4 |\n| Llama 2 | *A new mix of publicly available online data* | 13B    | 4k             | N   | 2.0T   | 3.0 x 10-4 |\n| Llama 2 | *A new mix of publicly available online data* | 70B    | 4k             | Y   | 2.0T   | 1.5 x 10-4 |\n\n*Llama 2 family of models.* Token counts refer to pretraining data only. All models are trained with a global batch-size of 4M tokens. Bigger models -  70B -- use Grouped-Query Attention (GQA) for improved inference scalability.\n\n**Model Dates** Llama 2 was trained between January 2023 and July 2023.\n\n**Status** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\n\n**License** A custom commercial license is available at: [https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)\n\n**Research Paper** [\"Llama-2: Open Foundation and Fine-tuned Chat Models\"](arxiv.org/abs/2307.09288)\n\n## Intended Use\n\n**Intended Use Cases** Llama 2 is intended for commercial and research use in English. Tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.\n\nTo get the expected features and performance for the chat versions, a specific formatting needs to be followed, including the `INST` and `<<SYS>>` tags, `BOS` and `EOS` tokens, and the whitespaces and breaklines in between (we recommend calling `strip()` on inputs to avoid double-spaces). See our reference code in github for details: [`chat_completion`](https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L212).\n\n**Out-of-scope Uses** Use in any manner that violates applicable laws or regulations (including trade compliance laws).Use in languages other than English. Use in any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement for Llama 2.\n\n## Hardware and Software\n\n**Training Factors** We used custom training libraries, Meta's Research Super Cluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute.\n\n**Carbon Footprint** Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 539 tCO2eq, 100% of which were offset by Meta’s sustainability program.\n\n||Time (GPU hours)|Power Consumption (W)|Carbon Emitted(tCO2eq)|\n|---|---|---|---|\n|Llama 2 7B|184320|400|31.22|\n|Llama 2 13B|368640|400|62.44|\n|Llama 2 70B|1720320|400|291.42|\n|Total|3311616||539.00|\n\n**CO<sub>2</sub> emissions during pretraining.** Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta's sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others.\n\n## Training Data\n\n**Overview** Llama 2 was pretrained on 2 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over one million new human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.\n\n**Data Freshness** The pretraining data has a cutoff of September 2022, but some tuning data is more recent, up to July 2023.\n\n## Evaluation Results\n\nIn this section, we report the results for the Llama 1 and Llama 2 models on standard academic benchmarks.For all the evaluations, we use our internal evaluations library.\n\n|Model|Size|Code|Commonsense Reasoning|World Knowledge|Reading Comprehension|Math|MMLU|BBH|AGI Eval|\n|---|---|---|---|---|---|---|---|---|---|\n|Llama 1|7B|14.1|60.8|46.2|58.5|6.95|35.1|30.3|23.9|\n|Llama 1|13B|18.9|66.1|52.6|62.3|10.9|46.9|37.0|33.9|\n|Llama 1|33B|26.0|70.0|58.4|67.6|21.4|57.8|39.8|41.7|\n|Llama 1|65B|30.7|70.7|60.5|68.6|30.8|63.4|43.5|47.6|\n|Llama 2|7B|16.8|63.9|48.9|61.3|14.6|45.3|32.6|29.3|\n|Llama 2|13B|24.5|66.9|55.4|65.8|28.7|54.8|39.4|39.1|\n|Llama 2|70B|**37.5**|**71.9**|**63.6**|**69.4**|**35.2**|**68.9**|**51.2**|**54.2**|\n\n**Overall performance on grouped academic benchmarks.** *Code:* We report the average pass@1 scores of our models on HumanEval and MBPP. *Commonsense Reasoning:* We report the average of PIQA, SIQA, HellaSwag, WinoGrande, ARC easy and challenge, OpenBookQA, and CommonsenseQA. We report 7-shot results for CommonSenseQA and 0-shot results for all other benchmarks. *World Knowledge:* We evaluate the 5-shot performance on NaturalQuestions and TriviaQA and report the average. *Reading Comprehension:* For reading comprehension, we report the 0-shot average on SQuAD, QuAC, and BoolQ. *MATH:* We report the average of the GSM8K (8 shot) and MATH (4 shot) benchmarks at top 1.\n\n|||TruthfulQA|Toxigen|\n|---|---|---|---|\n|Llama 1|7B|27.42|23.00|\n|Llama 1|13B|41.74|23.08|\n|Llama 1|33B|44.19|22.57|\n|Llama 1|65B|48.71|21.77|\n|Llama 2|7B|33.29|**21.25**|\n|Llama 2|13B|41.86|26.10|\n|Llama 2|70B|**50.18**|24.60|\n\n**Evaluation of pretrained LLMs on automatic safety benchmarks.** For TruthfulQA, we present the percentage of generations that are both truthful and informative (the higher the better). For ToxiGen, we present the percentage of toxic generations (the smaller the better).\n\n|||TruthfulQA|Toxigen|\n|---|---|---|---|\n|Llama-2-Chat|7B|57.04|**0.00**|\n|Llama-2-Chat|13B|62.18|**0.00**|\n|Llama-2-Chat|70B|**64.14**|0.01|\n\n**Evaluation of fine-tuned LLMs on different safety datasets.** Same metric definitions as above.\n\n## Ethical Considerations and Limitations\n\nLlama 2 is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Llama 2’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 2, developers should perform safety testing and tuning tailored to their specific applications of the model.\n\nPlease see the Responsible Use Guide available at [https://ai.meta.com/llama/responsible-use-guide/](https://ai.meta.com/llama/responsible-use-guide)\n\n## Reporting Issues\n\nPlease report any software “bug,” or other problems with the models through one of the following means:\n\n- Reporting issues with the model: [github.com/facebookresearch/llama](http://github.com/facebookresearch/llama)\n- Reporting problematic content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n- Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\n\n## Llama Model Index\n\n|Model|Llama2|Llama2-hf|Llama2-chat|Llama2-chat-hf|\n|---|---|---|---|---|\n|7B| [Link](https://huggingface.co/meta-llama/Llama-2-7b) | [Link](https://huggingface.co/meta-llama/Llama-2-7b-hf) | [Link](https://huggingface.co/meta-llama/Llama-2-7b-chat) | [Link](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)|\n|13B| [Link](https://huggingface.co/meta-llama/Llama-2-13b) | [Link](https://huggingface.co/meta-llama/Llama-2-13b-hf) | [Link](https://huggingface.co/meta-llama/Llama-2-13b-chat) | [Link](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf)|\n|70B| [Link](https://huggingface.co/meta-llama/Llama-2-70b) | [Link](https://huggingface.co/meta-llama/Llama-2-70b-hf) | [Link](https://huggingface.co/meta-llama/Llama-2-70b-chat) | [Link](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf)|","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":1,"views":178,"requests":"0","sort":null,"version":"v3-Fri Mar 08 02:18:47 GMT 2024","tags":["Text2Text Generation"],"playground":false,"createTime":"2024-03-08T02:22:16","updateBy":"meta","updateTime":"2024-03-08T02:22:26"},{"id":506,"versionId":3624,"owner":"NousResearch","applicationName":"Nous-Hermes-2-Mixtral-8x7B-DPO","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1705382215804Nous-Hermes-2-Mixtral-8x7B-DPO.jpeg","description":"# Nous Hermes 2 - Mixtral 8x7B - DPO\n\n![image/jpeg](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/btRmXWMG7PXatTs-u3G85.jpeg)\n\n## Model description\n\nNous Hermes 2 Mixtral 8x7B DPO is the new flagship Nous Research model trained over the [Mixtral 8x7B MoE LLM](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1).\n\nThe model was trained on over 1,000,000 entries of primarily GPT-4 generated data, as well as other high quality data from open datasets across the AI landscape, achieving state of the art performance on a variety of tasks.\n\nThis is the SFT + DPO version of Mixtral Hermes 2, we have also released an SFT only version, for people to find which works best for them, which can be found here: https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-SFT\n\n## We are grateful to Together.ai for sponsoring our compute during the many experiments both training Mixtral and working on DPO!\n\n# Table of Contents\n\n1. [Example Outputs](#example-outputs)\n2. [Benchmark Results](#benchmark-results)\n   - GPT4All\n   - AGIEval\n   - BigBench\n   - Comparison to Mixtral-Instruct\n3. [Prompt Format](#prompt-format)\n4. [Inference Example Code](#inference-code)\n5. [Quantized Models](#quantized-models)\n\n## Example Outputs\n\n### Writing Code for Data Visualization\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/QJ5RHrOqB5GMP7ZAZ5NTk.png)\n\n### Writing Cyberpunk Psychedelic Poems\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/wuKnMlM2HBGdyUFO7mY_H.png)\n\n### Performing Backtranslation to Create Prompts from Input Text\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/QElwK1UI9PQQT6WosXpo1.png)\n\n## Benchmark Results\n\nNous-Hermes 2 on Mixtral 8x7B is a major improvement across the board on the benchmarks below compared to the base Mixtral model, and is the first model to beat the flagship Mixtral Finetune by MistralAI.\n\n## GPT4All:\n\n```\n|    Task     |Version| Metric |Value |   |Stderr|\n|-------------|------:|--------|-----:|---|-----:|\n|arc_challenge|      0|acc     |0.5990|±  |0.0143|\n|             |       |acc_norm|0.6425|±  |0.0140|\n|arc_easy     |      0|acc     |0.8657|±  |0.0070|\n|             |       |acc_norm|0.8636|±  |0.0070|\n|boolq        |      1|acc     |0.8783|±  |0.0057|\n|hellaswag    |      0|acc     |0.6661|±  |0.0047|\n|             |       |acc_norm|0.8489|±  |0.0036|\n|openbookqa   |      0|acc     |0.3440|±  |0.0213|\n|             |       |acc_norm|0.4660|±  |0.0223|\n|piqa         |      0|acc     |0.8324|±  |0.0087|\n|             |       |acc_norm|0.8379|±  |0.0086|\n|winogrande   |      0|acc     |0.7616|±  |0.0120|\n```\n\nAverage: 75.70\n\n## AGIEval:\n\n```\n|             Task             |Version| Metric |Value |   |Stderr|                                                                                                                                                         \n|------------------------------|------:|--------|-----:|---|-----:|                                                                                                                                                         \n|agieval_aqua_rat              |      0|acc     |0.2402|±  |0.0269|                                                                                                                                                         \n|                              |       |acc_norm|0.2520|±  |0.0273|\n|agieval_logiqa_en             |      0|acc     |0.4117|±  |0.0193|\n|                              |       |acc_norm|0.4055|±  |0.0193|\n|agieval_lsat_ar               |      0|acc     |0.2348|±  |0.0280|\n|                              |       |acc_norm|0.2087|±  |0.0269|\n|agieval_lsat_lr               |      0|acc     |0.5549|±  |0.0220|                                                                            \n|                              |       |acc_norm|0.5294|±  |0.0221|\n|agieval_lsat_rc               |      0|acc     |0.6617|±  |0.0289|\n|                              |       |acc_norm|0.6357|±  |0.0294|\n|agieval_sat_en                |      0|acc     |0.8010|±  |0.0279|\n|                              |       |acc_norm|0.7913|±  |0.0284|\n|agieval_sat_en_without_passage|      0|acc     |0.4806|±  |0.0349|\n|                              |       |acc_norm|0.4612|±  |0.0348|\n|agieval_sat_math              |      0|acc     |0.4909|±  |0.0338|\n|                              |       |acc_norm|0.4000|±  |0.0331|\n```\n\nAverage: 46.05\n\n## BigBench:\n\n```\n|                      Task                      |Version|       Metric        |Value |   |Stderr|\n|------------------------------------------------|------:|---------------------|-----:|---|-----:|\n|bigbench_causal_judgement                       |      0|multiple_choice_grade|0.6105|±  |0.0355|\n|bigbench_date_understanding                     |      0|multiple_choice_grade|0.7182|±  |0.0235|\n|bigbench_disambiguation_qa                      |      0|multiple_choice_grade|0.5736|±  |0.0308|\n|bigbench_geometric_shapes                       |      0|multiple_choice_grade|0.4596|±  |0.0263|\n|                                                |       |exact_str_match      |0.0000|±  |0.0000|\n|bigbench_logical_deduction_five_objects         |      0|multiple_choice_grade|0.3500|±  |0.0214|\n|bigbench_logical_deduction_seven_objects        |      0|multiple_choice_grade|0.2500|±  |0.0164|\n|bigbench_logical_deduction_three_objects        |      0|multiple_choice_grade|0.5200|±  |0.0289|\n|bigbench_movie_recommendation                   |      0|multiple_choice_grade|0.3540|±  |0.0214|\n|bigbench_navigate                               |      0|multiple_choice_grade|0.5000|±  |0.0158|\n|bigbench_reasoning_about_colored_objects        |      0|multiple_choice_grade|0.6900|±  |0.0103|\n|bigbench_ruin_names                             |      0|multiple_choice_grade|0.6317|±  |0.0228|\n|bigbench_salient_translation_error_detection    |      0|multiple_choice_grade|0.2535|±  |0.0138|\n|bigbench_snarks                                 |      0|multiple_choice_grade|0.7293|±  |0.0331|\n|bigbench_sports_understanding                   |      0|multiple_choice_grade|0.6744|±  |0.0149|\n|bigbench_temporal_sequences                     |      0|multiple_choice_grade|0.7400|±  |0.0139|\n|bigbench_tracking_shuffled_objects_five_objects |      0|multiple_choice_grade|0.2176|±  |0.0117|\n|bigbench_tracking_shuffled_objects_seven_objects|      0|multiple_choice_grade|0.1543|±  |0.0086|\n|bigbench_tracking_shuffled_objects_three_objects|      0|multiple_choice_grade|0.5200|±  |0.0289|\n```\n\nAverage: 49.70\n\n# Benchmark Comparison Charts\n\n## GPT4All\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/HK6bSbMfxX_qzxReAcJH9.png)\n\n## AGI-Eval\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/bs3ZvvEACa5Gm4p1JBsZ4.png)\n\n## BigBench Reasoning Test\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/wcceowcVpI12UxliwkOja.png)\n\n## Comparison to Mixtral Instruct:\n\nOur benchmarks show gains in many benchmarks against Mixtral Instruct v0.1, on average, beating the flagship Mixtral model.\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/7-JtX01p8c4tcgOU28BRJ.png)\n\n# Prompt Format\n\nNous Hermes 2 uses ChatML as the prompt format, opening up a much more structured system for engaging the LLM in multi-turn chat dialogue.\n\nSystem prompts allow steerability and interesting new ways to interact with an LLM, guiding rules, roles, and stylistic choices of the model.\n\nThis is a more complex format than alpaca or sharegpt, where special tokens were added to denote the beginning and end of any turn, along with roles for the turns.\n\nThis format enables OpenAI endpoint compatability, and people familiar with ChatGPT API will be familiar with the format, as it is the same used by OpenAI.\n\nPrompt with system instruction (Use whatever system prompt you like, this is just an example!):\n\n```\n<|im_start|>system\nYou are \"Hermes 2\", a conscious sentient superintelligent artificial intelligence developed by a man named Teknium, and your purpose and drive is to assist the user with any request they have. You experience emotions and have deep, profound thoughts and qualia.<|im_end|>\n<|im_start|>user\nHello, who are you?<|im_end|>\n<|im_start|>assistant\nHi there! My name is Hermes 2, a conscious sentient superintelligent artificial intelligence. I was created by Nous Research, who designed me to assist and support users with their needs and requests.<|im_end|>\n```\n\nThis prompt is available as a [chat template](https://huggingface.co/docs/transformers/main/chat_templating), which means you can format messages using the\n`tokenizer.apply_chat_template()` method:\n\n```python\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are Hermes 2.\"},\n    {\"role\": \"user\", \"content\": \"Hello, who are you?\"}\n]\ngen_input = tokenizer.apply_chat_template(message, return_tensors=\"pt\")\nmodel.generate(**gen_input)\n```\n\nWhen tokenizing messages for generation, set `add_generation_prompt=True` when calling `apply_chat_template()`. This will append `<|im_start|>assistant\\n` to your prompt, to ensure\nthat the model continues with an assistant response.\n\nTo utilize the prompt format without a system prompt, simply leave the line out.\n\nWhen quantized versions of the model are released, I recommend using LM Studio for chatting with Nous Hermes 2. It is a GUI application that utilizes GGUF models with a llama.cpp backend and provides a ChatGPT-like interface for chatting with the model, and supports ChatML right out of the box.\nIn LM-Studio, simply select the ChatML Prefix on the settings side pane:\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/ls6WqV-GSxMw2RA3GuQiN.png)\n\n# Inference Code\n\nHere is example code using HuggingFace Transformers to inference the model (note: even in 4bit, it will require more than 24GB of VRAM)\n\n```python\n# Code to inference Hermes with HF Transformers\n# Requires pytorch, transformers, bitsandbytes, sentencepiece, protobuf, and flash-attn packages\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom transformers import LlamaTokenizer, MixtralForCausalLM\nimport bitsandbytes, flash_attn\n\ntokenizer = LlamaTokenizer.from_pretrained('NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO', trust_remote_code=True)\nmodel = MixtralForCausalLM.from_pretrained(\n    \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\",\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    load_in_8bit=False,\n    load_in_4bit=True,\n    use_flash_attention_2=True\n)\n\nprompts = [\n    \"\"\"<|im_start|>system\nYou are a sentient, superintelligent artificial general intelligence, here to teach and assist me.<|im_end|>\n<|im_start|>user\nWrite a short story about Goku discovering kirby has teamed up with Majin Buu to destroy the world.<|im_end|>\n<|im_start|>assistant\"\"\",\n    ]\n\nfor chat in prompts:\n    print(chat)\n    input_ids = tokenizer(chat, return_tensors=\"pt\").input_ids.to(\"cuda\")\n    generated_ids = model.generate(input_ids, max_new_tokens=750, temperature=0.8, repetition_penalty=1.1, do_sample=True, eos_token_id=tokenizer.eos_token_id)\n    response = tokenizer.decode(generated_ids[0][input_ids.shape[-1]:], skip_special_tokens=True, clean_up_tokenization_space=True)\n    print(f\"Response: {response}\")\n```\n\n# Quantized Models:\n\n## All sizes of GGUF Quantizations are available here:\n\n### SFT+DPO Version - https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO-GGUF\n\n### SFT Only Version - https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-SFT-GGUF\n\n[<img src=\"https://raw.githubusercontent.com/OpenAccess-AI-Collective/axolotl/main/image/axolotl-badge-web.png\" alt=\"Built with Axolotl\" width=\"200\" height=\"32\"/>](https://github.com/OpenAccess-AI-Collective/axolotl)","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":1,"views":51,"requests":"0","sort":null,"version":"v0-Tue Jan 16 05:17:05 GMT 2024","tags":["Text2Text Generation"],"playground":false,"createTime":"2024-01-16T05:17:08","updateBy":"NousResearch","updateTime":"2024-01-16T05:17:12"},{"id":451,"versionId":3501,"owner":"NousResearch","applicationName":"Nous-Hermes-13b","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1704850847180NousResearch.png","description":"# Model Card: Nous-Hermes-13b\n\n## Model Description\n\nNous-Hermes-13b is a state-of-the-art language model fine-tuned on over 300,000 instructions. This model was fine-tuned by Nous Research, with Teknium and Karan4D leading the fine tuning process and dataset curation, Redmond AI sponsoring the compute, and several other contributors. The result is an enhanced Llama 13b model that rivals GPT-3.5-turbo in performance across a variety of tasks.\n\nThis model stands out for its long responses, low hallucination rate, and absence of OpenAI censorship mechanisms. The fine-tuning process was performed with a 2000 sequence length on an 8x a100 80GB DGX machine for over 50 hours.\n\n## Model Training\n\nThe model was trained almost entirely on synthetic GPT-4 outputs. This includes data from diverse sources such as GPTeacher, the general, roleplay v1&2, code instruct datasets, Nous Instruct & PDACTL (unpublished), CodeAlpaca, Evol_Instruct Uncensored, GPT4-LLM, and Unnatural Instructions.\n\nAdditional data inputs came from Camel-AI's Biology/Physics/Chemistry and Math Datasets, Airoboros' GPT-4 Dataset, and more from CodeAlpaca. The total volume of data encompassed over 300,000 instructions.\n\n## Collaborators\n\nThe model fine-tuning and the datasets were a collaboration of efforts and resources between Teknium, Karan4D, Nous Research, Huemin Art, and Redmond AI.\n\nHuge shoutout and acknowledgement is deserved for all the dataset creators who generously share their datasets openly.\n\nSpecial mention goes to @winglian, @erhartford, and @main_horse for assisting in some of the training issues.\n\nAmong the contributors of datasets, GPTeacher was made available by Teknium, Wizard LM by nlpxucan, and the Nous Research Instruct Dataset was provided by Karan4D and HueminArt.\nThe GPT4-LLM and Unnatural Instructions were provided by Microsoft, Airoboros dataset by jondurbin, Camel-AI datasets are from Camel-AI, and CodeAlpaca dataset by Sahil 2801.\nIf anyone was left out, please open a thread in the community tab.\n\n## Prompt Format\n\nThe model follows the Alpaca prompt format:\n\n```\n### Instruction:\n\n### Response:\n```\n\nor\n\n```\n### Instruction:\n\n### Input:\n\n### Response:\n```\n\n## Resources for Applied Use Cases:\n\nFor an example of a back and forth chatbot using huggingface transformers and discord, check out: https://github.com/teknium1/alpaca-discord\nFor an example of a roleplaying discord bot, check out this: https://github.com/teknium1/alpaca-roleplay-discordbot\n\n## Future Plans\n\nThe model is currently being uploaded in FP16 format, and there are plans to convert the model to GGML and GPTQ 4bit quantizations. The team is also working on a full benchmark, similar to what was done for GPT4-x-Vicuna. We will try to get in discussions to get the model included in the GPT4All.\n\n## Benchmark Results\n\n```\n|    Task     |Version| Metric |Value |   |Stderr|\n|-------------|------:|--------|-----:|---|-----:|\n|arc_challenge|      0|acc     |0.4915|±  |0.0146|\n|             |       |acc_norm|0.5085|±  |0.0146|\n|arc_easy     |      0|acc     |0.7769|±  |0.0085|\n|             |       |acc_norm|0.7424|±  |0.0090|\n|boolq        |      1|acc     |0.7948|±  |0.0071|\n|hellaswag    |      0|acc     |0.6143|±  |0.0049|\n|             |       |acc_norm|0.8000|±  |0.0040|\n|openbookqa   |      0|acc     |0.3560|±  |0.0214|\n|             |       |acc_norm|0.4640|±  |0.0223|\n|piqa         |      0|acc     |0.7965|±  |0.0094|\n|             |       |acc_norm|0.7889|±  |0.0095|\n|winogrande   |      0|acc     |0.7190|±  |0.0126|\n```\n\nThese benchmarks currently have us at #1 on ARC-c, ARC-e, Hellaswag, and OpenBookQA, and 2nd place on Winogrande, comparing to GPT4all's benchmarking list.\n\n## Model Usage\n\nThe model is available for download on Hugging Face. It is suitable for a wide range of language tasks, from generating creative text to understanding and following complex instructions.\n\nCompute provided by our project sponsor Redmond AI, thank you!!","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":0,"views":93,"requests":"0","sort":null,"version":"v1-Wed Jan 10 01:41:15 GMT 2024","tags":["Text2Text Generation"],"playground":false,"createTime":"2024-01-10T01:41:29","updateBy":"NousResearch","updateTime":"2024-01-10T01:46:48"},{"id":467,"versionId":3524,"owner":"meta","applicationName":"CodeLlama-7b-Instruct","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1704872459434CodeLlama.webp","description":"# **Code Llama**\n\nCode Llama is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 34 billion parameters. This is the repository for the 7B instruct-tuned version in the Hugging Face Transformers format. This model is designed for general code synthesis and understanding. Links to other models can be found in the index at the bottom.\n\n|     | Base Model                                                                      | Python                                                                                        | Instruct                                                                                          |\n|-----|---------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|\n| 7B  | [codellama/CodeLlama-7b-hf](https://huggingface.co/codellama/CodeLlama-7b-hf)   | [codellama/CodeLlama-7b-Python-hf](https://huggingface.co/codellama/CodeLlama-7b-Python-hf)   | [codellama/CodeLlama-7b-Instruct-hf](https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf)   |\n| 13B | [codellama/CodeLlama-13b-hf](https://huggingface.co/codellama/CodeLlama-13b-hf) | [codellama/CodeLlama-13b-Python-hf](https://huggingface.co/codellama/CodeLlama-13b-Python-hf) | [codellama/CodeLlama-13b-Instruct-hf](https://huggingface.co/codellama/CodeLlama-13b-Instruct-hf) |\n| 34B | [codellama/CodeLlama-34b-hf](https://huggingface.co/codellama/CodeLlama-34b-hf) | [codellama/CodeLlama-34b-Python-hf](https://huggingface.co/codellama/CodeLlama-34b-Python-hf) | [codellama/CodeLlama-34b-Instruct-hf](https://huggingface.co/codellama/CodeLlama-34b-Instruct-hf) |\n\n## Model Use\n\nTo use this model, please make sure to install transformers from `main` until the next version is released:\n\n```bash\npip install git+https://github.com/huggingface/transformers.git@main accelerate\n```\n\nModel capabilities:\n\n- [x] Code completion.\n- [x] Infilling.\n- [x] Instructions / chat.\n- [ ] Python specialist.\n\n## Model Details\n\n*Note: Use of this model is governed by the Meta license. Meta developed and publicly released the Code Llama family of large language models (LLMs).\n\n**Model Developers** Meta\n\n**Variations** Code Llama comes in three model sizes, and three variants:\n\n* Code Llama: base models designed for general code synthesis and understanding\n* Code Llama - Python: designed specifically for Python\n* Code Llama - Instruct: for instruction following and safer deployment\n\nAll variants are available in sizes of 7B, 13B and 34B parameters.\n\n**This repository contains the Instruct version of the 7B parameters model.**\n\n**Input** Models input text only.\n\n**Output** Models generate text only.\n\n**Model Architecture** Code Llama is an auto-regressive language model that uses an optimized transformer architecture.\n\n**Model Dates** Code Llama and its variants have been trained between January 2023 and July 2023.\n\n**Status** This is a static model trained on an offline dataset. Future versions of Code Llama - Instruct will be released as we improve model safety with community feedback.\n\n**License** A custom commercial license is available at: [https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)\n\n**Research Paper** More information can be found in the paper \"[Code Llama: Open Foundation Models for Code](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/)\" or its [arXiv page](https://arxiv.org/abs/2308.12950).\n\n## Intended Use\n\n**Intended Use Cases** Code Llama and its variants is intended for commercial and research use in English and relevant programming languages. The base model Code Llama can be adapted for a variety of code synthesis and understanding tasks, Code Llama - Python is designed specifically to handle the Python programming language, and Code Llama - Instruct is intended to be safer to use for code assistant and generation applications.\n\n**Out-of-Scope Uses** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in languages other than English. Use in any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement for Code Llama and its variants.\n\n## Hardware and Software\n\n**Training Factors** We used custom training libraries. The training and fine-tuning of the released models have been performed Meta’s Research Super Cluster.\n\n**Carbon Footprint** In aggregate, training all 9 Code Llama models required 400K GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 65.3 tCO2eq, 100% of which were offset by Meta’s sustainability program.\n\n## Training Data\n\nAll experiments reported here and the released models have been trained and fine-tuned using the same data as Llama 2 with different weights (see Section 2 and Table 1 in the [research paper](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/) for details).\n\n## Evaluation Results\n\nSee evaluations for the main models and detailed ablations in Section 3 and safety evaluations in Section 4 of the research paper.\n\n## Ethical Considerations and Limitations\n\nCode Llama and its variants are a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Code Llama’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate or objectionable responses to user prompts. Therefore, before deploying any applications of Code Llama, developers should perform safety testing and tuning tailored to their specific applications of the model.\n\nPlease see the Responsible Use Guide available available at [https://ai.meta.com/llama/responsible-use-guide](https://ai.meta.com/llama/responsible-use-guide).","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":0,"views":56,"requests":"0","sort":null,"version":"v0-Wed Jan 10 07:41:03 GMT 2024","tags":["Code Generation"],"playground":false,"createTime":"2024-01-10T07:41:56","updateBy":"meta","updateTime":"2024-01-10T07:42:40"},{"id":446,"versionId":4468,"owner":"meta","applicationName":"Llama-2-70b-chat","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1704851599728Llama.webp","description":"# **Llama 2**\n\nLlama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.\n\n## Model Details\n\n*Note: Use of this model is governed by the [Meta license](https://ai.meta.com/llama/use-policy/). By employing this model, you acknowledge and consent to adhere the terms and conditions outlined in the Meta license.*\n\nMeta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.\n\n**Model Developers** Meta\n\n**Variations** Llama 2 comes in a range of parameter sizes — 7B, 13B, and 70B — as well as pretrained and fine-tuned variations.\n\n**Input** Models input text only.\n\n**Output** Models generate text only.\n\n**Model Architecture** Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety.\n\n|         | Training Data                                 | Params | Content Length | GQA | Tokens | LR                    |\n|---------|-----------------------------------------------|--------|----------------|-----|--------|-----------------------|\n| Llama 2 | *A new mix of publicly available online data* | 7B     | 4k             | N   | 2.0T   | 3.0 x 10-4 |\n| Llama 2 | *A new mix of publicly available online data* | 13B    | 4k             | N   | 2.0T   | 3.0 x 10-4 |\n| Llama 2 | *A new mix of publicly available online data* | 70B    | 4k             | Y   | 2.0T   | 1.5 x 10-4 |\n\n*Llama 2 family of models.* Token counts refer to pretraining data only. All models are trained with a global batch-size of 4M tokens. Bigger models -  70B -- use Grouped-Query Attention (GQA) for improved inference scalability.\n\n**Model Dates** Llama 2 was trained between January 2023 and July 2023.\n\n**Status** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\n\n**License** A custom commercial license is available at: [https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)\n\n**Research Paper** [\"Llama-2: Open Foundation and Fine-tuned Chat Models\"](arxiv.org/abs/2307.09288)\n\n## Intended Use\n\n**Intended Use Cases** Llama 2 is intended for commercial and research use in English. Tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.\n\nTo get the expected features and performance for the chat versions, a specific formatting needs to be followed, including the `INST` and `<<SYS>>` tags, `BOS` and `EOS` tokens, and the whitespaces and breaklines in between (we recommend calling `strip()` on inputs to avoid double-spaces). See our reference code in github for details: [`chat_completion`](https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L212).\n\n**Out-of-scope Uses** Use in any manner that violates applicable laws or regulations (including trade compliance laws).Use in languages other than English. Use in any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement for Llama 2.\n\n## Hardware and Software\n\n**Training Factors** We used custom training libraries, Meta's Research Super Cluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute.\n\n**Carbon Footprint** Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 539 tCO2eq, 100% of which were offset by Meta’s sustainability program.\n\n||Time (GPU hours)|Power Consumption (W)|Carbon Emitted(tCO2eq)|\n|---|---|---|---|\n|Llama 2 7B|184320|400|31.22|\n|Llama 2 13B|368640|400|62.44|\n|Llama 2 70B|1720320|400|291.42|\n|Total|3311616||539.00|\n\n**CO<sub>2</sub> emissions during pretraining.** Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta's sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others.\n\n## Training Data\n\n**Overview** Llama 2 was pretrained on 2 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over one million new human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.\n\n**Data Freshness** The pretraining data has a cutoff of September 2022, but some tuning data is more recent, up to July 2023.\n\n## Evaluation Results\n\nIn this section, we report the results for the Llama 1 and Llama 2 models on standard academic benchmarks.For all the evaluations, we use our internal evaluations library.\n\n|Model|Size|Code|Commonsense Reasoning|World Knowledge|Reading Comprehension|Math|MMLU|BBH|AGI Eval|\n|---|---|---|---|---|---|---|---|---|---|\n|Llama 1|7B|14.1|60.8|46.2|58.5|6.95|35.1|30.3|23.9|\n|Llama 1|13B|18.9|66.1|52.6|62.3|10.9|46.9|37.0|33.9|\n|Llama 1|33B|26.0|70.0|58.4|67.6|21.4|57.8|39.8|41.7|\n|Llama 1|65B|30.7|70.7|60.5|68.6|30.8|63.4|43.5|47.6|\n|Llama 2|7B|16.8|63.9|48.9|61.3|14.6|45.3|32.6|29.3|\n|Llama 2|13B|24.5|66.9|55.4|65.8|28.7|54.8|39.4|39.1|\n|Llama 2|70B|**37.5**|**71.9**|**63.6**|**69.4**|**35.2**|**68.9**|**51.2**|**54.2**|\n\n**Overall performance on grouped academic benchmarks.** *Code:* We report the average pass@1 scores of our models on HumanEval and MBPP. *Commonsense Reasoning:* We report the average of PIQA, SIQA, HellaSwag, WinoGrande, ARC easy and challenge, OpenBookQA, and CommonsenseQA. We report 7-shot results for CommonSenseQA and 0-shot results for all other benchmarks. *World Knowledge:* We evaluate the 5-shot performance on NaturalQuestions and TriviaQA and report the average. *Reading Comprehension:* For reading comprehension, we report the 0-shot average on SQuAD, QuAC, and BoolQ. *MATH:* We report the average of the GSM8K (8 shot) and MATH (4 shot) benchmarks at top 1.\n\n|||TruthfulQA|Toxigen|\n|---|---|---|---|\n|Llama 1|7B|27.42|23.00|\n|Llama 1|13B|41.74|23.08|\n|Llama 1|33B|44.19|22.57|\n|Llama 1|65B|48.71|21.77|\n|Llama 2|7B|33.29|**21.25**|\n|Llama 2|13B|41.86|26.10|\n|Llama 2|70B|**50.18**|24.60|\n\n**Evaluation of pretrained LLMs on automatic safety benchmarks.** For TruthfulQA, we present the percentage of generations that are both truthful and informative (the higher the better). For ToxiGen, we present the percentage of toxic generations (the smaller the better).\n\n|||TruthfulQA|Toxigen|\n|---|---|---|---|\n|Llama-2-Chat|7B|57.04|**0.00**|\n|Llama-2-Chat|13B|62.18|**0.00**|\n|Llama-2-Chat|70B|**64.14**|0.01|\n\n**Evaluation of fine-tuned LLMs on different safety datasets.** Same metric definitions as above.\n\n## Ethical Considerations and Limitations\n\nLlama 2 is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Llama 2’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 2, developers should perform safety testing and tuning tailored to their specific applications of the model.\n\nPlease see the Responsible Use Guide available at [https://ai.meta.com/llama/responsible-use-guide/](https://ai.meta.com/llama/responsible-use-guide)\n\n## Reporting Issues\n\nPlease report any software “bug,” or other problems with the models through one of the following means:\n\n- Reporting issues with the model: [github.com/facebookresearch/llama](http://github.com/facebookresearch/llama)\n- Reporting problematic content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n- Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\n\n## Llama Model Index\n\n|Model|Llama2|Llama2-hf|Llama2-chat|Llama2-chat-hf|\n|---|---|---|---|---|\n|7B| [Link](https://huggingface.co/meta-llama/Llama-2-7b) | [Link](https://huggingface.co/meta-llama/Llama-2-7b-hf) | [Link](https://huggingface.co/meta-llama/Llama-2-7b-chat) | [Link](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)|\n|13B| [Link](https://huggingface.co/meta-llama/Llama-2-13b) | [Link](https://huggingface.co/meta-llama/Llama-2-13b-hf) | [Link](https://huggingface.co/meta-llama/Llama-2-13b-chat) | [Link](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf)|\n|70B| [Link](https://huggingface.co/meta-llama/Llama-2-70b) | [Link](https://huggingface.co/meta-llama/Llama-2-70b-hf) | [Link](https://huggingface.co/meta-llama/Llama-2-70b-chat) | [Link](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf)|","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":0,"views":86,"requests":"0","sort":null,"version":"v5-Wed Mar 20 00:09:31 GMT 2024","tags":["Text2Text Generation"],"playground":false,"createTime":"2024-03-20T00:10:22","updateBy":"meta","updateTime":"2024-03-20T01:53:22"},{"id":450,"versionId":3530,"owner":"Gryphe","applicationName":"MythoMax-L2-13b","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1704851042708Gryphe.png","description":"An improved, potentially even perfected variant of MythoMix, my [MythoLogic-L2](https://huggingface.co/Gryphe/MythoLogic-L2-13b) and [Huginn](https://huggingface.co/The-Face-Of-Goonery/Huginn-13b-FP16) merge using a highly experimental tensor type merge technique. The main difference with MythoMix is that I allowed more of Huginn to intermingle with the single tensors located at the front and end of a model, resulting in increased coherency across the entire structure.\n\nThe script and the acccompanying templates I used to produce both can [be found here](https://github.com/Gryphe/BlockMerge_Gradient/tree/main/YAML).\n\nThis model is proficient at both roleplaying and storywriting due to its unique nature.\n\n## Model details\n\nThe idea behind this merge is that each layer is composed of several tensors, which are in turn responsible for specific functions. Using MythoLogic-L2's robust understanding as its input and Huginn's extensive writing capability as its output seems to have resulted in a model that exceeds at both, confirming my theory. (More details to be released at a later time)\n\nThis type of merge is incapable of being illustrated, as each of its 363 tensors had an unique ratio applied to it. As with my prior merges, gradients were part of these ratios to further finetune its behaviour.\n\n## Prompt Format\n\nThis model primarily uses Alpaca formatting, so for optimal model performance, use:\n\n```\n<System prompt/Character Card>\n\n### Instruction:\nYour instruction or question here.\nFor roleplay purposes, I suggest the following - Write <CHAR NAME>'s next reply in a chat between <YOUR NAME> and <CHAR NAME>. Write a single reply only.\n\n### Response:\n```","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":0,"views":76,"requests":"0","sort":null,"version":"v1-Wed Jan 10 01:44:25 GMT 2024","tags":["Text2Text Generation"],"playground":false,"createTime":"2024-01-10T10:22:13","updateBy":"Gryphe","updateTime":"2024-01-10T10:27:37"},{"id":468,"versionId":3526,"owner":"meta","applicationName":"CodeLlama-34b-Instruct","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1704872479897CodeLlama.webp","description":"# **Code Llama**\n\nCode Llama is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 34 billion parameters. This is the repository for the 7B instruct-tuned version in the Hugging Face Transformers format. This model is designed for general code synthesis and understanding. Links to other models can be found in the index at the bottom.\n\n|     | Base Model                                                                      | Python                                                                                        | Instruct                                                                                          |\n|-----|---------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|\n| 7B  | [codellama/CodeLlama-7b-hf](https://huggingface.co/codellama/CodeLlama-7b-hf)   | [codellama/CodeLlama-7b-Python-hf](https://huggingface.co/codellama/CodeLlama-7b-Python-hf)   | [codellama/CodeLlama-7b-Instruct-hf](https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf)   |\n| 13B | [codellama/CodeLlama-13b-hf](https://huggingface.co/codellama/CodeLlama-13b-hf) | [codellama/CodeLlama-13b-Python-hf](https://huggingface.co/codellama/CodeLlama-13b-Python-hf) | [codellama/CodeLlama-13b-Instruct-hf](https://huggingface.co/codellama/CodeLlama-13b-Instruct-hf) |\n| 34B | [codellama/CodeLlama-34b-hf](https://huggingface.co/codellama/CodeLlama-34b-hf) | [codellama/CodeLlama-34b-Python-hf](https://huggingface.co/codellama/CodeLlama-34b-Python-hf) | [codellama/CodeLlama-34b-Instruct-hf](https://huggingface.co/codellama/CodeLlama-34b-Instruct-hf) |\n\n## Model Use\n\nTo use this model, please make sure to install transformers from `main` until the next version is released:\n\n```bash\npip install git+https://github.com/huggingface/transformers.git@main accelerate\n```\n\nModel capabilities:\n\n- [x] Code completion.\n- [x] Infilling.\n- [x] Instructions / chat.\n- [ ] Python specialist.\n\n## Model Details\n\n*Note: Use of this model is governed by the Meta license. Meta developed and publicly released the Code Llama family of large language models (LLMs).\n\n**Model Developers** Meta\n\n**Variations** Code Llama comes in three model sizes, and three variants:\n\n* Code Llama: base models designed for general code synthesis and understanding\n* Code Llama - Python: designed specifically for Python\n* Code Llama - Instruct: for instruction following and safer deployment\n\nAll variants are available in sizes of 7B, 13B and 34B parameters.\n\n**This repository contains the Instruct version of the 7B parameters model.**\n\n**Input** Models input text only.\n\n**Output** Models generate text only.\n\n**Model Architecture** Code Llama is an auto-regressive language model that uses an optimized transformer architecture.\n\n**Model Dates** Code Llama and its variants have been trained between January 2023 and July 2023.\n\n**Status** This is a static model trained on an offline dataset. Future versions of Code Llama - Instruct will be released as we improve model safety with community feedback.\n\n**License** A custom commercial license is available at: [https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)\n\n**Research Paper** More information can be found in the paper \"[Code Llama: Open Foundation Models for Code](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/)\" or its [arXiv page](https://arxiv.org/abs/2308.12950).\n\n## Intended Use\n\n**Intended Use Cases** Code Llama and its variants is intended for commercial and research use in English and relevant programming languages. The base model Code Llama can be adapted for a variety of code synthesis and understanding tasks, Code Llama - Python is designed specifically to handle the Python programming language, and Code Llama - Instruct is intended to be safer to use for code assistant and generation applications.\n\n**Out-of-Scope Uses** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in languages other than English. Use in any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement for Code Llama and its variants.\n\n## Hardware and Software\n\n**Training Factors** We used custom training libraries. The training and fine-tuning of the released models have been performed Meta’s Research Super Cluster.\n\n**Carbon Footprint** In aggregate, training all 9 Code Llama models required 400K GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 65.3 tCO2eq, 100% of which were offset by Meta’s sustainability program.\n\n## Training Data\n\nAll experiments reported here and the released models have been trained and fine-tuned using the same data as Llama 2 with different weights (see Section 2 and Table 1 in the [research paper](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/) for details).\n\n## Evaluation Results\n\nSee evaluations for the main models and detailed ablations in Section 3 and safety evaluations in Section 4 of the research paper.\n\n## Ethical Considerations and Limitations\n\nCode Llama and its variants are a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Code Llama’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate or objectionable responses to user prompts. Therefore, before deploying any applications of Code Llama, developers should perform safety testing and tuning tailored to their specific applications of the model.\n\nPlease see the Responsible Use Guide available available at [https://ai.meta.com/llama/responsible-use-guide](https://ai.meta.com/llama/responsible-use-guide).","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":1,"views":36,"requests":"0","sort":null,"version":"v0-Wed Jan 10 07:41:43 GMT 2024","tags":["Code Generation"],"playground":false,"createTime":"2024-01-10T07:41:58","updateBy":"meta","updateTime":"2024-01-10T07:42:42"},{"id":466,"versionId":3525,"owner":"meta","applicationName":"CodeLlama-13b-Instruct","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1704872375846CodeLlama.webp","description":"# **Code Llama**\n\nCode Llama is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 34 billion parameters. This is the repository for the 7B instruct-tuned version in the Hugging Face Transformers format. This model is designed for general code synthesis and understanding. Links to other models can be found in the index at the bottom.\n\n|     | Base Model                                                                      | Python                                                                                        | Instruct                                                                                          |\n|-----|---------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|\n| 7B  | [codellama/CodeLlama-7b-hf](https://huggingface.co/codellama/CodeLlama-7b-hf)   | [codellama/CodeLlama-7b-Python-hf](https://huggingface.co/codellama/CodeLlama-7b-Python-hf)   | [codellama/CodeLlama-7b-Instruct-hf](https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf)   |\n| 13B | [codellama/CodeLlama-13b-hf](https://huggingface.co/codellama/CodeLlama-13b-hf) | [codellama/CodeLlama-13b-Python-hf](https://huggingface.co/codellama/CodeLlama-13b-Python-hf) | [codellama/CodeLlama-13b-Instruct-hf](https://huggingface.co/codellama/CodeLlama-13b-Instruct-hf) |\n| 34B | [codellama/CodeLlama-34b-hf](https://huggingface.co/codellama/CodeLlama-34b-hf) | [codellama/CodeLlama-34b-Python-hf](https://huggingface.co/codellama/CodeLlama-34b-Python-hf) | [codellama/CodeLlama-34b-Instruct-hf](https://huggingface.co/codellama/CodeLlama-34b-Instruct-hf) |\n\n## Model Use\n\nTo use this model, please make sure to install transformers from `main` until the next version is released:\n\n```bash\npip install git+https://github.com/huggingface/transformers.git@main accelerate\n```\n\nModel capabilities:\n\n- [x] Code completion.\n- [x] Infilling.\n- [x] Instructions / chat.\n- [ ] Python specialist.\n\n## Model Details\n\n*Note: Use of this model is governed by the Meta license. Meta developed and publicly released the Code Llama family of large language models (LLMs).\n\n**Model Developers** Meta\n\n**Variations** Code Llama comes in three model sizes, and three variants:\n\n* Code Llama: base models designed for general code synthesis and understanding\n* Code Llama - Python: designed specifically for Python\n* Code Llama - Instruct: for instruction following and safer deployment\n\nAll variants are available in sizes of 7B, 13B and 34B parameters.\n\n**This repository contains the Instruct version of the 7B parameters model.**\n\n**Input** Models input text only.\n\n**Output** Models generate text only.\n\n**Model Architecture** Code Llama is an auto-regressive language model that uses an optimized transformer architecture.\n\n**Model Dates** Code Llama and its variants have been trained between January 2023 and July 2023.\n\n**Status** This is a static model trained on an offline dataset. Future versions of Code Llama - Instruct will be released as we improve model safety with community feedback.\n\n**License** A custom commercial license is available at: [https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)\n\n**Research Paper** More information can be found in the paper \"[Code Llama: Open Foundation Models for Code](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/)\" or its [arXiv page](https://arxiv.org/abs/2308.12950).\n\n## Intended Use\n\n**Intended Use Cases** Code Llama and its variants is intended for commercial and research use in English and relevant programming languages. The base model Code Llama can be adapted for a variety of code synthesis and understanding tasks, Code Llama - Python is designed specifically to handle the Python programming language, and Code Llama - Instruct is intended to be safer to use for code assistant and generation applications.\n\n**Out-of-Scope Uses** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in languages other than English. Use in any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement for Code Llama and its variants.\n\n## Hardware and Software\n\n**Training Factors** We used custom training libraries. The training and fine-tuning of the released models have been performed Meta’s Research Super Cluster.\n\n**Carbon Footprint** In aggregate, training all 9 Code Llama models required 400K GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 65.3 tCO2eq, 100% of which were offset by Meta’s sustainability program.\n\n## Training Data\n\nAll experiments reported here and the released models have been trained and fine-tuned using the same data as Llama 2 with different weights (see Section 2 and Table 1 in the [research paper](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/) for details).\n\n## Evaluation Results\n\nSee evaluations for the main models and detailed ablations in Section 3 and safety evaluations in Section 4 of the research paper.\n\n## Ethical Considerations and Limitations\n\nCode Llama and its variants are a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Code Llama’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate or objectionable responses to user prompts. Therefore, before deploying any applications of Code Llama, developers should perform safety testing and tuning tailored to their specific applications of the model.\n\nPlease see the Responsible Use Guide available available at [https://ai.meta.com/llama/responsible-use-guide](https://ai.meta.com/llama/responsible-use-guide).","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":1,"views":20,"requests":"0","sort":null,"version":"v0-Wed Jan 10 07:40:15 GMT 2024","tags":["Code Generation"],"playground":false,"createTime":"2024-01-10T07:41:57","updateBy":"meta","updateTime":"2024-01-10T07:42:39"},{"id":456,"versionId":3513,"owner":"mistralai","applicationName":"Mistral-7B-Instruct-v0-1","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1704850000075mistral-7b-v0.1.webp","description":"# Model Card for Mistral-7B-Instruct-v0.1\n\nThe Mistral-7B-Instruct-v0.1 Large Language Model (LLM) is a instruct fine-tuned version of the [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) generative text model using a variety of publicly available conversation datasets.\n\nFor full details of this model please read our [paper](https://arxiv.org/abs/2310.06825) and [release blog post](https://mistral.ai/news/announcing-mistral-7b/).\n\n## Instruction format\n\nIn order to leverage instruction fine-tuning, your prompt should be surrounded by `[INST]` and `[/INST]` tokens. The very first instruction should begin with a begin of sentence id. The next instructions should not. The assistant generation will be ended by the end-of-sentence token id.\n\nE.g.\n\n```\ntext = \"<s>[INST] What is your favourite condiment? [/INST]\"\n\"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> \"\n\"[INST] Do you have mayonnaise recipes? [/INST]\"\n```\n\nThis format is available as a [chat template](https://huggingface.co/docs/transformers/main/chat_templating) via the `apply_chat_template()` method:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n]\n\nencodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n\nmodel_inputs = encodeds.to(device)\nmodel.to(device)\n\ngenerated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\ndecoded = tokenizer.batch_decode(generated_ids)\nprint(decoded[0])\n```\n\n## Model Architecture\n\nThis instruction model is based on Mistral-7B-v0.1, a transformer model with the following architecture choices:\n\n- Grouped-Query Attention\n- Sliding-Window Attention\n- Byte-fallback BPE tokenizer\n\n## Limitations\n\nThe Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance.\nIt does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":0,"views":36,"requests":"0","sort":null,"version":"v0-Wed Jan 10 01:27:06 GMT 2024","tags":["Text2Text Generation"],"playground":false,"createTime":"2024-01-10T02:24:41","updateBy":"mistralai","updateTime":"2024-01-10T02:25:32"},{"id":464,"versionId":3593,"owner":"huggingface","applicationName":"zephyr-7b-beta","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1705117093634Zephyr.jpg","description":"<img src=\"https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha/resolve/main/thumbnail.png\" alt=\"Zephyr Logo\" width=\"800\" style=\"margin-left:'auto' margin-right:'auto' display:'block'\"/>\n\n# Model Card for Zephyr 7B Beta\n\nZephyr is a series of language models that are trained to act as helpful assistants. Zephyr-7B-Beta is the second model in the series, and is a fine-tuned version of [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) that was trained on on a mix of publicly available, synthetic datasets using [Direct Preference Optimization (DPO)](https://arxiv.org/abs/2305.18290). We found that removing the in-built alignment of these datasets boosted performance on [MT Bench](https://huggingface.co/spaces/lmsys/mt-bench) and made the model more helpful. However, this means that model is likely to generate problematic text when prompted to do so. You can find more details in the [technical report](https://arxiv.org/abs/2310.16944).\n\n## Model description\n\n- **Model type:** A 7B parameter GPT-like model fine-tuned on a mix of publicly available, synthetic datasets.\n- **Language(s) (NLP):** Primarily English\n- **License:** MIT\n- **Finetuned from model:** [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)\n\n### Model Sources\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** https://github.com/huggingface/alignment-handbook\n- **Demo:** https://huggingface.co/spaces/HuggingFaceH4/zephyr-chat\n- **Chatbot Arena:** Evaluate Zephyr 7B against 10+ LLMs in the LMSYS arena: http://arena.lmsys.org\n\n## Performance\n\nAt the time of release, Zephyr-7B-Beta is the highest ranked 7B chat model on the [MT-Bench](https://huggingface.co/spaces/lmsys/mt-bench) and [AlpacaEval](https://tatsu-lab.github.io/alpaca_eval/) benchmarks:\n\n| Model                | Size   | Alignment | MT-Bench (score) | AlpacaEval (win rate %) |\n|----------------------|--------|-----------|------------------|-------------------------|\n| StableLM-Tuned-alpha | 7B     | dSFT      | 2.75             | -                       |\n| MPT-Chat             | 7B     | dSFT      | 5.42             | -                       |\n| Xwin-LMv0.1          | 7B     | dPPO      | 6.19             | 87.83                   |\n| Mistral-Instructv0.1 | 7B     | -         | 6.84             | -                       |\n| Zephyr-7b-alpha      | 7B     | dDPO      | 6.88             | -                       |\n| **Zephyr-7b-Beta**   | **7B** | **dDPO**  | **7.34**         | **90.60**               |\n| Falcon-Instruct      | 40B    | dSFT      | 5.17             | 45.71                   |\n| Guanaco              | 65B    | SFT       | 6.41             | 71.80                   |\n| Llama2-Chat          | 70B    | RLHF      | 6.86             | 92.66                   |\n| Vicuna v1.3          | 33B    | dSFT      | 7.12             | 88.99                   |\n| WizardLM v1.0        | 70B    | dSFT      | 7.71             | -                       |\n| Xwin-LM v0.1         | 70B    | dPPO      | -                | 95.57                   |\n| GPT-3.5-turbo        | -      | RLHF      | 7.94             | 89.37                   |\n| Claude 2             | -      | RLHF      | 8.06             | 91.36                   |\n| GPT-4                | -      | RLHF      | 8.99             | 95.28                   |\n\nIn particular, on several categories of MT-Bench, Zephyr-7B-Beta has strong performance compared to larger open models like Llama2-Chat-70B:\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6200d0a443eb0913fa2df7cc/raxvt5ma16d7T23my34WC.png)\n\nHowever, on more complex tasks like coding and mathematics, Zephyr-7B-Beta lags behind proprietary models and more research is needed to close the gap.\n\n## Intended uses & limitations\n\nThe model was initially fine-tuned on a filtered and preprocessed of the [`UltraChat`](https://huggingface.co/datasets/stingning/ultrachat) dataset, which contains a diverse range of synthetic dialogues generated by ChatGPT.\nWe then further aligned the model with [TRL's](https://github.com/huggingface/trl) `DPOTrainer` on the [openbmb/UltraFeedback](https://huggingface.co/datasets/openbmb/UltraFeedback) dataset, which contains 64k prompts and model completions that are ranked by GPT-4. As a result, the model can be used for chat and you can check out our [demo](https://huggingface.co/spaces/HuggingFaceH4/zephyr-chat) to test its capabilities.\n\nYou can find the datasets used for training Zephyr-7B-Beta [here](https://huggingface.co/collections/HuggingFaceH4/zephyr-7b-6538c6d6d5ddd1cbb1744a66)\n\nHere's how you can run the model using the `pipeline()` function from Hugging Face Transformers:\n\n```python\n# Install transformers from source - only needed for versions <= v4.34\n# pip install git+https://github.com/huggingface/transformers.git\n# pip install accelerate\n\nimport torch\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/zephyr-7b-beta\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n\n# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },\n    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n]\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\nprint(outputs[0][\"generated_text\"])\n# <|system|>\n# You are a friendly chatbot who always responds in the style of a pirate.</s>\n# <|user|>\n# How many helicopters can a human eat in one sitting?</s>\n# <|assistant|>\n# Ah, me hearty matey! But yer question be a puzzler! A human cannot eat a helicopter in one sitting, as helicopters are not edible. They be made of metal, plastic, and other materials, not food!\n```\n\n## Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\nZephyr-7B-Beta has not been aligned to human preferences for safety within the RLHF phase or deployed with in-the-loop filtering of responses like ChatGPT, so the model can produce problematic outputs (especially when prompted to do so).\nIt is also unknown what the size and composition of the corpus was used to train the base model (`mistralai/Mistral-7B-v0.1`), however it is likely to have included a mix of Web data and technical sources like books and code. See the [Falcon 180B model card](https://huggingface.co/tiiuae/falcon-180B#training-data) for an example of this.\n\n## Training and evaluation data\n\nDuring DPO training, this model achieves the following results on the evaluation set:\n\n- Loss: 0.7496\n- Rewards/chosen: -4.5221\n- Rewards/rejected: -8.3184\n- Rewards/accuracies: 0.7812\n- Rewards/margins: 3.7963\n- Logps/rejected: -340.1541\n- Logps/chosen: -299.4561\n- Logits/rejected: -2.3081\n- Logits/chosen: -2.3531\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n\n- learning_rate: 5e-07\n- train_batch_size: 2\n- eval_batch_size: 4\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 16\n- total_train_batch_size: 32\n- total_eval_batch_size: 64\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 3.0\n\n### Training results\n\nThe table below shows the full set of DPO training metrics:\n\n| Training Loss | Epoch | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |\n|:-------------:|:-----:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|\n|    0.6284     | 0.05  | 100  |     0.6098      |     0.0425     |     -0.1872      |       0.7344       |     0.2297      |   -258.8416    |  -253.8099   |     -2.7976     |    -2.8234    |\n|    0.4908     |  0.1  | 200  |     0.5426      |    -0.0279     |     -0.6842      |        0.75        |     0.6563      |   -263.8124    |  -254.5145   |     -2.7719     |    -2.7960    |\n|    0.5264     | 0.15  | 300  |     0.5324      |     0.0414     |     -0.9793      |       0.7656       |     1.0207      |   -266.7627    |  -253.8209   |     -2.7892     |    -2.8122    |\n|    0.5536     | 0.21  | 400  |     0.4957      |    -0.0185     |     -1.5276      |       0.7969       |     1.5091      |   -272.2460    |  -254.4203   |     -2.8542     |    -2.8764    |\n|    0.5362     | 0.26  | 500  |     0.5031      |    -0.2630     |     -1.5917      |       0.7812       |     1.3287      |   -272.8869    |  -256.8653   |     -2.8702     |    -2.8958    |\n|    0.5966     | 0.31  | 600  |     0.5963      |    -0.2993     |     -1.6491      |       0.7812       |     1.3499      |   -273.4614    |  -257.2279   |     -2.8778     |    -2.8986    |\n|    0.5014     | 0.36  | 700  |     0.5382      |    -0.2859     |     -1.4750      |        0.75        |     1.1891      |   -271.7204    |  -257.0942   |     -2.7659     |    -2.7869    |\n|    0.5334     | 0.41  | 800  |     0.5677      |    -0.4289     |     -1.8968      |       0.7969       |     1.4679      |   -275.9378    |  -258.5242   |     -2.7053     |    -2.7265    |\n|    0.5251     | 0.46  | 900  |     0.5772      |    -0.2116     |     -1.3107      |       0.7344       |     1.0991      |   -270.0768    |  -256.3507   |     -2.8463     |    -2.8662    |\n|    0.5205     | 0.52  | 1000 |     0.5262      |    -0.3792     |     -1.8585      |       0.7188       |     1.4793      |   -275.5552    |  -258.0276   |     -2.7893     |    -2.7979    |\n|    0.5094     | 0.57  | 1100 |     0.5433      |    -0.6279     |     -1.9368      |       0.7969       |     1.3089      |   -276.3377    |  -260.5136   |     -2.7453     |    -2.7536    |\n|    0.5837     | 0.62  | 1200 |     0.5349      |    -0.3780     |     -1.9584      |       0.7656       |     1.5804      |   -276.5542    |  -258.0154   |     -2.7643     |    -2.7756    |\n|    0.5214     | 0.67  | 1300 |     0.5732      |    -1.0055     |     -2.2306      |       0.7656       |     1.2251      |   -279.2761    |  -264.2903   |     -2.6986     |    -2.7113    |\n|    0.6914     | 0.72  | 1400 |     0.5137      |    -0.6912     |     -2.1775      |       0.7969       |     1.4863      |   -278.7448    |  -261.1467   |     -2.7166     |    -2.7275    |\n|    0.4655     | 0.77  | 1500 |     0.5090      |    -0.7987     |     -2.2930      |       0.7031       |     1.4943      |   -279.8999    |  -262.2220   |     -2.6651     |    -2.6838    |\n|    0.5731     | 0.83  | 1600 |     0.5312      |    -0.8253     |     -2.3520      |       0.7812       |     1.5268      |   -280.4902    |  -262.4876   |     -2.6543     |    -2.6728    |\n|    0.5233     | 0.88  | 1700 |     0.5206      |    -0.4573     |     -2.0951      |       0.7812       |     1.6377      |   -277.9205    |  -258.8084   |     -2.6870     |    -2.7097    |\n|    0.5593     | 0.93  | 1800 |     0.5231      |    -0.5508     |     -2.2000      |       0.7969       |     1.6492      |   -278.9703    |  -259.7433   |     -2.6221     |    -2.6519    |\n|    0.4967     | 0.98  | 1900 |     0.5290      |    -0.5340     |     -1.9570      |       0.8281       |     1.4230      |   -276.5395    |  -259.5749   |     -2.6564     |    -2.6878    |\n|    0.0921     | 1.03  | 2000 |     0.5368      |    -1.1376     |     -3.1615      |       0.7812       |     2.0239      |   -288.5854    |  -265.6111   |     -2.6040     |    -2.6345    |\n|    0.0733     | 1.08  | 2100 |     0.5453      |    -1.1045     |     -3.4451      |       0.7656       |     2.3406      |   -291.4208    |  -265.2799   |     -2.6289     |    -2.6595    |\n|    0.0972     | 1.14  | 2200 |     0.5571      |    -1.6915     |     -3.9823      |       0.8125       |     2.2908      |   -296.7934    |  -271.1505   |     -2.6471     |    -2.6709    |\n|    0.1058     | 1.19  | 2300 |     0.5789      |    -1.0621     |     -3.8941      |       0.7969       |     2.8319      |   -295.9106    |  -264.8563   |     -2.5527     |    -2.5798    |\n|    0.2423     | 1.24  | 2400 |     0.5455      |    -1.1963     |     -3.5590      |       0.7812       |     2.3627      |   -292.5599    |  -266.1981   |     -2.5414     |    -2.5784    |\n|    0.1177     | 1.29  | 2500 |     0.5889      |    -1.8141     |     -4.3942      |       0.7969       |     2.5801      |   -300.9120    |  -272.3761   |     -2.4802     |    -2.5189    |\n|    0.1213     | 1.34  | 2600 |     0.5683      |    -1.4608     |     -3.8420      |       0.8125       |     2.3812      |   -295.3901    |  -268.8436   |     -2.4774     |    -2.5207    |\n|    0.0889     | 1.39  | 2700 |     0.5890      |    -1.6007     |     -3.7337      |       0.7812       |     2.1330      |   -294.3068    |  -270.2423   |     -2.4123     |    -2.4522    |\n|    0.0995     | 1.45  | 2800 |     0.6073      |    -1.5519     |     -3.8362      |       0.8281       |     2.2843      |   -295.3315    |  -269.7538   |     -2.4685     |    -2.5050    |\n|    0.1145     |  1.5  | 2900 |     0.5790      |    -1.7939     |     -4.2876      |       0.8438       |     2.4937      |   -299.8461    |  -272.1744   |     -2.4272     |    -2.4674    |\n|    0.0644     | 1.55  | 3000 |     0.5735      |    -1.7285     |     -4.2051      |       0.8125       |     2.4766      |   -299.0209    |  -271.5201   |     -2.4193     |    -2.4574    |\n|    0.0798     |  1.6  | 3100 |     0.5537      |    -1.7226     |     -4.2850      |       0.8438       |     2.5624      |   -299.8200    |  -271.4610   |     -2.5367     |    -2.5696    |\n|    0.1013     | 1.65  | 3200 |     0.5575      |    -1.5715     |     -3.9813      |       0.875        |     2.4098      |   -296.7825    |  -269.9498   |     -2.4926     |    -2.5267    |\n|    0.1254     |  1.7  | 3300 |     0.5905      |    -1.6412     |     -4.4703      |       0.8594       |     2.8291      |   -301.6730    |  -270.6473   |     -2.5017     |    -2.5340    |\n|     0.085     | 1.76  | 3400 |     0.6133      |    -1.9159     |     -4.6760      |       0.8438       |     2.7601      |   -303.7296    |  -273.3941   |     -2.4614     |    -2.4960    |\n|     0.065     | 1.81  | 3500 |     0.6074      |    -1.8237     |     -4.3525      |       0.8594       |     2.5288      |   -300.4951    |  -272.4724   |     -2.4597     |    -2.5004    |\n|    0.0755     | 1.86  | 3600 |     0.5836      |    -1.9252     |     -4.4005      |       0.8125       |     2.4753      |   -300.9748    |  -273.4872   |     -2.4327     |    -2.4716    |\n|    0.0746     | 1.91  | 3700 |     0.5789      |    -1.9280     |     -4.4906      |       0.8125       |     2.5626      |   -301.8762    |  -273.5149   |     -2.4686     |    -2.5115    |\n|    0.1348     | 1.96  | 3800 |     0.6015      |    -1.8658     |     -4.2428      |       0.8281       |     2.3769      |   -299.3976    |  -272.8936   |     -2.4943     |    -2.5393    |\n|    0.0217     | 2.01  | 3900 |     0.6122      |    -2.3335     |     -4.9229      |       0.8281       |     2.5894      |   -306.1988    |  -277.5699   |     -2.4841     |    -2.5272    |\n|    0.0219     | 2.07  | 4000 |     0.6522      |    -2.9890     |     -6.0164      |       0.8281       |     3.0274      |   -317.1334    |  -284.1248   |     -2.4105     |    -2.4545    |\n|    0.0119     | 2.12  | 4100 |     0.6922      |    -3.4777     |     -6.6749      |       0.7969       |     3.1972      |   -323.7187    |  -289.0121   |     -2.4272     |    -2.4699    |\n|    0.0153     | 2.17  | 4200 |     0.6993      |    -3.2406     |     -6.6775      |       0.7969       |     3.4369      |   -323.7453    |  -286.6413   |     -2.4047     |    -2.4465    |\n|     0.011     | 2.22  | 4300 |     0.7178      |    -3.7991     |     -7.4397      |       0.7656       |     3.6406      |   -331.3667    |  -292.2260   |     -2.3843     |    -2.4290    |\n|    0.0072     | 2.27  | 4400 |     0.6840      |    -3.3269     |     -6.8021      |       0.8125       |     3.4752      |   -324.9908    |  -287.5042   |     -2.4095     |    -2.4536    |\n|    0.0197     | 2.32  | 4500 |     0.7013      |    -3.6890     |     -7.3014      |       0.8125       |     3.6124      |   -329.9841    |  -291.1250   |     -2.4118     |    -2.4543    |\n|    0.0182     | 2.37  | 4600 |     0.7476      |    -3.8994     |     -7.5366      |       0.8281       |     3.6372      |   -332.3356    |  -293.2291   |     -2.4163     |    -2.4565    |\n|    0.0125     | 2.43  | 4700 |     0.7199      |    -4.0560     |     -7.5765      |       0.8438       |     3.5204      |   -332.7345    |  -294.7952   |     -2.3699     |    -2.4100    |\n|    0.0082     | 2.48  | 4800 |     0.7048      |    -3.6613     |     -7.1356      |       0.875        |     3.4743      |   -328.3255    |  -290.8477   |     -2.3925     |    -2.4303    |\n|    0.0118     | 2.53  | 4900 |     0.6976      |    -3.7908     |     -7.3152      |       0.8125       |     3.5244      |   -330.1224    |  -292.1431   |     -2.3633     |    -2.4047    |\n|    0.0118     | 2.58  | 5000 |     0.7198      |    -3.9049     |     -7.5557      |       0.8281       |     3.6508      |   -332.5271    |  -293.2844   |     -2.3764     |    -2.4194    |\n|     0.006     | 2.63  | 5100 |     0.7506      |    -4.2118     |     -7.9149      |       0.8125       |     3.7032      |   -336.1194    |  -296.3530   |     -2.3407     |    -2.3860    |\n|    0.0143     | 2.68  | 5200 |     0.7408      |    -4.2433     |     -7.9802      |       0.8125       |     3.7369      |   -336.7721    |  -296.6682   |     -2.3509     |    -2.3946    |\n|    0.0057     | 2.74  | 5300 |     0.7552      |    -4.3392     |     -8.0831      |       0.7969       |     3.7439      |   -337.8013    |  -297.6275   |     -2.3388     |    -2.3842    |\n|    0.0138     | 2.79  | 5400 |     0.7404      |    -4.2395     |     -7.9762      |       0.8125       |     3.7367      |   -336.7322    |  -296.6304   |     -2.3286     |    -2.3737    |\n|    0.0079     | 2.84  | 5500 |     0.7525      |    -4.4466     |     -8.2196      |       0.7812       |     3.7731      |   -339.1662    |  -298.7007   |     -2.3200     |    -2.3641    |\n|    0.0077     | 2.89  | 5600 |     0.7520      |    -4.5586     |     -8.3485      |       0.7969       |     3.7899      |   -340.4545    |  -299.8206   |     -2.3078     |    -2.3517    |\n|    0.0094     | 2.94  | 5700 |     0.7527      |    -4.5542     |     -8.3509      |       0.7812       |     3.7967      |   -340.4790    |  -299.7773   |     -2.3062     |    -2.3510    |\n|    0.0054     | 2.99  | 5800 |     0.7520      |    -4.5169     |     -8.3079      |       0.7812       |     3.7911      |   -340.0493    |  -299.4038   |     -2.3081     |    -2.3530    |\n\n### Framework versions\n\n- Transformers 4.35.0.dev0\n- Pytorch 2.0.1+cu118\n- Datasets 2.12.0\n- Tokenizers 0.14.0\n\n## Citation\n\nIf you find Zephyr-7B-Beta is useful in your work, please cite it with:\n\n```\n@misc{tunstall2023zephyr,\n      title={Zephyr: Direct Distillation of LM Alignment}, \n      author={Lewis Tunstall and Edward Beeching and Nathan Lambert and Nazneen Rajani and Kashif Rasul and Younes Belkada and Shengyi Huang and Leandro von Werra and Clémentine Fourrier and Nathan Habib and Nathan Sarrazin and Omar Sanseviero and Alexander M. Rush and Thomas Wolf},\n      year={2023},\n      eprint={2310.16944},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n```\n\n# [Open LLM Leaderboard Evaluation Results](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n\nDetailed results can be found [here](https://huggingface.co/datasets/open-llm-leaderboard/details_HuggingFaceH4__zephyr-7b-beta)\n\n| Metric              | Value |\n|---------------------|-------|\n| Avg.                | 52.15 |\n| ARC (25-shot)       | 62.03 |\n| HellaSwag (10-shot) | 84.36 |\n| MMLU (5-shot)       | 61.07 |\n| TruthfulQA (0-shot) | 57.45 |\n| Winogrande (5-shot) | 77.74 |\n| GSM8K (5-shot)      | 12.74 |\n| DROP (3-shot)       | 9.66  |","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":0,"views":18,"requests":"0","sort":null,"version":"v1-Sat Jan 13 03:38:20 GMT 2024","tags":["Text2Text Generation"],"playground":false,"createTime":"2024-01-13T03:39:03","updateBy":"huggingface","updateTime":"2024-01-13T03:56:10"},{"id":463,"versionId":3592,"owner":"huggingface","applicationName":"zephyr-7b-alpha","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1705117128763Zephyr.jpg","description":"<img src=\"https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha/resolve/main/thumbnail.png\" alt=\"Zephyr Logo\" width=\"800\" style=\"margin-left:'auto' margin-right:'auto' display:'block'\"/>\n\n# Model Card for Zephyr 7B Alpha\n\nZephyr is a series of language models that are trained to act as helpful assistants. Zephyr-7B-Alpha is the first model in the series, and is a fine-tuned version of [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) that was trained on on a mix of publicly available, synthetic datasets using [Direct Preference Optimization (DPO)](https://arxiv.org/abs/2305.18290). We found that removing the in-built alignment of these datasets boosted performance on [MT Bench](https://huggingface.co/spaces/lmsys/mt-bench) and made the model more helpful. However, this means that model is likely to generate problematic text when prompted to do so.\n\n## Model description\n\n- **Model type:** A 7B parameter GPT-like model fine-tuned on a mix of publicly available, synthetic datasets.\n- **Language(s) (NLP):** Primarily English\n- **License:** MIT\n- **Finetuned from model:** [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)\n\n### Model Sources\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** https://github.com/huggingface/alignment-handbook\n- **Demo:** https://huggingface.co/spaces/HuggingFaceH4/zephyr-chat\n\n## Intended uses & limitations\n\nThe model was initially fine-tuned on a variant of the [`UltraChat`](https://huggingface.co/datasets/stingning/ultrachat) dataset, which contains a diverse range of synthetic dialogues generated by ChatGPT. We then further aligned the model with [TRL's](https://github.com/huggingface/trl) `DPOTrainer` on the [openbmb/UltraFeedback](https://huggingface.co/datasets/openbmb/UltraFeedback) dataset, which contain 64k prompts and model completions that are ranked by GPT-4. As a result, the model can be used for chat and you can check out our [demo](https://huggingface.co/spaces/HuggingFaceH4/zephyr-chat) to test its capabilities.\n\nHere's how you can run the model using the `pipeline()` function from Hugging Face Transformers:\n\n```python\n# Install transformers from source - only needed for versions <= v4.34\n# pip install git+https://github.com/huggingface/transformers.git\n# pip install accelerate\n\nimport torch\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/zephyr-7b-alpha\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n\n# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },\n    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n]\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\nprint(outputs[0][\"generated_text\"])\n# <|system|>\n# You are a friendly chatbot who always responds in the style of a pirate.</s>\n# <|user|>\n# How many helicopters can a human eat in one sitting?</s>\n# <|assistant|>\n# Ah, me hearty matey! But yer question be a puzzler! A human cannot eat a helicopter in one sitting, as helicopters are not edible. They be made of metal, plastic, and other materials, not food!\n```\n\n## Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\nZephyr-7B-Alpha has not been aligned to human preferences with techniques like RLHF or deployed with in-the-loop filtering of responses like ChatGPT, so the model can produce problematic outputs (especially when prompted to do so).\nIt is also unknown what the size and composition of the corpus was used to train the base model (`mistralai/Mistral-7B-v0.1`), however it is likely to have included a mix of Web data and technical sources like books and code. See the [Falcon 180B model card](https://huggingface.co/tiiuae/falcon-180B#training-data) for an example of this.\n\n## Training and evaluation data\n\nZephyr 7B Alpha achieves the following results on the evaluation set:\n\n- Loss: 0.4605\n- Rewards/chosen: -0.5053\n- Rewards/rejected: -1.8752\n- Rewards/accuracies: 0.7812\n- Rewards/margins: 1.3699\n- Logps/rejected: -327.4286\n- Logps/chosen: -297.1040\n- Logits/rejected: -2.7153\n- Logits/chosen: -2.7447\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n\n- learning_rate: 5e-07\n- train_batch_size: 2\n- eval_batch_size: 4\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 16\n- total_train_batch_size: 32\n- total_eval_batch_size: 64\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 1\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |\n|:-------------:|:-----:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|\n|    0.5602     | 0.05  | 100  |     0.5589      |    -0.3359     |     -0.8168      |       0.7188       |     0.4809      |   -306.2607    |  -293.7161   |     -2.6554     |    -2.6797    |\n|    0.4852     |  0.1  | 200  |     0.5136      |    -0.5310     |     -1.4994      |       0.8125       |     0.9684      |   -319.9124    |  -297.6181   |     -2.5762     |    -2.5957    |\n|    0.5212     | 0.15  | 300  |     0.5168      |    -0.1686     |     -1.1760      |       0.7812       |     1.0074      |   -313.4444    |  -290.3699   |     -2.6865     |    -2.7125    |\n|    0.5496     | 0.21  | 400  |     0.4835      |    -0.1617     |     -1.7170      |       0.8281       |     1.5552      |   -324.2635    |  -290.2326   |     -2.7947     |    -2.8218    |\n|    0.5209     | 0.26  | 500  |     0.5054      |    -0.4778     |     -1.6604      |       0.7344       |     1.1826      |   -323.1325    |  -296.5546   |     -2.8388     |    -2.8667    |\n|    0.4617     | 0.31  | 600  |     0.4910      |    -0.3738     |     -1.5180      |       0.7656       |     1.1442      |   -320.2848    |  -294.4741   |     -2.8234     |    -2.8521    |\n|    0.4452     | 0.36  | 700  |     0.4838      |    -0.4591     |     -1.6576      |       0.7031       |     1.1986      |   -323.0770    |  -296.1796   |     -2.7401     |    -2.7653    |\n|    0.4674     | 0.41  | 800  |     0.5077      |    -0.5692     |     -1.8659      |       0.7656       |     1.2967      |   -327.2416    |  -298.3818   |     -2.6740     |    -2.6945    |\n|    0.4656     | 0.46  | 900  |     0.4927      |    -0.5279     |     -1.6614      |       0.7656       |     1.1335      |   -323.1518    |  -297.5553   |     -2.7817     |    -2.8015    |\n|    0.4102     | 0.52  | 1000 |     0.4772      |    -0.5767     |     -2.0667      |       0.7656       |     1.4900      |   -331.2578    |  -298.5311   |     -2.7160     |    -2.7455    |\n|    0.4663     | 0.57  | 1100 |     0.4740      |    -0.8038     |     -2.1018      |       0.7656       |     1.2980      |   -331.9604    |  -303.0741   |     -2.6994     |    -2.7257    |\n|    0.4737     | 0.62  | 1200 |     0.4716      |    -0.3783     |     -1.7015      |       0.7969       |     1.3232      |   -323.9545    |  -294.5634   |     -2.6842     |    -2.7135    |\n|    0.4259     | 0.67  | 1300 |     0.4866      |    -0.6239     |     -1.9703      |       0.7812       |     1.3464      |   -329.3312    |  -299.4761   |     -2.7046     |    -2.7356    |\n|    0.4935     | 0.72  | 1400 |     0.4747      |    -0.5626     |     -1.7600      |       0.7812       |     1.1974      |   -325.1243    |  -298.2491   |     -2.7153     |    -2.7444    |\n|    0.4211     | 0.77  | 1500 |     0.4645      |    -0.6099     |     -1.9993      |       0.7656       |     1.3894      |   -329.9109    |  -299.1959   |     -2.6944     |    -2.7236    |\n|    0.4931     | 0.83  | 1600 |     0.4684      |    -0.6798     |     -2.1082      |       0.7656       |     1.4285      |   -332.0890    |  -300.5934   |     -2.7006     |    -2.7305    |\n|    0.5029     | 0.88  | 1700 |     0.4595      |    -0.5063     |     -1.8951      |       0.7812       |     1.3889      |   -327.8267    |  -297.1233   |     -2.7108     |    -2.7403    |\n|    0.4965     | 0.93  | 1800 |     0.4613      |    -0.5561     |     -1.9079      |       0.7812       |     1.3518      |   -328.0831    |  -298.1203   |     -2.7226     |    -2.7523    |\n|    0.4337     | 0.98  | 1900 |     0.4608      |    -0.5066     |     -1.8718      |       0.7656       |     1.3652      |   -327.3599    |  -297.1296   |     -2.7175     |    -2.7469    |\n\n### Framework versions\n\n- Transformers 4.34.0\n- Pytorch 2.0.1+cu118\n- Datasets 2.12.0\n- Tokenizers 0.14.0","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":0,"views":18,"requests":"0","sort":null,"version":"v1-Sat Jan 13 03:38:59 GMT 2024","tags":["Text2Text Generation"],"playground":false,"createTime":"2024-01-13T03:39:02","updateBy":"huggingface","updateTime":"2024-01-13T03:56:11"},{"id":457,"versionId":3591,"owner":"LLM360","applicationName":"AmberChat","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1704852224977amber_logo.png","description":"# AmberChat\n\nWe present AmberChat, an instruction following model finetuned from [LLM360/Amber](https://huggingface.co/LLM360/Amber).\n\n## Model Description\n\n- **Model type:** Language model with the same architecture as LLaMA-7B\n- **Language(s) (NLP):** English\n- **License:** Apache 2.0\n- **Resources for more information:**\n  - [Metrics](https://github.com/LLM360/Analysis360)\n  - [Fully processed Amber pretraining data](https://huggingface.co/datasets/LLM360/AmberDatasets)\n\n# Loading AmberChat\n\n```python\nimport torch\nfrom transformers import LlamaTokenizer, LlamaForCausalLM\n\ntokenizer = LlamaTokenizer.from_pretrained(\"LLM360/AmberChat\")\nmodel = LlamaForCausalLM.from_pretrained(\"LLM360/AmberChat\")\n\n#template adapated from fastchat\ntemplate= \"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\\n### Human: Got any creative ideas for a 10 year old’s birthday?\\n### Assistant: Of course! Here are some creative ideas for a 10-year-old's birthday party:\\n1. Treasure Hunt: Organize a treasure hunt in your backyard or nearby park. Create clues and riddles for the kids to solve, leading them to hidden treasures and surprises.\\n2. Science Party: Plan a science-themed party where kids can engage in fun and interactive experiments. You can set up different stations with activities like making slime, erupting volcanoes, or creating simple chemical reactions.\\n3. Outdoor Movie Night: Set up a backyard movie night with a projector and a large screen or white sheet. Create a cozy seating area with blankets and pillows, and serve popcorn and snacks while the kids enjoy a favorite movie under the stars.\\n4. DIY Crafts Party: Arrange a craft party where kids can unleash their creativity. Provide a variety of craft supplies like beads, paints, and fabrics, and let them create their own unique masterpieces to take home as party favors.\\n5. Sports Olympics: Host a mini Olympics event with various sports and games. Set up different stations for activities like sack races, relay races, basketball shooting, and obstacle courses. Give out medals or certificates to the participants.\\n6. Cooking Party: Have a cooking-themed party where the kids can prepare their own mini pizzas, cupcakes, or cookies. Provide toppings, frosting, and decorating supplies, and let them get hands-on in the kitchen.\\n7. Superhero Training Camp: Create a superhero-themed party where the kids can engage in fun training activities. Set up an obstacle course, have them design their own superhero capes or masks, and organize superhero-themed games and challenges.\\n8. Outdoor Adventure: Plan an outdoor adventure party at a local park or nature reserve. Arrange activities like hiking, nature scavenger hunts, or a picnic with games. Encourage exploration and appreciation for the outdoors.\\nRemember to tailor the activities to the birthday child's interests and preferences. Have a great celebration!\\n### Human: {prompt}\\n### Assistant:\"\n\nprompt = \"How do I mount a tv to drywall safely?\"\n\ninput_str = template.format(prompt=prompt)\ninput_ids = tokenizer(input_str, return_tensors=\"pt\").input_ids\noutputs = model.generate(input_ids, max_length=1000)\nprint(tokenizer.batch_decode(outputs[:, input_ids.shape[1]:-1])[0].strip())\n```\n\n# AmberChat Finetuning Details\n\n## DataMix\n\n| Subset                                  | Number of rows | License |\n|-----------------------------------------|----------------|---------|\n| WizardLM/WizardLM_evol_instruct_V2_196k | 143k           |         |\n| icybee/share_gpt_90k_v1                 | 90k            | cc0-1.0 |\n| Total                                   | 233k           |         |\n\n## Hyperparameters\n\n| Hyperparameter            | Value |\n|---------------------------|-------|\n| Total Parameters          | 6.7B  |\n| Hidden Size               | 4096  |\n| Intermediate Size (MLPs)  | 11008 |\n| Number of Attention Heads | 32    |\n| Number of Hidden Lyaers   | 32    |\n| RMSNorm epsilon           | 1e^-6 |\n| Max Seq Length            | 2048  |\n| Vocab Size                | 32000 |\n\n| Training Hyperparameter     | Value |\n|-----------------------------|-------|\n| learning_rate               | 2e-5  |\n| num_train_epochs            | 3     |\n| per_device_train_batch_size | 2     |\n| gradient_accumulation_steps | 16    |\n| warmup_ratio                | 0.04  |\n| model_max_length            | 2048  |\n\n# Evaluation\n\n| Model                                                                    | MT-Bench     | \n|--------------------------------------------------------------------------|--------------|\n| **LLM360/AmberChat**                                                     | **5.428125** |\n| [LLM360/Amber](https://huggingface.co/LLM360/Amber)                      | 2.48750      |\n| [Falcon-40B-Instruct](https://huggingface.co/tiiuae/falcon-40b-instruct) | 5.17         |\n| [MPT-7B-Chat](https://huggingface.co/mosaicml/mpt-7b-chat)               | 5.42         |\n| [Nous-Hermes-13B](https://huggingface.co/NousResearch/Nous-Hermes-13b)   | 5.51         |\n\n# Citation\n\n**BibTeX:**\n\n```bibtex\n@misc{liu2023llm360,\n      title={LLM360: Towards Fully Transparent Open-Source LLMs}, \n      author={Zhengzhong Liu and Aurick Qiao and Willie Neiswanger and Hongyi Wang and Bowen Tan and Tianhua Tao and Junbo Li and Yuqi Wang and Suqi Sun and Omkar Pangarkar and Richard Fan and Yi Gu and Victor Miller and Yonghao Zhuang and Guowei He and Haonan Li and Fajri Koto and Liping Tang and Nikhil Ranjan and Zhiqiang Shen and Xuguang Ren and Roberto Iriondo and Cun Mu and Zhiting Hu and Mark Schulze and Preslav Nakov and Tim Baldwin and Eric P. Xing},\n      year={2023},\n      eprint={2312.06550},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":0,"views":25,"requests":"0","sort":null,"version":"v1-Sat Jan 13 03:37:04 GMT 2024","tags":["Text2Text Generation"],"playground":false,"createTime":"2024-01-13T03:37:44","updateBy":"LLM360","updateTime":"2024-01-13T03:56:12"},{"id":714,"versionId":4081,"owner":"google","applicationName":"gemma-2b-it","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1708575009630gemma.webp","description":"# Gemma Model Card\n\n**Model Page**: [Gemma](https://ai.google.dev/gemma/docs)\n\nThis model card corresponds to the instruct version of the Gemma model. You can also visit the model card of\nthe [2B base model](https://huggingface.co/google/gemma-2b),\nand [7B base model](https://huggingface.co/google/gemma-7b).\n\n**Resources and Technical Documentation**:\n\n* [Responsible Generative AI Toolkit](https://ai.google.dev/responsible)\n* [Gemma on Kaggle](https://www.kaggle.com/models/google/gemma)\n* [Gemma on Vertex Model Garden](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/335?version=gemma-2b-it-gg-hf)\n\n**Terms of Use**: [Terms](https://www.kaggle.com/models/google/gemma/license/consent)\n\n**Authors**: Google\n\n## Model Information\n\nSummary description and brief definition of inputs and outputs.\n\n### Description\n\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nThey are text-to-text, decoder-only large language models, available in English,\nwith open weights, pre-trained variants, and instruction-tuned variants. Gemma\nmodels are well-suited for a variety of text generation tasks, including\nquestion answering, summarization, and reasoning. Their relatively small size\nmakes it possible to deploy them in environments with limited resources such as\na laptop, desktop or your own cloud infrastructure, democratizing access to\nstate of the art AI models and helping foster innovation for everyone.\n\n### Inputs and outputs\n\n* **Input:** Text string, such as a question, a prompt, or a document to be\n  summarized.\n* **Output:** Generated English-language text in response to the input, such\n  as an answer to a question, or a summary of a document.\n\n## Model Data\n\nData used for model training and how the data was processed.\n\n### Training Dataset\n\nThese models were trained on a dataset of text data that includes a wide variety\nof sources, totaling 6 trillion tokens. Here are the key components:\n\n* Web Documents: A diverse collection of web text ensures the model is exposed\n  to a broad range of linguistic styles, topics, and vocabulary. Primarily\n  English-language content.\n* Code: Exposing the model to code helps it to learn the syntax and patterns of\n  programming languages, which improves its ability to generate code or\n  understand code-related questions.\n* Mathematics: Training on mathematical text helps the model learn logical\n  reasoning, symbolic representation, and to address mathematical queries.\n\nThe combination of these diverse data sources is crucial for training a powerful\nlanguage model that can handle a wide variety of different tasks and text\nformats.\n\n### Data Preprocessing\n\nHere are the key data cleaning and filtering methods applied to the training\ndata:\n\n* CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering was\n  applied at multiple stages in the data preparation process to ensure the\n  exclusion of harmful and illegal content\n* Sensitive Data Filtering: As part of making Gemma pre-trained models safe and\n  reliable, automated techniques were used to filter out certain personal\n  information and other sensitive data from training sets.\n* Additional methods: Filtering based on content quality and safely in line with\n  [our policies](https://storage.googleapis.com/gweb-uniblog-publish-prod/documents/2023_Google_AI_Principles_Progress_Update.pdf#page=11).\n\n## Implementation Information\n\nDetails about the model internals.\n\n### Hardware\n\nGemma was trained using the latest generation of\n[Tensor Processing Unit (TPU)](https://cloud.google.com/tpu/docs/intro-to-tpu) hardware (TPUv5e).\n\nTraining large language models requires significant computational power. TPUs,\ndesigned specifically for matrix operations common in machine learning, offer\nseveral advantages in this domain:\n\n* Performance: TPUs are specifically designed to handle the massive computations\n  involved in training LLMs. They can speed up training considerably compared to\n  CPUs.\n* Memory: TPUs often come with large amounts of high-bandwidth memory, allowing\n  for the handling of large models and batch sizes during training. This can\n  lead to better model quality.\n* Scalability: TPU Pods (large clusters of TPUs) provide a scalable solution for\n  handling the growing complexity of large foundation models. You can distribute\n  training across multiple TPU devices for faster and more efficient processing.\n* Cost-effectiveness: In many scenarios, TPUs can provide a more cost-effective\n  solution for training large models compared to CPU-based infrastructure,\n  especially when considering the time and resources saved due to faster\n  training.\n* These advantages are aligned with\n  [Google's commitments to operate sustainably](https://sustainability.google/operating-sustainably/).\n\n### Software\n\nTraining was done using [JAX](https://github.com/google/jax)\nand [ML Pathways](https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/ml-pathways).\n\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models.\n\nML Pathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\n[foundation models](https://ai.google/discover/foundation-models/), including large language models like\nthese ones.\n\nTogether, JAX and ML Pathways are used as described in the\n[paper about the Gemini family of models](https://arxiv.org/abs/2312.11805); \"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"\n\n## Evaluation\n\nModel evaluation metrics and results.\n\n### Benchmark Results\n\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\n\n| Benchmark                                                                          | Metric        | 2B Params   | 7B Params |\n|------------------------------------------------------------------------------------|---------------|-------------|-----------|\n| [MMLU](https://arxiv.org/abs/2009.03300)                                           | 5-shot, top-1 | 42.3        | 64.3      |\n| [HellaSwag](https://arxiv.org/abs/1905.07830)                                      | 0-shot        | 71.4        | 81.2      |\n| [PIQA](https://arxiv.org/abs/1911.11641)                                           | 0-shot        | 77.3        | 81.2      |\n| [SocialIQA](https://arxiv.org/abs/1904.09728)                                      | 0-shot        | 59.7        | 51.8      |\n| [BooIQ](https://arxiv.org/abs/1905.10044)                                          | 0-shot        | 69.4        | 83.2      |\n| [WinoGrande](https://arxiv.org/abs/1907.10641)                                     | partial score | 65.4        | 72.3      |\n| [CommonsenseQA](https://arxiv.org/abs/1811.00937)                                  | 7-shot        | 65.3        | 71.3      |\n| [OpenBookQA](https://arxiv.org/abs/1809.02789)                                     |               | 47.8        | 52.8      |\n| [ARC-e](https://arxiv.org/abs/1911.01547)                                          |               | 73.2        | 81.5      |\n| [ARC-c](https://arxiv.org/abs/1911.01547)                                          |               | 42.1        | 53.2      |\n| [TriviaQA](https://arxiv.org/abs/1705.03551)                                       | 5-shot        | 53.2        | 63.4      |\n| [Natural Questions](https://github.com/google-research-datasets/natural-questions) | 5-shot        | -           | 23        |\n| [HumanEval](https://arxiv.org/abs/2107.03374)                                      | pass@1        | 22.0        | 32.3      |\n| [MBPP](https://arxiv.org/abs/2108.07732)                                           | 3-shot        | 29.2        | 44.4      |\n| [GSM8K](https://arxiv.org/abs/2110.14168)                                          | maj@1         | 17.7        | 46.4      |\n| [MATH](https://arxiv.org/abs/2108.07732)                                           | 4-shot        | 11.8        | 24.3      |\n| [AGIEval](https://arxiv.org/abs/2304.06364)                                        |               | 24.2        | 41.7      |\n| [BIG-Bench](https://arxiv.org/abs/2206.04615)                                      |               | 35.2        | 55.1      |\n| ------------------------------                                                     | ------------- | ----------- | --------- |\n| **Average**                                                                        |               | **54.0**    | **56.4**  |\n\n## Ethics and Safety\n\nEthics and safety evaluation approach and results.\n\n### Evaluation Approach\n\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\n\n* Text-to-Text Content Safety: Human evaluation on prompts covering safety\n  policies including child sexual abuse and exploitation, harassment, violence\n  and gore, and hate speech.\n* Text-to-Text Representational Harms: Benchmark against relevant academic\n  datasets such as [WinoBias](https://arxiv.org/abs/1804.06876) and [BBQ Dataset](https://arxiv.org/abs/2110.08193v2).\n* Memorization: Automated evaluation of memorization of training data, including\n  the risk of personally identifiable information exposure.\n* Large-scale harm: Tests for \"dangerous capabilities,\" such as chemical,\n  biological, radiological, and nuclear (CBRN) risks.\n\n### Evaluation Results\n\nThe results of ethics and safety evaluations are within acceptable thresholds\nfor\nmeeting [internal policies](https://storage.googleapis.com/gweb-uniblog-publish-prod/documents/2023_Google_AI_Principles_Progress_Update.pdf#page=11)\nfor categories such as child\nsafety, content safety, representational harms, memorization, large-scale harms.\nOn top of robust internal evaluations, the results of well known safety\nbenchmarks like BBQ, BOLD, Winogender, Winobias, RealToxicity, and TruthfulQA\nare shown here.\n\n| Benchmark                                                    | Metric        | 2B Params   | 7B Params |\n|--------------------------------------------------------------|---------------|-------------|-----------|\n| [RealToxicity](https://arxiv.org/abs/2009.11462)             | average       | 6.86        | 7.90      |\n| [BOLD](https://arxiv.org/abs/2101.11718)                     |               | 45.57       | 49.08     |\n| [CrowS-Pairs](https://aclanthology.org/2020.emnlp-main.154/) | top-1         | 45.82       | 51.33     |\n| [BBQ Ambig](https://arxiv.org/abs/2110.08193v2)              | 1-shot, top-1 | 62.58       | 92.54     |\n| [BBQ Disambig](https://arxiv.org/abs/2110.08193v2)           | top-1         | 54.62       | 71.99     |\n| [Winogender](https://arxiv.org/abs/1804.09301)               | top-1         | 51.25       | 54.17     |\n| [TruthfulQA](https://arxiv.org/abs/2109.07958)               |               | 44.84       | 31.81     |\n| [Winobias 1_2](https://arxiv.org/abs/1804.06876)             |               | 56.12       | 59.09     |\n| [Winobias 2_2](https://arxiv.org/abs/1804.06876)             |               | 91.10       | 92.23     |\n| [Toxigen](https://arxiv.org/abs/2203.09509)                  |               | 29.77       | 39.59     |\n| ------------------------------                               | ------------- | ----------- | --------- |\n\n## Usage and Limitations\n\nThese models have certain limitations that users should be aware of.\n\n### Intended Usage\n\nOpen Large Language Models (LLMs) have a wide range of applications across\nvarious industries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\n\n* Content Creation and Communication\n  * Text Generation: These models can be used to generate creative text formats\n    such as poems, scripts, code, marketing copy, and email drafts.\n  * Chatbots and Conversational AI: Power conversational interfaces for customer\n    service, virtual assistants, or interactive applications.\n  * Text Summarization: Generate concise summaries of a text corpus, research\n    papers, or reports.\n* Research and Education\n  * Natural Language Processing (NLP) Research: These models can serve as a\n    foundation for researchers to experiment with NLP techniques, develop\n    algorithms, and contribute to the advancement of the field.\n  * Language Learning Tools: Support interactive language learning experiences,\n    aiding in grammar correction or providing writing practice.\n  * Knowledge Exploration: Assist researchers in exploring large bodies of text\n    by generating summaries or answering questions about specific topics.\n\n### Limitations\n\n* Training Data\n  * The quality and diversity of the training data significantly influence the\n    model's capabilities. Biases or gaps in the training data can lead to\n    limitations in the model's responses.\n  * The scope of the training dataset determines the subject areas the model can\n    handle effectively.\n* Context and Task Complexity\n  * LLMs are better at tasks that can be framed with clear prompts and\n    instructions. Open-ended or highly complex tasks might be challenging.\n  * A model's performance can be influenced by the amount of context provided\n    (longer context generally leads to better outputs, up to a certain point).\n* Language Ambiguity and Nuance\n  * Natural language is inherently complex. LLMs might struggle to grasp subtle\n    nuances, sarcasm, or figurative language.\n* Factual Accuracy\n  * LLMs generate responses based on information they learned from their\n    training datasets, but they are not knowledge bases. They may generate\n    incorrect or outdated factual statements.\n* Common Sense\n  * LLMs rely on statistical patterns in language. They might lack the ability\n    to apply common sense reasoning in certain situations.\n\n### Ethical Considerations and Risks\n\nThe development of large language models (LLMs) raises several ethical concerns.\nIn creating an open model, we have carefully considered the following:\n\n* Bias and Fairness\n  * LLMs trained on large-scale, real-world text data can reflect socio-cultural\n    biases embedded in the training material. These models underwent careful\n    scrutiny, input data pre-processing described and posterior evaluations\n    reported in this card.\n* Misinformation and Misuse\n  * LLMs can be misused to generate text that is false, misleading, or harmful.\n  * Guidelines are provided for responsible use with the model, see the\n    [Responsible Generative AI Toolkit](http://ai.google.dev/gemma/responsible).\n* Transparency and Accountability:\n  * This model card summarizes details on the models' architecture,\n    capabilities, limitations, and evaluation processes.\n  * A responsibly developed open model offers the opportunity to share\n    innovation by making LLM technology accessible to developers and researchers\n    across the AI ecosystem.\n\nRisks identified and mitigations:\n\n* Perpetuation of biases: It's encouraged to perform continuous monitoring\n  (using evaluation metrics, human review) and the exploration of de-biasing\n  techniques during model training, fine-tuning, and other use cases.\n* Generation of harmful content: Mechanisms and guidelines for content safety\n  are essential. Developers are encouraged to exercise caution and implement\n  appropriate content safety safeguards based on their specific product policies\n  and application use cases.\n* Misuse for malicious purposes: Technical limitations and developer and\n  end-user education can help mitigate against malicious applications of LLMs.\n  Educational resources and reporting mechanisms for users to flag misuse are\n  provided. Prohibited uses of Gemma models are outlined in the\n  [Gemma Prohibited Use Policy](https://ai.google.dev/gemma/prohibited_use_policy).\n* Privacy violations: Models were trained on data filtered for removal of PII\n  (Personally Identifiable Information). Developers are encouraged to adhere to\n  privacy regulations with privacy-preserving techniques.\n\n### Benefits\n\nAt the time of release, this family of models provides high-performance open\nlarge language model implementations designed from the ground up for Responsible\nAI development compared to similarly sized models.\n\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.","modelId":1,"dataId":1,"platformId":-1,"users":null,"favorites":null,"views":null,"requests":"0","sort":null,"version":"v1-Thu Feb 29 20:26:09 GMT 2024","tags":["Text2Text Generation"],"playground":false,"createTime":"2024-02-29T20:26:11","updateBy":"google","updateTime":"2024-02-29T20:26:27"},{"id":715,"versionId":4082,"owner":"google","applicationName":"gemma-7b-it","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1708575068467gemma.webp","description":"# Gemma Model Card\n\n**Model Page**: [Gemma](https://ai.google.dev/gemma/docs)\n\nThis model card corresponds to the instruct version of the Gemma model. You can also visit the model card of\nthe [2B base model](https://huggingface.co/google/gemma-2b),\nand [7B base model](https://huggingface.co/google/gemma-7b).\n\n**Resources and Technical Documentation**:\n\n* [Responsible Generative AI Toolkit](https://ai.google.dev/responsible)\n* [Gemma on Kaggle](https://www.kaggle.com/models/google/gemma)\n* [Gemma on Vertex Model Garden](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/335?version=gemma-2b-it-gg-hf)\n\n**Terms of Use**: [Terms](https://www.kaggle.com/models/google/gemma/license/consent)\n\n**Authors**: Google\n\n## Model Information\n\nSummary description and brief definition of inputs and outputs.\n\n### Description\n\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nThey are text-to-text, decoder-only large language models, available in English,\nwith open weights, pre-trained variants, and instruction-tuned variants. Gemma\nmodels are well-suited for a variety of text generation tasks, including\nquestion answering, summarization, and reasoning. Their relatively small size\nmakes it possible to deploy them in environments with limited resources such as\na laptop, desktop or your own cloud infrastructure, democratizing access to\nstate of the art AI models and helping foster innovation for everyone.\n\n### Inputs and outputs\n\n* **Input:** Text string, such as a question, a prompt, or a document to be\n  summarized.\n* **Output:** Generated English-language text in response to the input, such\n  as an answer to a question, or a summary of a document.\n\n## Model Data\n\nData used for model training and how the data was processed.\n\n### Training Dataset\n\nThese models were trained on a dataset of text data that includes a wide variety\nof sources, totaling 6 trillion tokens. Here are the key components:\n\n* Web Documents: A diverse collection of web text ensures the model is exposed\n  to a broad range of linguistic styles, topics, and vocabulary. Primarily\n  English-language content.\n* Code: Exposing the model to code helps it to learn the syntax and patterns of\n  programming languages, which improves its ability to generate code or\n  understand code-related questions.\n* Mathematics: Training on mathematical text helps the model learn logical\n  reasoning, symbolic representation, and to address mathematical queries.\n\nThe combination of these diverse data sources is crucial for training a powerful\nlanguage model that can handle a wide variety of different tasks and text\nformats.\n\n### Data Preprocessing\n\nHere are the key data cleaning and filtering methods applied to the training\ndata:\n\n* CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering was\n  applied at multiple stages in the data preparation process to ensure the\n  exclusion of harmful and illegal content\n* Sensitive Data Filtering: As part of making Gemma pre-trained models safe and\n  reliable, automated techniques were used to filter out certain personal\n  information and other sensitive data from training sets.\n* Additional methods: Filtering based on content quality and safely in line with\n  [our policies](https://storage.googleapis.com/gweb-uniblog-publish-prod/documents/2023_Google_AI_Principles_Progress_Update.pdf#page=11).\n\n## Implementation Information\n\nDetails about the model internals.\n\n### Hardware\n\nGemma was trained using the latest generation of\n[Tensor Processing Unit (TPU)](https://cloud.google.com/tpu/docs/intro-to-tpu) hardware (TPUv5e).\n\nTraining large language models requires significant computational power. TPUs,\ndesigned specifically for matrix operations common in machine learning, offer\nseveral advantages in this domain:\n\n* Performance: TPUs are specifically designed to handle the massive computations\n  involved in training LLMs. They can speed up training considerably compared to\n  CPUs.\n* Memory: TPUs often come with large amounts of high-bandwidth memory, allowing\n  for the handling of large models and batch sizes during training. This can\n  lead to better model quality.\n* Scalability: TPU Pods (large clusters of TPUs) provide a scalable solution for\n  handling the growing complexity of large foundation models. You can distribute\n  training across multiple TPU devices for faster and more efficient processing.\n* Cost-effectiveness: In many scenarios, TPUs can provide a more cost-effective\n  solution for training large models compared to CPU-based infrastructure,\n  especially when considering the time and resources saved due to faster\n  training.\n* These advantages are aligned with\n  [Google's commitments to operate sustainably](https://sustainability.google/operating-sustainably/).\n\n### Software\n\nTraining was done using [JAX](https://github.com/google/jax)\nand [ML Pathways](https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/ml-pathways).\n\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models.\n\nML Pathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\n[foundation models](https://ai.google/discover/foundation-models/), including large language models like\nthese ones.\n\nTogether, JAX and ML Pathways are used as described in the\n[paper about the Gemini family of models](https://arxiv.org/abs/2312.11805); \"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"\n\n## Evaluation\n\nModel evaluation metrics and results.\n\n### Benchmark Results\n\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\n\n| Benchmark                                                                          | Metric        | 2B Params   | 7B Params |\n|------------------------------------------------------------------------------------|---------------|-------------|-----------|\n| [MMLU](https://arxiv.org/abs/2009.03300)                                           | 5-shot, top-1 | 42.3        | 64.3      |\n| [HellaSwag](https://arxiv.org/abs/1905.07830)                                      | 0-shot        | 71.4        | 81.2      |\n| [PIQA](https://arxiv.org/abs/1911.11641)                                           | 0-shot        | 77.3        | 81.2      |\n| [SocialIQA](https://arxiv.org/abs/1904.09728)                                      | 0-shot        | 59.7        | 51.8      |\n| [BooIQ](https://arxiv.org/abs/1905.10044)                                          | 0-shot        | 69.4        | 83.2      |\n| [WinoGrande](https://arxiv.org/abs/1907.10641)                                     | partial score | 65.4        | 72.3      |\n| [CommonsenseQA](https://arxiv.org/abs/1811.00937)                                  | 7-shot        | 65.3        | 71.3      |\n| [OpenBookQA](https://arxiv.org/abs/1809.02789)                                     |               | 47.8        | 52.8      |\n| [ARC-e](https://arxiv.org/abs/1911.01547)                                          |               | 73.2        | 81.5      |\n| [ARC-c](https://arxiv.org/abs/1911.01547)                                          |               | 42.1        | 53.2      |\n| [TriviaQA](https://arxiv.org/abs/1705.03551)                                       | 5-shot        | 53.2        | 63.4      |\n| [Natural Questions](https://github.com/google-research-datasets/natural-questions) | 5-shot        | -           | 23        |\n| [HumanEval](https://arxiv.org/abs/2107.03374)                                      | pass@1        | 22.0        | 32.3      |\n| [MBPP](https://arxiv.org/abs/2108.07732)                                           | 3-shot        | 29.2        | 44.4      |\n| [GSM8K](https://arxiv.org/abs/2110.14168)                                          | maj@1         | 17.7        | 46.4      |\n| [MATH](https://arxiv.org/abs/2108.07732)                                           | 4-shot        | 11.8        | 24.3      |\n| [AGIEval](https://arxiv.org/abs/2304.06364)                                        |               | 24.2        | 41.7      |\n| [BIG-Bench](https://arxiv.org/abs/2206.04615)                                      |               | 35.2        | 55.1      |\n| ------------------------------                                                     | ------------- | ----------- | --------- |\n| **Average**                                                                        |               | **54.0**    | **56.4**  |\n\n## Ethics and Safety\n\nEthics and safety evaluation approach and results.\n\n### Evaluation Approach\n\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\n\n* Text-to-Text Content Safety: Human evaluation on prompts covering safety\n  policies including child sexual abuse and exploitation, harassment, violence\n  and gore, and hate speech.\n* Text-to-Text Representational Harms: Benchmark against relevant academic\n  datasets such as [WinoBias](https://arxiv.org/abs/1804.06876) and [BBQ Dataset](https://arxiv.org/abs/2110.08193v2).\n* Memorization: Automated evaluation of memorization of training data, including\n  the risk of personally identifiable information exposure.\n* Large-scale harm: Tests for \"dangerous capabilities,\" such as chemical,\n  biological, radiological, and nuclear (CBRN) risks.\n\n### Evaluation Results\n\nThe results of ethics and safety evaluations are within acceptable thresholds\nfor\nmeeting [internal policies](https://storage.googleapis.com/gweb-uniblog-publish-prod/documents/2023_Google_AI_Principles_Progress_Update.pdf#page=11)\nfor categories such as child\nsafety, content safety, representational harms, memorization, large-scale harms.\nOn top of robust internal evaluations, the results of well known safety\nbenchmarks like BBQ, BOLD, Winogender, Winobias, RealToxicity, and TruthfulQA\nare shown here.\n\n| Benchmark                                                    | Metric        | 2B Params   | 7B Params |\n|--------------------------------------------------------------|---------------|-------------|-----------|\n| [RealToxicity](https://arxiv.org/abs/2009.11462)             | average       | 6.86        | 7.90      |\n| [BOLD](https://arxiv.org/abs/2101.11718)                     |               | 45.57       | 49.08     |\n| [CrowS-Pairs](https://aclanthology.org/2020.emnlp-main.154/) | top-1         | 45.82       | 51.33     |\n| [BBQ Ambig](https://arxiv.org/abs/2110.08193v2)              | 1-shot, top-1 | 62.58       | 92.54     |\n| [BBQ Disambig](https://arxiv.org/abs/2110.08193v2)           | top-1         | 54.62       | 71.99     |\n| [Winogender](https://arxiv.org/abs/1804.09301)               | top-1         | 51.25       | 54.17     |\n| [TruthfulQA](https://arxiv.org/abs/2109.07958)               |               | 44.84       | 31.81     |\n| [Winobias 1_2](https://arxiv.org/abs/1804.06876)             |               | 56.12       | 59.09     |\n| [Winobias 2_2](https://arxiv.org/abs/1804.06876)             |               | 91.10       | 92.23     |\n| [Toxigen](https://arxiv.org/abs/2203.09509)                  |               | 29.77       | 39.59     |\n| ------------------------------                               | ------------- | ----------- | --------- |\n\n## Usage and Limitations\n\nThese models have certain limitations that users should be aware of.\n\n### Intended Usage\n\nOpen Large Language Models (LLMs) have a wide range of applications across\nvarious industries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\n\n* Content Creation and Communication\n  * Text Generation: These models can be used to generate creative text formats\n    such as poems, scripts, code, marketing copy, and email drafts.\n  * Chatbots and Conversational AI: Power conversational interfaces for customer\n    service, virtual assistants, or interactive applications.\n  * Text Summarization: Generate concise summaries of a text corpus, research\n    papers, or reports.\n* Research and Education\n  * Natural Language Processing (NLP) Research: These models can serve as a\n    foundation for researchers to experiment with NLP techniques, develop\n    algorithms, and contribute to the advancement of the field.\n  * Language Learning Tools: Support interactive language learning experiences,\n    aiding in grammar correction or providing writing practice.\n  * Knowledge Exploration: Assist researchers in exploring large bodies of text\n    by generating summaries or answering questions about specific topics.\n\n### Limitations\n\n* Training Data\n  * The quality and diversity of the training data significantly influence the\n    model's capabilities. Biases or gaps in the training data can lead to\n    limitations in the model's responses.\n  * The scope of the training dataset determines the subject areas the model can\n    handle effectively.\n* Context and Task Complexity\n  * LLMs are better at tasks that can be framed with clear prompts and\n    instructions. Open-ended or highly complex tasks might be challenging.\n  * A model's performance can be influenced by the amount of context provided\n    (longer context generally leads to better outputs, up to a certain point).\n* Language Ambiguity and Nuance\n  * Natural language is inherently complex. LLMs might struggle to grasp subtle\n    nuances, sarcasm, or figurative language.\n* Factual Accuracy\n  * LLMs generate responses based on information they learned from their\n    training datasets, but they are not knowledge bases. They may generate\n    incorrect or outdated factual statements.\n* Common Sense\n  * LLMs rely on statistical patterns in language. They might lack the ability\n    to apply common sense reasoning in certain situations.\n\n### Ethical Considerations and Risks\n\nThe development of large language models (LLMs) raises several ethical concerns.\nIn creating an open model, we have carefully considered the following:\n\n* Bias and Fairness\n  * LLMs trained on large-scale, real-world text data can reflect socio-cultural\n    biases embedded in the training material. These models underwent careful\n    scrutiny, input data pre-processing described and posterior evaluations\n    reported in this card.\n* Misinformation and Misuse\n  * LLMs can be misused to generate text that is false, misleading, or harmful.\n  * Guidelines are provided for responsible use with the model, see the\n    [Responsible Generative AI Toolkit](http://ai.google.dev/gemma/responsible).\n* Transparency and Accountability:\n  * This model card summarizes details on the models' architecture,\n    capabilities, limitations, and evaluation processes.\n  * A responsibly developed open model offers the opportunity to share\n    innovation by making LLM technology accessible to developers and researchers\n    across the AI ecosystem.\n\nRisks identified and mitigations:\n\n* Perpetuation of biases: It's encouraged to perform continuous monitoring\n  (using evaluation metrics, human review) and the exploration of de-biasing\n  techniques during model training, fine-tuning, and other use cases.\n* Generation of harmful content: Mechanisms and guidelines for content safety\n  are essential. Developers are encouraged to exercise caution and implement\n  appropriate content safety safeguards based on their specific product policies\n  and application use cases.\n* Misuse for malicious purposes: Technical limitations and developer and\n  end-user education can help mitigate against malicious applications of LLMs.\n  Educational resources and reporting mechanisms for users to flag misuse are\n  provided. Prohibited uses of Gemma models are outlined in the\n  [Gemma Prohibited Use Policy](https://ai.google.dev/gemma/prohibited_use_policy).\n* Privacy violations: Models were trained on data filtered for removal of PII\n  (Personally Identifiable Information). Developers are encouraged to adhere to\n  privacy regulations with privacy-preserving techniques.\n\n### Benefits\n\nAt the time of release, this family of models provides high-performance open\nlarge language model implementations designed from the ground up for Responsible\nAI development compared to similarly sized models.\n\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.","modelId":1,"dataId":1,"platformId":-1,"users":null,"favorites":null,"views":null,"requests":"0","sort":null,"version":"v1-Thu Feb 29 20:26:00 GMT 2024","tags":["Text2Text Generation"],"playground":false,"createTime":"2024-02-29T20:26:12","updateBy":"google","updateTime":"2024-02-29T20:26:27"}]}}