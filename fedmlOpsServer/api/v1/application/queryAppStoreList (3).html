{"message":"Succeeded to process
request","code":"SUCCESS","data":{"total":1,"totalPage":1,"pageNum":1,"pageSize":16,"data":[{"id":900,"versionId":4387,"owner":"llava-hf","applicationName":"vip-llava-13b-hf","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/17103045511651%2AaSX0L_msIXE5Wdwn3phPuQ.png","description":"LLaVa
is an open-source chatbot trained by fine-tuning LlamA/Vicuna on GPT-generated
multimodal instruction-following data. It is an auto-regressive language model,
based on the transformer architecture. In other words, it is an multi-modal
version of LLMs fine-tuned for chat / instructions.\n\nThe LLaVa model was
proposed in **[Visual Instruction Tuning](https://arxiv.org/abs/2304.08485) and
improved in **[Improved Baselines with Visual Instruction
Tuning](https://arxiv.org/pdf/2310.03744)by Haotian Liu, Chunyuan Li, Yuheng Li
and Yong Jae Lee.\n\nThe abstract from the paper is the following:\n\n*Large
multimodal models (LMM) have recently shown encouraging progress with visual
instruction tuning. In this note, we show that the fully-connected
vision-language cross-modal connector in LLaVA is surprisingly powerful and
data-efficient. With simple modifications to LLaVA, namely, using
CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA
data with simple response formatting prompts, we establish stronger baselines
that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint
uses merely 1.2M publicly available data, and finishes full training in âˆ¼1 day
on a single 8-A100 node. We hope this can make state-of-the-art LMM research
more
accessible.*\n\n![https://fedml.s3.us-west-1.amazonaws.com/1710304224153llava_architecture.jpg](https://fedml.s3.us-west-1.amazonaws.com/1710304224153llava_architecture.jpg)\n\nLLaVa
architecture. Taken from the [original
paper.](https://arxiv.org/abs/2304.08485)","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":3,"views":259,"requests":"71","sort":null,"version":"v1-Wed
Mar 13 04:36:17 GMT 2024","tags":["Image Text to
Text"],"playground":false,"createTime":"2024-03-13T06:02:16","updateBy":"llava-hf","updateTime":"2024-03-13T06:24:28"}]}}
