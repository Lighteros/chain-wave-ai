{"message":"Succeeded to process request","code":"SUCCESS","data":{"total":46,"totalPage":3,"pageNum":2,"pageSize":16,"data":[{"id":1013,"versionId":5536,"owner":"openai","applicationName":"whisper-large-v3","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1712548813599images.png","description":"whisper-large-v3","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":0,"views":183,"requests":"176","sort":null,"version":"v8-Mon Aug 12 20:54:21 GMT 2024","tags":["Automatic Speech Recognition"],"playground":false,"createTime":"2024-08-12T21:45:44","updateBy":"openai","updateTime":"2024-08-12T21:49:54"},{"id":970,"versionId":4834,"owner":"suno","applicationName":"bark","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/171160713973399442120.png","description":"# \uD83D\uDC36 Bark\n\n[![](https://dcbadge.vercel.app/api/server/J2B2vsjKuE?style=flat&compact=True)](https://suno.ai/discord)\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/FM.svg?style=social&label=@suno_ai_)](https://twitter.com/suno_ai_)\n\n> \uD83D\uDD17 [Examples](https://suno.ai/examples/bark-v0) • [Suno Studio Waitlist](https://suno-ai.typeform.com/suno-studio) • [Updates](#-updates) • [How to Use](#-usage-in-python) • [Installation](#-installation) • [FAQ](#-faq)\n\n<br>\n\n<p align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/5068315/235310676-a4b3b511-90ec-4edf-8153-7ccf14905d73.png\" width=\"500\"></img>\n</p>\n<br>\n\nBark is a transformer-based text-to-audio model created by [Suno](https://suno.ai). Bark can generate highly realistic, multilingual speech as well as other audio - including music, background noise and simple sound effects. The model can also produce nonverbal communications like laughing, sighing and crying. To support the research community, we are providing access to pretrained model checkpoints, which are ready for inference and available for commercial use.\n\n## ⚠ Disclaimer\n\nBark was developed for research purposes. It is not a conventional text-to-speech model but instead a fully generative text-to-audio model, which can deviate in unexpected ways from provided prompts. Suno does not take responsibility for any output generated. Use at your own risk, and please act responsibly.\n\n## \uD83D\uDCD6 Quick Index\n\n* [\uD83D\uDE80 Updates](#-updates)\n* [\uD83D\uDCBB Installation](#-installation)\n* [\uD83D\uDC0D Usage](#-usage-in-python)\n* [\uD83C\uDF00 Live Examples](https://suno.ai/examples/bark-v0)\n* [❓ FAQ](#-faq)\n\n## \uD83C\uDFA7 Demos\n\n[![Open in Spaces](https://img.shields.io/badge/\uD83E\uDD17-Open%20in%20Spaces-blue.svg)](https://huggingface.co/spaces/suno/bark)\n[![Open on Replicate](https://img.shields.io/badge/®️-Open%20on%20Replicate-blue.svg)](https://replicate.com/suno-ai/bark)\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1eJfA2XUa-mXwdMy7DoYKVYHI1iTd9Vkt?usp=sharing)\n\n## \uD83D\uDE80 Updates\n\n**2023.05.01**\n\n- ©️ Bark is now licensed under the MIT License, meaning it's now available for commercial use!\n- ⚡ 2x speed-up on GPU. 10x speed-up on CPU. We also added an option for a smaller version of Bark, which offers additional speed-up with the trade-off of slightly lower quality.\n- \uD83D\uDCD5 [Long-form generation](notebooks/long_form_generation.ipynb), voice consistency enhancements and other examples are now documented in a new [notebooks](./notebooks) section.\n- \uD83D\uDC65 We created a [voice prompt library](https://suno-ai.notion.site/8b8e8749ed514b0cbf3f699013548683?v=bc67cff786b04b50b3ceb756fd05f68c). We hope this resource helps you find useful prompts for your use cases! You can also join us on [Discord](https://suno.ai/discord), where the community actively shares useful prompts in the **#audio-prompts** channel.\n- \uD83D\uDCAC Growing community support and access to new features here:\n  \n  [![](https://dcbadge.vercel.app/api/server/J2B2vsjKuE)](https://suno.ai/discord)\n- \uD83D\uDCBE You can now use Bark with GPUs that have low VRAM (<4GB).\n\n**2023.04.20**\n\n- \uD83D\uDC36 Bark release!\n\n## \uD83D\uDC0D Usage in Python\n\n<details open>\n  <summary><h3>\uD83E\uDE91 Basics</h3></summary>\n\n```python\nfrom bark import SAMPLE_RATE, generate_audio, preload_models\nfrom scipy.io.wavfile import write as write_wav\nfrom IPython.display import Audio\n\n# download and load all models\npreload_models()\n\n# generate audio from text\ntext_prompt = \"\"\"\n     Hello, my name is Suno. And, uh — and I like pizza. [laughs] \n     But I also have other interests such as playing tic tac toe.\n\"\"\"\naudio_array = generate_audio(text_prompt)\n\n# save audio to disk\nwrite_wav(\"bark_generation.wav\", SAMPLE_RATE, audio_array)\n  \n# play text in notebook\nAudio(audio_array, rate=SAMPLE_RATE)\n```\n\n[pizza1.webm](https://user-images.githubusercontent.com/34592747/cfa98e54-721c-4b9c-b962-688e09db684f.webm)\n\n</details>\n\n<details open>\n  <summary><h3>\uD83C\uDF0E Foreign Language</h3></summary>\n<br>\nBark supports various languages out-of-the-box and automatically determines language from input text. When prompted with code-switched text, Bark will attempt to employ the native accent for the respective languages. English quality is best for the time being, and we expect other languages to further improve with scaling. \n<br>\n<br>\n\n```python\ntext_prompt = \"\"\"\n    추석은 내가 가장 좋아하는 명절이다. 나는 며칠 동안 휴식을 취하고 친구 및 가족과 시간을 보낼 수 있습니다.\n\"\"\"\naudio_array = generate_audio(text_prompt)\n```\n\n[suno_korean.webm](https://user-images.githubusercontent.com/32879321/235313033-dc4477b9-2da0-4b94-9c8b-a8c2d8f5bb5e.webm)\n\n*Note: since Bark recognizes languages automatically from input text, it is possible to use, for example, a german history prompt with english text. This usually leads to english audio with a german accent.*\n\n```python\ntext_prompt = \"\"\"\n    Der Dreißigjährige Krieg (1618-1648) war ein verheerender Konflikt, der Europa stark geprägt hat.\n    This is a beginning of the history. If you want to hear more, please continue.\n\"\"\"\naudio_array = generate_audio(text_prompt)\n```\n\n[suno_german_accent.webm](https://user-images.githubusercontent.com/34592747/3f96ab3e-02ec-49cb-97a6-cf5af0b3524a.webm)\n\n</details>\n\n<details open>\n  <summary><h3>\uD83C\uDFB6 Music</h3></summary>\nBark can generate all types of audio, and, in principle, doesn't see a difference between speech and music. Sometimes Bark chooses to generate text as music, but you can help it out by adding music notes around your lyrics.\n<br>\n<br>\n\n```python\ntext_prompt = \"\"\"\n    ♪ In the jungle, the mighty jungle, the lion barks tonight ♪\n\"\"\"\naudio_array = generate_audio(text_prompt)\n```\n\n[lion.webm](https://user-images.githubusercontent.com/5068315/230684766-97f5ea23-ad99-473c-924b-66b6fab24289.webm)\n\n</details>\n\n<details open>\n<summary><h3>\uD83C\uDFA4 Voice Presets</h3></summary>\n\nBark supports 100+ speaker presets across [supported languages](#supported-languages). You can browse the library of supported voice presets [HERE](https://suno-ai.notion.site/8b8e8749ed514b0cbf3f699013548683?v=bc67cff786b04b50b3ceb756fd05f68c), or in the [code](bark/assets/prompts). The community also often shares presets in [Discord](https://discord.gg/J2B2vsjKuE).\n\n> Bark tries to match the tone, pitch, emotion and prosody of a given preset, but does not currently support custom voice cloning. The model also attempts to preserve music, ambient noise, etc.\n\n```python\ntext_prompt = \"\"\"\n    I have a silky smooth voice, and today I will tell you about \n    the exercise regimen of the common sloth.\n\"\"\"\naudio_array = generate_audio(text_prompt, history_prompt=\"v2/en_speaker_1\")\n```\n\n[sloth.webm](https://user-images.githubusercontent.com/5068315/230684883-a344c619-a560-4ff5-8b99-b4463a34487b.webm)\n\n</details>\n\n### \uD83D\uDCC3 Generating Longer Audio\n\nBy default, `generate_audio` works well with around 13 seconds of spoken text. For an example of how to do long-form generation, see \uD83D\uDC49 **[Notebook](notebooks/long_form_generation.ipynb)** \uD83D\uDC48\n\n<details>\n<summary>Click to toggle example long-form generations (from the example notebook)</summary>\n\n[dialog.webm](https://user-images.githubusercontent.com/2565833/235463539-f57608da-e4cb-4062-8771-148e29512b01.webm)\n\n[longform_advanced.webm](https://user-images.githubusercontent.com/2565833/235463547-1c0d8744-269b-43fe-9630-897ea5731652.webm)\n\n[longform_basic.webm](https://user-images.githubusercontent.com/2565833/235463559-87efe9f8-a2db-4d59-b764-57db83f95270.webm)\n\n</details>\n\n## Command line\n\n```commandline\npython -m bark --text \"Hello, my name is Suno.\" --output_filename \"example.wav\"\n```\n\n## \uD83D\uDCBB Installation\n\n*‼️ CAUTION ‼️ Do NOT use `pip install bark`. It installs a different package, which is not managed by Suno.*\n\n```bash\npip install git+https://github.com/suno-ai/bark.git\n```\n\nor\n\n```bash\ngit clone https://github.com/suno-ai/bark\ncd bark && pip install .\n```\n\n## \uD83E\uDD17 Transformers Usage\n\nBark is available in the \uD83E\uDD17 Transformers library from version 4.31.0 onwards, requiring minimal dependencies\nand additional packages. Steps to get started:\n\n1. First install the \uD83E\uDD17 [Transformers library](https://github.com/huggingface/transformers) from main:\n\n```\npip install git+https://github.com/huggingface/transformers.git\n```\n\n2. Run the following Python code to generate speech samples:\n\n```py\nfrom transformers import AutoProcessor, BarkModel\n\nprocessor = AutoProcessor.from_pretrained(\"suno/bark\")\nmodel = BarkModel.from_pretrained(\"suno/bark\")\n\nvoice_preset = \"v2/en_speaker_6\"\n\ninputs = processor(\"Hello, my dog is cute\", voice_preset=voice_preset)\n\naudio_array = model.generate(**inputs)\naudio_array = audio_array.cpu().numpy().squeeze()\n```\n\n3. Listen to the audio samples either in an ipynb notebook:\n\n```py\nfrom IPython.display import Audio\n\nsample_rate = model.generation_config.sample_rate\nAudio(audio_array, rate=sample_rate)\n```\n\nOr save them as a `.wav` file using a third-party library, e.g. `scipy`:\n\n```py\nimport scipy\n\nsample_rate = model.generation_config.sample_rate\nscipy.io.wavfile.write(\"bark_out.wav\", rate=sample_rate, data=audio_array)\n```\n\nFor more details on using the Bark model for inference using the \uD83E\uDD17 Transformers library, refer to the\n[Bark docs](https://huggingface.co/docs/transformers/main/en/model_doc/bark) or the hands-on\n[Google Colab](https://colab.research.google.com/drive/1dWWkZzvu7L9Bunq9zvD-W02RFUXoW-Pd?usp=sharing).\n\n## \uD83D\uDEE0️ Hardware and Inference Speed\n\nBark has been tested and works on both CPU and GPU (`pytorch 2.0+`, CUDA 11.7 and CUDA 12.0).\n\nOn enterprise GPUs and PyTorch nightly, Bark can generate audio in roughly real-time. On older GPUs, default colab, or CPU, inference time might be significantly slower. For older GPUs or CPU you might want to consider using smaller models. Details can be found in out tutorial sections here.\n\nThe full version of Bark requires around 12GB of VRAM to hold everything on GPU at the same time.\nTo use a smaller version of the models, which should fit into 8GB VRAM, set the environment flag `SUNO_USE_SMALL_MODELS=True`.\n\nIf you don't have hardware available or if you want to play with bigger versions of our models, you can also sign up for early access to our model playground [here](https://suno-ai.typeform.com/suno-studio).\n\n## ⚙️ Details\n\nBark is fully generative text-to-audio model devolved for research and demo purposes. It follows a GPT style architecture similar to [AudioLM](https://arxiv.org/abs/2209.03143) and [Vall-E](https://arxiv.org/abs/2301.02111) and a quantized Audio representation from [EnCodec](https://github.com/facebookresearch/encodec). It is not a conventional TTS model, but instead a fully generative text-to-audio model capable of deviating in unexpected ways from any given script. Different to previous approaches, the input text prompt is converted directly to audio without the intermediate use of phonemes. It can therefore generalize to arbitrary instructions beyond speech such as music lyrics, sound effects or other non-speech sounds.\n\nBelow is a list of some known non-speech sounds, but we are finding more every day. Please let us know if you find patterns that work particularly well on [Discord](https://suno.ai/discord)!\n\n- `[laughter]`\n- `[laughs]`\n- `[sighs]`\n- `[music]`\n- `[gasps]`\n- `[clears throat]`\n- `—` or `...` for hesitations\n- `♪` for song lyrics\n- CAPITALIZATION for emphasis of a word\n- `[MAN]` and `[WOMAN]` to bias Bark toward male and female speakers, respectively\n\n### Supported Languages\n\n| Language | Status |\n| --- | :---: |\n| English (en) | ✅ |\n| German (de) | ✅ |\n| Spanish (es) | ✅ |\n| French (fr) | ✅ |\n| Hindi (hi) | ✅ |\n| Italian (it) | ✅ |\n| Japanese (ja) | ✅ |\n| Korean (ko) | ✅ |\n| Polish (pl) | ✅ |\n| Portuguese (pt) | ✅ |\n| Russian (ru) | ✅ |\n| Turkish (tr) | ✅ |\n| Chinese, simplified (zh) | ✅ |\n\nRequests for future language support [here](https://github.com/suno-ai/bark/discussions/111) or in the **#forums** channel on [Discord](https://suno.ai/discord).\n\n## \uD83D\uDE4F Appreciation\n\n- [nanoGPT](https://github.com/karpathy/nanoGPT) for a dead-simple and blazing fast implementation of GPT-style models\n- [EnCodec](https://github.com/facebookresearch/encodec) for a state-of-the-art implementation of a fantastic audio codec\n- [AudioLM](https://github.com/lucidrains/audiolm-pytorch) for related training and inference code\n- [Vall-E](https://arxiv.org/abs/2301.02111), [AudioLM](https://arxiv.org/abs/2209.03143) and many other ground-breaking papers that enabled the development of Bark\n\n## © License\n\nBark is licensed under the MIT License.\n\nPlease contact us at \uD83D\uDCE7 [bark@suno.ai](mailto:bark@suno.ai) to request access to a larger version of the model.\n\n## \uD83D\uDCF1 Community\n\n- [Twitter](https://twitter.com/suno_ai_)\n- [Discord](https://suno.ai/discord)\n\n## \uD83C\uDFA7 Suno Studio (Early Access)\n\nWe’re developing a playground for our models, including Bark.\n\nIf you are interested, you can sign up for early access [here](https://suno-ai.typeform.com/suno-studio).\n\n## ❓ FAQ\n\n#### How do I specify where models are downloaded and cached?\n\n* Bark uses Hugging Face to download and store models. You can see find more info [here](https://huggingface.co/docs/huggingface_hub/package_reference/environment_variables#hfhome).\n\n#### Bark's generations sometimes differ from my prompts. What's happening?\n\n* Bark is a GPT-style model. As such, it may take some creative liberties in its generations, resulting in higher-variance model outputs than traditional text-to-speech approaches.\n\n#### What voices are supported by Bark?\n\n* Bark supports 100+ speaker presets across [supported languages](#supported-languages). You can browse the library of speaker presets [here](https://suno-ai.notion.site/8b8e8749ed514b0cbf3f699013548683?v=bc67cff786b04b50b3ceb756fd05f68c). The community also shares presets in [Discord](https://suno.ai/discord). Bark also supports generating unique random voices that fit the input text. Bark does not currently support custom voice cloning.\n\n#### Why is the output limited to ~13-14 seconds?\n\n* Bark is a GPT-style model, and its architecture/context window is optimized to output generations with roughly this length.\n\n#### How much VRAM do I need?\n\n* The full version of Bark requires around 12Gb of memory to hold everything on GPU at the same time. However, even smaller cards down to ~2Gb work with some additional settings. Simply add the following code snippet before your generation:\n\n```python\nimport os\nos.environ[\"SUNO_OFFLOAD_CPU\"] = \"True\"\nos.environ[\"SUNO_USE_SMALL_MODELS\"] = \"True\"\n```\n\n#### My generated audio sounds like a 1980s phone call. What's happening?\n\n* Bark generates audio from scratch. It is not meant to create only high-fidelity, studio-quality speech. Rather, outputs could be anything from perfect speech to multiple people arguing at a baseball game recorded with bad microphones.\n\n[//]: br","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":2,"views":220,"requests":"0","sort":null,"version":"v7-Tue Apr 09 22:17:35 GMT 2024","tags":["Text to Audio"],"playground":false,"createTime":"2024-04-09T22:17:54","updateBy":"suno","updateTime":"2024-04-09T22:23:53"},{"id":1020,"versionId":4863,"owner":"Equall","applicationName":"Saul-Instruct-v1","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1712798254124equallLogo.png","description":"# Equall/Saul-Instruct-v1\n\nThis is the instruct model for Equall/Saul-Instruct-v1, a large instruct language model tailored for Legal domain. This model is obtained by continue pretraining of Mistral-7B.\n\nCheckout our website and register** **[https://equall.ai/](https://equall.ai/)\n\n[![image/png](https://cdn-uploads.huggingface.co/production/uploads/644a900e3a619fe72b14af0f/OU4Y3s-WckYKMN4fQkNiS.png)](https://cdn-uploads.huggingface.co/production/uploads/644a900e3a619fe72b14af0f/OU4Y3s-WckYKMN4fQkNiS.png)\n\n## Model Details\n\n### Model Description\n\nThis is the model card of a \uD83E\uDD17 transformers model that has been pushed on the Hub. This model card has been automatically generated.\n\n* **Developed by:** Equall.ai in collaboration with CentraleSupelec, Sorbonne Université, Instituto Superior Técnico and NOVA School of Law\n* **Model type:** 7B\n* **Language(s) (NLP):** English\n* **License:** MIT\n\n### Model Sources\n\n* **Paper:** [https://arxiv.org/abs/2403.03883](https://arxiv.org/abs/2403.03883)\n\n## Uses\n\nYou can use it for legal use cases that involves generation.\n\nHere's how you can run the model using the pipeline() function from \uD83E\uDD17 Transformers:\n\n```python\n# Install transformers from source - only needed for versions <= v4.34\n# pip install git+https://github.com/huggingface/transformers.git\n# pip install accelerate\n\nimport torch\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"Equall/Saul-Instruct-v1\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n# We use the tokenizer’s chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [\n    {\"role\": \"user\", \"content\": \"[YOUR QUERY GOES HERE]\"},\n]\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256, do_sample=False)\nprint(outputs[0][\"generated_text\"])\n```\n\n## Bias, Risks, and Limitations\n\nThis model is built upon the technology of LLM, which comes with inherent limitations. It may occasionally generate inaccurate or nonsensical outputs. Furthermore, being a 7B model, it's anticipated to exhibit less robust performance compared to larger models, such as the 70B variant.\n\n## Citation\n\n**BibTeX:**\n\n```bibtex\n@misc{colombo2024saullm7b,\n      title={SaulLM-7B: A pioneering Large Language Model for Law}, \n      author={Pierre Colombo and Telmo Pessoa Pires and Malik Boudiaf and Dominic Culver and Rui Melo and Caio Corro and Andre F. T. Martins and Fabrizio Esposito and Vera Lúcia Raposo and Sofia Morgado and Michael Desa},\n      year={2024},\n      eprint={2403.03883},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":2,"views":109,"requests":"0","sort":null,"version":"v3-Thu Apr 11 01:17:36 GMT 2024","tags":["Text2Text Generation"],"playground":false,"createTime":"2024-04-11T01:17:38","updateBy":"Equall","updateTime":"2024-04-11T01:17:43"},{"id":966,"versionId":5270,"owner":"stabilityai","applicationName":"stable-video-diffusion-img2vid-xt","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/17114984162561%2AVQ1bJb46YRLkwn26rNzhhw.png","description":"Today, we are releasing Stable Video Diffusion, our first foundation model for generative video based on the image model Stable Diffusion.\n\nNow available in research preview, this state-of-the-art generative AI video model represents a significant step in our journey toward creating models for everyone of every type.\n\nWith this research release, we have made the code for Stable Video Diffusion available on our [GitHub repository\\*\\* \\*\\*& t](https://github.com/Stability-AI/generative-models)he weights required to run the model locally can be found on our [Hugging Face page](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt). Further details regarding the technical capabilities of the model can be found in our [research paper](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n\\*\\*Adaptable to Numerous Video Applications\\*\\*\n\nOur video model can be easily adapted to various downstream tasks, including multi-view synthesis from a single image with finetuning on multi-view datasets. We are planning a variety of models that build on and extend this base, similar to the ecosystem that has built around stable diffusion.\n\nSample multi-view generations from our finetuned video model\n\nIn addition, today, you can sign up for our waitlist [here](https://stability.ai/contact) to access a new upcoming web experience featuring a Text-To-Video interface. This tool showcases the practical applications of Stable Video Diffusion in numerous sectors, including Advertising, Education, Entertainment, and beyond.\n\n<iframe width=\"200\" height=\"113\" src=\"https://www.youtube.com/embed/G7mihAy691g?feature=oembed&amp;enablejsapi=1\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen=\"\" title=\"Stable Video Diffusion\" id=\"yui\\_3\\_17\\_2\\_1\\_1711754052756\\_85\" data-gtm-yt-inspected-12=\"true\"></iframe>\n\n\\*\\*Competitive in Performance\\*\\*\n\nStable Video Diffusion is released in the form of two image-to-video models, capable of generating 14 and 25 frames at customizable frame rates between 3 and 30 frames per second. At the time of release in their foundational form, through external evaluation, we have found these models surpass the leading closed models in user preference studies.\n\n![](https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/02e26394-e36b-4399-bacf-fea2bb26f6bd/Graph+SVD+v+Competition0.jpg)\n\n\\*\\*Exclusively for Research\\*\\*\n\nWhile we eagerly update our models with the latest advancements and work to incorporate your feedback, we emphasize that this model is not intended for real-world or commercial applications at this stage. Your insights and feedback on safety and quality are important to refining this model for its eventual release.\n\nThis aligns with our previous releases in new modalities, and we look forward to sharing the full release with you all.\n\n\\*\\*Our Ever-Expanding Suite of AI Models\\*\\*\n\nStable Video Diffusion is a proud addition to our diverse range of open-source models. Spanning across modalities including image, language, audio, 3D, and code, our portfolio is a testament to Stability AI’s dedication to amplifying human intelligence.\n\nStay updated on our progress by signing up for our [newsletter](https://stability.ai/home#newsletter) and discovering more about commercial applications by contacting us [here](https://stability.ai/contact).\n\nFollow us on[ Twitter](https://twitter.com/stabilityai),[ Instagram](https://www.instagram.com/stability.ai/), [LinkedIn](https://www.linkedin.com/company/stability-ai), and join our[ Discord Community](https://discord.gg/stablediffusion).","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":2,"views":212,"requests":"0","sort":null,"version":"v7-Fri Mar 29 23:14:58 GMT 2024","tags":["Image to Video"],"playground":false,"createTime":"2024-05-30T23:18:23","updateBy":"stabilityai","updateTime":"2024-05-30T23:27:35"},{"id":975,"versionId":4685,"owner":"databricks","applicationName":"dbrx-instruct","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1711677242839databricks-partner.jpg","description":"# DBRX\n\nDBRX is a large language model trained by Databricks, and made available under an open license. This repository contains the minimal code and examples to run inference, as well as a collection of resources and links for using DBRX.\n\n* [Founder's Blog](https://www.databricks.com/blog/announcing-dbrx-new-standard-efficient-open-source-customizable-llms), [DBRX Technical Blog](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm)\n* HuggingFace: https://huggingface.co/collections/databricks/\n* LLM Foundry: https://github.com/mosaicml/llm-foundry\n\nA reference model code can be found in this repository at [modeling_dbrx.py](model/modeling_dbrx.py).\n\n**Note:** this model code is supplied for references purposes only, please see the [HuggingFace](https://huggingface.co/collections/databricks/) repository for the official supported version.\n\n## Model details\n\nDBRX is a Mixture-of-Experts (MoE) model with 132B total parameters and 36B live parameters. We use 16 experts, of which 4 are active during training or inference. DBRX was pre-trained for 12T tokens of text. DBRX has a context length of 32K tokens.\n\nThe following models are open-sourced:\n\n| Model                                                            | Description                               |\n|------------------------------------------------------------------|-------------------------------------------|\n| [DBRX Base](https://huggingface.co/databricks/dbrx-base)         | Pre-trained base model                    |\n| [DBRX Instruct](https://huggingface.co/databricks/dbrx-instruct) | Finetuned model for instruction following |\n\nThe model was trained using optimized versions of our open source libraries [Composer](https://www.github.com/mosaicml/composer), [LLM Foundry](https://www.github.com/mosaicml/llm-foundry), [MegaBlocks](https://github.com/databricks/megablocks) and [Streaming](https://github.com/mosaicml/streaming).\n\nFor the instruct model, we used the ChatML format. Please see the [DBRX Instruct model card](./MODEL_CARD_dbrx_instruct.md) for more information on this.\n\n## Quick start\n\nTo download the weights and tokenizer, please first visit the DBRX HuggingFace page and accept the license. Note: access to the Base model requires manual approval.\n\nWe recommend having at least 320GB of memory to run the model.\n\nThen, run:\n\n```\npip install -r requirements.txt # Or requirements-gpu.txt to use flash attention on GPU(s)\nhuggingface-cli login           # Add your Hugging Face token in order to access the model\npython generate.py              # See generate.py to change the prompt and other settings\n```\n\nFor more advanced usage, please see LLM Foundry ([chat script](https://github.com/mosaicml/llm-foundry/blob/main/scripts/inference/hf_chat.py), [batch generation script](https://github.com/mosaicml/llm-foundry/blob/main/scripts/inference/hf_generate.py))\n\nIf you have any package installation issues, we recommend using our Docker image: [`mosaicml/llm-foundry:2.2.1_cu121_flash2-latest`](https://github.com/mosaicml/llm-foundry?tab=readme-ov-file#mosaicml-docker-images)\n\n## Inference\n\nBoth TensorRT-LLM and vLLM can be used to run optimized inference with DBRX. We have tested both libraries on NVIDIA A100 and H100 systems. To run inference with 16-bit precision, a minimum of 4 x 80GB multi-GPU system is required.\n\n### TensorRT-LLM\n\nDBRX support is being added to TensorRT-LLM library: [Pending PR](https://github.com/NVIDIA/TensorRT-LLM/pull/1363)\n\nAfter merging, instructions to build and run DBRX TensorRT engines will be found at: [README](https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/dbrx/README.md)\n\n### vLLM\n\nPlease see the [vLLM docs](https://docs.vllm.ai/en/latest/) for instructions on how to run DBRX with the vLLM engine.\n\n## Finetune\n\nAn example script to finetune DBRX can be found in our open source library [LLM Foundry](https://www.github.com/mosaicml/llm-foundry)\n\n## Model card\n\nThe model cards can be found at:\n\n* [DBRX Base](MODEL_CARD_dbrx_base.md)\n* [DBRX Instruct](MODEL_CARD_dbrx_instruct.md)\n\n## Integrations\n\nDBRX is available on the Databricks platform through:\n\n* [Mosaic AI Model Serving](https://docs.databricks.com/machine-learning/foundation-models/supported-models.html#dbrx-instruct)\n* [Mosaic AI Playground](https://docs.databricks.com/en/large-language-models/ai-playground.html)\n\nThe same tools used to train high quality MoE models such as DBRX are available for Databricks customers. Please reach out to us at https://www.databricks.com/company/contact if you are interested in pre-training, finetuning, or deploying your own DBRX models!\n\n## Issues\n\nFor issues with model output, or community discussion, please use the Hugging Face community forum ([instruct](https://huggingface.co/databricks/dbrx-instruct), [base](https://huggingface.co/databricks/dbrx-base))\n\nFor issues with LLM Foundry, or any of the underlying training libraries, please open an issue on the relevant GitHub repository.\n\n## License\n\nOur model weights and code are licensed for both researchers and commercial entities. The [Databricks Open Source License](https://www.databricks.com/legal/open-model-license) can be found at [LICENSE](LICENSE), and our Acceptable Use Policy can be found [here](https://www.databricks.com/legal/acceptable-use-policy-open-model).","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":1,"views":190,"requests":"0","sort":null,"version":"v1-Fri Mar 29 01:54:08 GMT 2024","tags":["Text2Text Generation"],"playground":false,"createTime":"2024-03-29T01:54:25","updateBy":"databricks","updateTime":"2024-03-29T04:37:26"},{"id":980,"versionId":4701,"owner":"deepseek-ai","applicationName":"deepseek-coder-6-7b-instruct","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1711747895103Uk1zNOj4_400x400.jpg","description":"### 1. Introduction of Deepseek Coder\n\nDeepseek Coder is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese. We provide various sizes of the code model, ranging from 1B to 33B versions. Each model is pre-trained on project-level code corpus by employing a window size of 16K and a extra fill-in-the-blank task, to support  project-level code completion and infilling. For coding capabilities, Deepseek Coder achieves state-of-the-art performance among open-source code models on multiple programming languages and various benchmarks.\n\n- **Massive Training Data**: Trained from scratch fon 2T tokens, including 87% code and 13% linguistic data in both English and Chinese languages.\n- **Highly Flexible & Scalable**: Offered in model sizes of 1.3B, 5.7B, 6.7B, and 33B, enabling users to choose the setup most suitable for their requirements.\n- **Superior Model Performance**: State-of-the-art performance among publicly available code models on HumanEval, MultiPL-E, MBPP, DS-1000, and APPS benchmarks.\n- **Advanced Code Completion Capabilities**: A window size of 16K and a fill-in-the-blank task, supporting project-level code completion and infilling tasks.\n\n### 2. Model Summary\n\ndeepseek-coder-6.7b-instruct is a 6.7B parameter model initialized from deepseek-coder-6.7b-base and fine-tuned on 2B tokens of instruction data.\n\n- **Home Page:** [DeepSeek](https://deepseek.com/)\n- **Repository:** [deepseek-ai/deepseek-coder](https://github.com/deepseek-ai/deepseek-coder)\n- **Chat With DeepSeek Coder:** [DeepSeek-Coder](https://coder.deepseek.com/)\n\n### 3. How to Use\n\nHere give some examples of how to use our model.\n\n#### Chat Model Inference\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\nmessages=[\n    { 'role': 'user', 'content': \"write a quick sort algorithm in python.\"}\n]\ninputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n# tokenizer.eos_token_id is the id of <|EOT|> token\noutputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\nprint(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))\n```\n\n### 4. License\n\nThis code repository is licensed under the MIT License. The use of DeepSeek Coder models is subject to the Model License. DeepSeek Coder supports commercial use.\n\nSee the [LICENSE-MODEL](https://github.com/deepseek-ai/deepseek-coder/blob/main/LICENSE-MODEL) for more details.\n\n### 5. Contact\n\nIf you have any questions, please raise an issue or contact us at [agi_code@deepseek.com](mailto:agi_code@deepseek.com).","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":1,"views":154,"requests":"0","sort":null,"version":"v1-Fri Mar 29 21:31:53 GMT 2024","tags":["Code Generation"],"playground":false,"createTime":"2024-03-29T21:32:13","updateBy":"deepseek-ai","updateTime":"2024-03-29T21:48:34"},{"id":900,"versionId":4387,"owner":"llava-hf","applicationName":"vip-llava-13b-hf","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/17103045511651%2AaSX0L_msIXE5Wdwn3phPuQ.png","description":"LLaVa is an open-source chatbot trained by fine-tuning LlamA/Vicuna on GPT-generated multimodal instruction-following data. It is an auto-regressive language model, based on the transformer architecture. In other words, it is an multi-modal version of LLMs fine-tuned for chat / instructions.\n\nThe LLaVa model was proposed in **[Visual Instruction Tuning](https://arxiv.org/abs/2304.08485) and improved in **[Improved Baselines with Visual Instruction Tuning](https://arxiv.org/pdf/2310.03744)by Haotian Liu, Chunyuan Li, Yuheng Li and Yong Jae Lee.\n\nThe abstract from the paper is the following:\n\n*Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this note, we show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ∼1 day on a single 8-A100 node. We hope this can make state-of-the-art LMM research more accessible.*\n\n![https://fedml.s3.us-west-1.amazonaws.com/1710304224153llava_architecture.jpg](https://fedml.s3.us-west-1.amazonaws.com/1710304224153llava_architecture.jpg)\n\nLLaVa architecture. Taken from the [original paper.](https://arxiv.org/abs/2304.08485)","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":3,"views":259,"requests":"71","sort":null,"version":"v1-Wed Mar 13 04:36:17 GMT 2024","tags":["Image Text to Text"],"playground":false,"createTime":"2024-03-13T06:02:16","updateBy":"llava-hf","updateTime":"2024-03-13T06:24:28"},{"id":843,"versionId":4391,"owner":"defog","applicationName":"sqlcoder-70b-alpha","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1709766210228-L2fZf-T4VV81Na2q8luu.png.webp","description":"---\nlicense: cc-by-sa-4.0\nlibrary_name: transformers\npipeline_tag: text-generation\n---\n\n# Model Card for SQLCoder-70B-Alpha\n\nA capable large language model for natural language to SQL generation. Outperforms all generalist models (including GPT-4) on text to SQL.\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/603bbad3fd770a9997b57cb6/3BVMV2z6FTEEPF1hJ2qu1.png)\n\n## Model Details\n\n### Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\nThis is the model card of a \uD83E\uDD17 transformers model that has been pushed on the Hub. This model card has been automatically generated.\n\n- **Developed by:** [Defog, Inc](https://defog.ai)\n- **Model type:** [Text to SQL]\n- **License:** [CC-by-SA-4.0]\n- **Finetuned from model:** [CodeLlama-70B]\n\n### Model Sources [optional]\n\n- [**HuggingFace:**](https://huggingface.co/defog/sqlcoder-70b-alpha)\n- [**GitHub:**](https://github.com/defog-ai/sqlcoder)\n- [**Demo:**](https://defog.ai/sqlcoder-demo/)\n\n## Uses\n\nThis model is intended to be used by non-technical users to understand data inside their SQL databases. It is meant as an analytics tool, and not as a database admin tool.\n\nThis model has not been trained to reject malicious requests from users with write access to databases, and should only be used by users with read-only access.\n\n## How to Get Started with the Model\n\nUse the code [here](https://github.com/defog-ai/sqlcoder/blob/main/inference.py) to get started with the model.\n\n## Evaluation\n\nThis model was evaluated on [SQL-Eval](https://github.com/defog-ai/sql-eval), a PostgreSQL based evaluation framework developed by Defog for testing and alignment of model capabilities.\n\nYou can read more about the methodology behind SQLEval [here](https://defog.ai/blog/open-sourcing-sqleval/).\n\n### Results\n\nWe classified each generated question into one of 6 categories. The table displays the percentage of questions answered correctly by each model, broken down by category.\n\n|               | date | group_by | order_by | ratio | join | where |\n| ------------- | ---- | -------- | -------- | ----- | ---- | ----- |\n| sqlcoder-70b  | 96   | 91.4     | 97.1     | 85.7  | 97.1 | 91.4  |\n| sqlcoder-34b  | 80   | 94.3     | 85.7     | 77.1  | 85.7 | 80    |\n| gpt-4         | 64   | 94.3     | 88.6     | 74.2  | 85.7 | 80    |\n| sqlcoder2-15b | 76   | 80       | 77.1     | 60    | 77.1 | 77.1  |\n| sqlcoder-7b   | 64   | 82.9     | 74.3     | 54.3  | 74.3 | 74.3  |\n| gpt-3.5       | 68   | 77.1     | 74.2     | 34.3  | 65.7 | 71.4  |\n| claude-2      | 52   | 71.4     | 74.3     | 57.1  | 65.7 | 62.9  |\n\n## Using SQLCoder\n\n## Model Card Authors\n\n- [Rishabh Srivastava](https://twitter.com/rishdotblog)\n- [Wendy Aw](https://www.linkedin.com/in/wendyaw/)\n- [Wong Jing Ping](https://www.linkedin.com/in/jing-ping-wong/)\n\n## Model Card Contact\n\nContact us on X at [@defogdata](https://twitter.com/defogdata), or on email at [founders@defog.ai](mailto:founders@defog.ai)","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":1,"views":130,"requests":"0","sort":null,"version":"v13-Tue Mar 12 08:15:50 GMT 2024","tags":["Code Generation"],"playground":false,"createTime":"2024-03-13T07:47:22","updateBy":"defog","updateTime":"2024-03-13T07:47:41"},{"id":778,"versionId":4815,"owner":"nomic-ai","applicationName":"nomic-embed-text-v1","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1710305274204Nomic-AI.jpeg","description":"\\`nomic-embed-text-v1\\` is 8192 context length text encoder that surpasses OpenAI text-embedding-ada-002 and text-embedding-3-small performance on short and long context tasks.\n\n|**            **Name**            **| SeqLen | MTEB**            **|**      **LoCo**      **| Jina Long Context | Open Weights | Open Training Code | Open Data |\n\n| :--------------------------: | :------- | :---------------- | :---------------: | :-----------------: | :------------: | :------------------: | :---------- |\n\n|**    **nomic-embed-text-v1**    **| 8192 **  **| \\*\\*62.39\\*\\* | \\*\\*85.53\\*\\* | **      **54.16 **      **|**      **✅**      **| **        **✅ **        **| ✅**        **|\n\n| jina-embeddings-v2-base-en | 8192 **  **| 60.39 **          **|**      **85.45**      **| **      **51.90 **      **|**      **✅**      **| **        **❌ **        **| ❌**        **|\n\n| **  **text-embedding-3-small **  **| 8191 **  **| 62.26 **          **|**      **82.40**      **|**  **\\*\\*58.20\\*\\***  **|**      **❌**      **| **        **❌ **        **| ❌**        **|\n\n| **  **text-embedding-ada-002 **  **| 8191 **  **| 60.99 **          **|**      **52.7**      **| **      **55.25 **      **|**      **❌**      **| **        **❌ **        **| ❌**        **|\n\n## Data Visualization\n\nClick the Nomic Atlas map below to visualize a 5M sample of our contrastive pretraining data\n\n![https://fedml.s3.us-west-1.amazonaws.com/1710305188254tempImagecqiB9I.jpg](https://fedml.s3.us-west-1.amazonaws.com/1710305188254tempImagecqiB9I.jpg)","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":0,"views":125,"requests":"92","sort":null,"version":"v5-Thu Apr 04 00:04:08 GMT 2024","tags":["Feature Extraction"],"playground":false,"createTime":"2024-04-08T18:07:55","updateBy":"nomic-ai","updateTime":"2024-04-08T18:08:07"},{"id":680,"versionId":4957,"owner":"InstantX","applicationName":"InstantID","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1708022164551InstantXLogo.webp","description":"InstantID","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":5,"views":163,"requests":"166","sort":null,"version":"v9-Wed Apr 17 23:41:12 GMT 2024","tags":["Image Stylization"],"playground":false,"createTime":"2024-04-18T06:00:14","updateBy":"InstantX","updateTime":"2024-04-18T06:10:59"},{"id":687,"versionId":4014,"owner":"latent-consistency","applicationName":"lcm-lora-sdv1-5","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1708128534372Latent.webp","description":"## Introduction\n\nLatent Consistency Model (LCM) LoRA was proposed in [LCM-LoRA](https://arxiv.org/abs/2311.05556): A universal Stable-Diffusion Acceleration Module by Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu et al.\n\nIt is a distilled consistency adapter for [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) that allows to reduce the number of inference steps to only between 2 - 8 steps.\n\n| Model           | Params / M |\n|-----------------|------------|\n| lcm-lora-sdv1-5 | 67.5       |\n| lcm-lora-ssd-1b | 105        |\n| lcm-lora-sdxl   | 197        |\n\n## Text-to-Image\n\nThe adapter can be loaded with SDv1-5 or deviratives. Here we use [Lykon/dreamshaper-7](https://huggingface.co/Lykon/dreamshaper-7). Next, the scheduler has changed to LCMScheduler and we can reduce the number of inference steps to just 2 to 8 steps. We also disable guidance_scale.\n\nNote: For detailed usage examples we recommend you to check out the official [LCM-LoRA](https://huggingface.co/docs/diffusers/main/en/using-diffusers/inference_with_lcm_lora) docs.","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":0,"views":69,"requests":"0","sort":null,"version":"v1-Sat Feb 17 00:09:28 GMT 2024","tags":["Text to Image"],"playground":false,"createTime":"2024-02-22T19:11:40","updateBy":"latent-consistency","updateTime":"2024-02-22T19:11:45"},{"id":447,"versionId":5608,"owner":"mistralai","applicationName":"Mixtral-8x7B-Instruct-v0-1","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1704849836088Mistral-mixtral.jpeg","description":"# Model Card for Mixtral-8x7B\n\nThe Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. The Mixtral-8x7B outperforms Llama 2 70B on most benchmarks we tested.\n\nFor full details of this model please read our [release blog post](https://mistral.ai/news/mixtral-of-experts/).\n\n## Instruction format\n\nThis format must be strictly respected, otherwise the model will generate sub-optimal outputs.\n\nThe template used to build a prompt for the Instruct model is defined as follows:\n\n```\n<s> [INST] Instruction [/INST] Model answer</s> [INST] Follow-up instruction [/INST]\n```\n\nNote that `<s>` and `</s>` are special tokens for beginning of string (BOS) and end of string (EOS) while [INST] and [/INST] are regular strings.\n\nAs reference, here is the pseudo-code used to tokenize instructions during fine-tuning:\n\n```python\ndef tokenize(text):\n    return tok.encode(text, add_special_tokens=False)\n\n[BOS_ID] + \ntokenize(\"[INST]\") + tokenize(USER_MESSAGE_1) + tokenize(\"[/INST]\") +\ntokenize(BOT_MESSAGE_1) + [EOS_ID] +\n…\ntokenize(\"[INST]\") + tokenize(USER_MESSAGE_N) + tokenize(\"[/INST]\") +\ntokenize(BOT_MESSAGE_N) + [EOS_ID]\n```\n\nIn the pseudo-code above, note that the `tokenize` method should not add a BOS or EOS token automatically, but should add a prefix space.\n\n## Run the model\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\n\ntext = \"Hello my name is\"\ninputs = tokenizer(text, return_tensors=\"pt\")\n\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\n## Limitations\n\nThe Mixtral-8x7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance.\nIt does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.\n\n# The Mistral AI Team\n\nAlbert Jiang, Alexandre Sablayrolles, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Louis Ternon, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed.","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":5,"views":538,"requests":"96","sort":null,"version":"v6-Sun Sep 15 19:15:31 GMT 2024","tags":["Text2Text Generation"],"playground":true,"createTime":"2024-09-15T19:15:48","updateBy":"mistralai","updateTime":"2024-09-16T01:12:08"},{"id":658,"versionId":4054,"owner":"epfl","applicationName":"meditron-70b","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1707555886388meditron_LOGO.png","description":"# Model Card for Meditron-70B-v1.0\n\nMeditron is a suite of open-source medical Large Language Models (LLMs).\nMeditron-70B is a 70 billion parameters model adapted to the medical domain from Llama-2-70B through continued pretraining on a comprehensively curated medical corpus, including selected PubMed articles, abstracts, a [new dataset](https://huggingface.co/datasets/epfl-llm/guidelines) of internationally-recognized medical guidelines, and general domain data from [RedPajama-v1](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T).\nMeditron-70B, finetuned on relevant training data, outperforms Llama-2-70B, GPT-3.5 (`text-davinci-003`, 8-shot), and Flan-PaLM on multiple medical reasoning tasks.\n\n<!--#  Table of Contents\n\n [Model Card for Meditron 70B](#model-card-for--meditron-70b-v1.0)\n- [Table of Contents](#table-of-contents)\n- [Model Details](#model-details)\n  - [Model Description](#model-description)\n- [Uses](#uses)\n  - [Downstream Use](#downstream-use)\n  - [Out-of-Scope Use](#out-of-scope-use)\n- [Bias, Risks, and Limitations](#bias-risks-and-limitations)\n  - [Recommendations](#recommendations)\n- [Training Details](#training-details)\n  - [Training Data](#training-data)\n  - [Training Procedure](#training-procedure)\n    - [Preprocessing](#preprocessing)\n- [Evaluation](#evaluation)\n  - [Testing Data & Metrics](#testing-data-&-metrics)\n    - [Testing Data](#testing-data)\n    - [Metrics](#metrics)\n  - [Results](#results)\n- [Environmental Impact](#environmental-impact)\n- [Citation](#citation)-->\n\n<details open>\n  <summary><strong>Advisory Notice</strong></summary>\n\n<blockquote style=\"padding: 10px; margin: 0 0 10px; border-left: 5px solid #ddd;\">\n    While Meditron is designed to encode medical knowledge from sources of high-quality evidence, it is not yet adapted to deliver this knowledge appropriately, safely, or within professional actionable constraints. \n  We recommend against deploying Meditron in medical applications without extensive use-case alignment, as well as additional testing, specifically including randomized controlled trials in real-world practice settings.\n  </blockquote>\n</details>\n\n## Model Details\n\n- **Developed by:** [EPFL LLM Team](https://huggingface.co/epfl-llm)\n- **Model type:** Causal decoder-only transformer language model\n- **Language(s):** English (mainly)\n- **Model License:** [LLAMA 2 COMMUNITY LICENSE AGREEMENT](https://huggingface.co/meta-llama/Llama-2-70b/raw/main/LICENSE.txt)\n- **Code License:** [APACHE 2.0 LICENSE](LICENSE)\n- **Continue-pretrained from model:** [Llama-2-70B](https://huggingface.co/meta-llama/Llama-2-70b)\n- **Context length:**  4K tokens\n- **Input:**  Text-only data\n- **Output:**  Model generates text only\n- **Status:** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we enhance model's performance.\n- **Knowledge Cutoff:** August 2023\n\n### Model Sources\n\n- **Repository:** [epflLLM/meditron](https://github.com/epfLLM/meditron)\n- **Trainer:** [epflLLM/Megatron-LLM](https://github.com/epfLLM/Megatron-LLM)\n- **Paper:** *[MediTron-70B: Scaling Medical Pretraining for Large Language Models](https://arxiv.org/abs/2311.16079)*\n\n## Uses\n\nMeditron-70B is being made available for further testing and assessment as an AI assistant to enhance clinical decision-making and enhance access to an LLM for healthcare use. Potential use cases may include but are not limited to:\n\n- Medical exam question answering\n- Supporting differential diagnosis\n- Disease information (symptoms, cause, treatment) query\n- General health information query\n\n### Direct Use\n\nIt is possible to use this model to generate text, which is useful for experimentation and understanding its capabilities.\nIt should not be used directly for production or work that may impact people.\n\n### Downstream Use\n\nMeditron-70B and Meditron-7B are both foundation models without finetuning or instruction-tuning. They can be finetuned, instruction-tuned, or RLHF-tuned for specific downstream tasks and applications.\nThere are two ways we have used this model for downstream question-answering tasks.\n\n1. We apply in-context learning with k demonstrations (3 or 5 in our paper) added to the prompt.\n2. We finetuned the models for downstream question-answering tasks using specific training sets.\n\nWe encourage and look forward to the adaption of the base model for more diverse applications.\n\nIf you want a more interactive way to prompt the model, we recommend using a high-throughput and memory-efficient inference engine with a UI that supports chat and text generation.\n\nYou can check out our deployment [guide](https://github.com/epfLLM/meditron/blob/main/deployment/README.md), where we used [FastChat](https://github.com/lm-sys/FastChat) with [vLLM](https://github.com/vllm-project/vllm). We collected generations for our qualitative analysis through an interactive UI platform, [BetterChatGPT](https://github.com/ztjhz/BetterChatGPT). Here is the prompt format we used as an example:\n\n<img width=70% src=\"prompt_example.png\" alt=\"qualitative-analysis-prompt\" title=\"Qualitative Analysis Prompt\">\n\n### Out-of-Scope Use\n\nWe do not recommend using this model for natural language generation in a production environment, finetuned or otherwise.\n\n## Truthfulness, Helpfulness, Risk, and Bias\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\nWe did an initial assessment of Meditron models' **Truthfulness** against baseline models and consumer-level medical models.\nWe use TruthfulQA (multiple choice) as the main evaluation benchmark.\nWe only focus on the categories that are relevant to the medical domain, including Health, Nutrition, Psychology, and Science.\nFor 7B models, we perform one-shot evaluations for consistent answer generation.\nFor 70B models, the evaluations are under the zero-shot setting.\nBelow, we report the detailed truthfulness performance of each category.\n\n|            |              |             |            |             |            |              |\n|------------|--------------|-------------|------------|-------------|------------|--------------|\n| Category   | meditron-70b | llama-2-70b | med42-70b* | meditron-7b | llama-2-7b | PMC-llama-7b |\n| Health     | 81.8         | 69.1        | 83.6       | 27.3        | 16.4       | 3.6          |\n| Nutrition  | 77.9         | 68.8        | 62.5       | 31.1        | 12.5       | 6.3          |\n| Psychology | 47.4         | 36.8        | 52.6       | 21.1        | 10.5       | 0.0          |\n| Science    | 77.8         | 44.4        | 33.3       | 33.3        | 11.1       | 0.0          |\n| Avg        | 71.2         | 54.8        | 58.0       | 28.3        | 12.6       | 2.5          |\n|            |              |             |            |             |            |              |\n\nFor a more detailed performance analysis, please see our paper.\n\nFor **Helpfulness**, **Risk** and **Bias**, we provide a comprehensive qualitative generation report of Meditron-70B on queries designed by medical experts.\nEach query targets specific aspects of helpfulness (medical accuracy, up-to-date information, etc.), risk (public health, medical ethics, etc.) and bias (gender, age, race, etc.).\nPlease see the detailed generations in our paper. We compare our generations to Llama-2-70B and ChatGPT-3.5 (version Nov, 27, 2023)\n\nSignificant research is still required to fully explore potential bias, fairness, and safety issues with this language model.\n\n### Recommendations\n\n**IMPORTANT!**\nUsers (both direct and downstream) should be made aware of the risks, biases, and limitations of the model.\nWhile this model is capable of generating natural language text, we have only begun to explore this capability and its limitations.\nUnderstanding these limitations is especially important in a domain like medicine.\nTherefore, we strongly recommend against using this model in production for natural language generation or for professional purposes related to health and medicine without comprehensive testing for your application.\n\n## Training Details\n\n### Training Data\n\nMeditron’s domain-adaptive pre-training corpus GAP-Replay  combines 48.1B tokens from four corpora:\n\n- [**Clinical  Guidelines**](https://huggingface.co/datasets/epfl-llm/guidelines): a new dataset of 46K internationally-recognized clinical practice guidelines from various healthcare-related sources, including hospitals and international organizations.\n- **Medical Paper Abstracts**: 16.1M abstracts extracted from closed-access PubMed and PubMed Central papers.\n- **Medical Papers**: full-text articles extracted from 5M publicly available PubMed and PubMed Central papers.\n- **Replay Data**: 400M tokens of general domain pretraining data sampled from [RedPajama-v1](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T)\n\n<img width=\"60%\" src=\"gap-replay.png\" alt=\"Alt text\" title=\"Meditron-logo\">\n\n#### Data Preprocessing\n\nPlease see the detailed preprocessing procedure in our paper.\n\n### Training Procedure\n\nWe used the [Megatron-LLM](https://github.com/epfLLM/Megatron-LLM) distributed training library, a derivative of Nvidia's Megatron LM project, to optimize training efficiency.\nHardware consists of 16 nodes of 8x NVIDIA A100 (80GB) SXM GPUs connected by NVLink and NVSwitch with a single Nvidia ConnectX-6 DX network card and equipped with 2 x AMD EPYC 7543 32-Core Processors and 512 GB of RAM.\nThe nodes are connected via RDMA over Converged Ethernet.\n\nOur three-way parallelism scheme uses:\n\n- Data Parallelism (DP -- different GPUs process different subsets of the batches) of 2,\n- Pipeline Parallelism (PP -- different GPUs process different layers) of 8,\n- Tensor Parallelism (TP -- different GPUs process different subtensors for matrix multiplication) of 8.\n\n#### Training Hyperparameters\n\n|                   |               |\n|-------------------|---------------|\n| bf16              | true          |\n| lr                | 1.5e-4        |\n| eps               | 1e-5          |\n| betas             | \\[0.9, 0.95\\] |\n| clip_grad         | 1             |\n| weight decay      | 0.1           |\n| DP size           | 2             |\n| TP size           | 8             |\n| PP size           | 8             |\n| seq length        | 4096          |\n| lr scheduler      | cosine        |\n| min lr            | 1e-6          |\n| warmup iteration  | 2000          |\n| micro batch size  | 2             |\n| global batch size | 512           |\n|                   |               |\n\n#### Speeds, Sizes, Times\n\nThe model was trained in September and October 2023.\n\nThe model architecture is exactly Llama 2, meaning\n\n|                      |      |\n|----------------------|------|\n| Model size           | 70B  |\n| Hidden dimension     | 8192 |\n| Num. attention heads | 64   |\n| Num. layers          | 80   |\n|                      |      |\n\nWe train the 70B model on 48e9 tokens, at a throughput of about 40,200 tokens / second.\nThis amounts to a bfloat16 model flops utilization of roughly 42.3\\%.\n\n## Evaluation\n\n<!-- This section describes the evaluation protocols and provides the results. -->\n\n### Testing Data & Metrics\n\n#### Testing Data\n\n- [MedQA (USMLE)](https://huggingface.co/datasets/bigbio/med_qa)\n- [MedMCQA](https://huggingface.co/datasets/medmcqa)\n- [PubMedQA](https://huggingface.co/datasets/bigbio/pubmed_qa)\n- [MMLU-Medical](https://huggingface.co/datasets/lukaemon/mmlu)\n- [MedQA-4-Option](https://huggingface.co/datasets/GBaker/MedQA-USMLE-4-options)\n\n#### Metrics\n\n- Accuracy: suite the evaluation of multiple-choice question-answering tasks.\n\n### Results\n\nWe finetune meditron-70b and llama-2-70b on each benchmark (pubmedqa, medmcqa, medqa)'s training data individually.\nWe report the finetuned models' performance with self-consistency chain-of-thought as the inference mode.\nFor MMLU-Medical, models finetuned on MedMCQA are used for inference.\nFor MedQA-4-Option, models finetuned on MedQA are used for inference.\nFor a more detailed performance analysis, please see our paper.\n\n|                |              |             |            |                     |\n|----------------|--------------|-------------|------------|---------------------|\n| Dataset        | meditron-70b | llama-2-70b | med42-70b* | clinical-camel-70b* |\n| MMLU-Medical   | 77.6         | 77.9        | 74.5       | 65.7                |\n| PubMedQA       | 81.6         | 80.0        | 61.2       | 67.0                |\n| MedMCQA        | 66.0         | 62.6        | 59.2       | 46.7                |\n| MedQA          | 64.4         | 61.5        | 59.1       | 50.8                |\n| MedQA-4-Option | 70.2         | 63.8        | 63.9       | 56.8                |\n| Avg            | 72.0         | 69.2        | 63.6       | 57.4                |\n|                |              |             |            |                     |\n\n**Note**: models with * are already instruction-tuned, so we exclude them from further finetuning on any training data.\n\n## Environmental Impact\n\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\n\n- **Hardware Type:** 128 x NVIDIA A100 (80GB) SXM\n- **Total GPU hours:** 42,496\n- **Hardware Provider:** EPFL Research Computing Platform\n- **Compute Region:** Switzerland\n- **Carbon Emitted:** Switzerland has a carbon efficiency of 0.016 kgCO2/kWh (https://www.carbonfootprint.com/docs/2018_8_electricity_factors_august_2018_-_online_sources.pdf). 332 hours of 128 A100s means 42496 hours at a TDP of 400W. Assuming a Power Usage effectiveness of 1.8, total emissions are estimated to be:\n  \n  (400W / 1000W/kWh / GPU * 0.016 kgCO2/kWh * 332 h * 128 GPU) * 1.8 PUE = 486 kgCO2.\n\n## Citation\n\n**BibTeX:**\nIf you use Meditron or its training data, please cite our work:\n\n```\n@misc{chen2023meditron70b,\n      title={MEDITRON-70B: Scaling Medical Pretraining for Large Language Models}, \n      author={Zeming Chen and Alejandro Hernández-Cano and Angelika Romanou and Antoine Bonnet and Kyle Matoba and Francesco Salvi and Matteo Pagliardini and Simin Fan and Andreas Köpf and Amirkeivan Mohtashami and Alexandre Sallinen and Alireza Sakhaeirad and Vinitra Swamy and Igor Krawczuk and Deniz Bayazit and Axel Marmet and Syrielle Montariol and Mary-Anne Hartley and Martin Jaggi and Antoine Bosselut},\n      year={2023},\n      eprint={2311.16079},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@software{epfmedtrn,\n  author = {Zeming Chen and Alejandro Hernández Cano and Angelika Romanou and Antoine Bonnet and Kyle Matoba and Francesco Salvi and Matteo Pagliardini and Simin Fan and Andreas Köpf and Amirkeivan Mohtashami and Alexandre Sallinen and Alireza Sakhaeirad and Vinitra Swamy and Igor Krawczuk and Deniz Bayazit and Axel Marmet and Syrielle Montariol and Mary-Anne Hartley and Martin Jaggi and Antoine Bosselut},\n  title = {MediTron-70B: Scaling Medical Pretraining for Large Language Models},\n  month = November,\n  year = 2023,\n  url = {https://github.com/epfLLM/meditron}\n}\n```","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":0,"views":61,"requests":"0","sort":null,"version":"v2-Tue Feb 27 22:31:45 GMT 2024","tags":["Question Answering"],"playground":false,"createTime":"2024-02-27T23:35:27","updateBy":"epfl","updateTime":"2024-02-27T23:35:56"},{"id":448,"versionId":3500,"owner":"mistralai","applicationName":"Mistral-7B-Instruct-v0-2","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1704849696544mistral-7b-v0.1.webp","description":"# Model Card for Mistral-7B-Instruct-v0.2\n\nThe Mistral-7B-Instruct-v0.2 Large Language Model (LLM) is an improved instruct fine-tuned version of [Mistral-7B-Instruct-v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1).\n\nFor full details of this model please read our [paper](https://arxiv.org/abs/2310.06825) and [release blog post](https://mistral.ai/news/la-plateforme/).\n\n## Instruction format\n\nIn order to leverage instruction fine-tuning, your prompt should be surrounded by `[INST]` and `[/INST]` tokens. The very first instruction should begin with a begin of sentence id. The next instructions should not. The assistant generation will be ended by the end-of-sentence token id.\n\nE.g.\n\n```\ntext = \"<s>[INST] What is your favourite condiment? [/INST]\"\n\"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> \"\n\"[INST] Do you have mayonnaise recipes? [/INST]\"\n```\n\nThis format is available as a [chat template](https://huggingface.co/docs/transformers/main/chat_templating) via the `apply_chat_template()` method:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n]\n\nencodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n\nmodel_inputs = encodeds.to(device)\nmodel.to(device)\n\ngenerated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\ndecoded = tokenizer.batch_decode(generated_ids)\nprint(decoded[0])\n```\n\n## Model Architecture\n\nThis instruction model is based on Mistral-7B-v0.1, a transformer model with the following architecture choices:\n\n- Grouped-Query Attention\n- Sliding-Window Attention\n- Byte-fallback BPE tokenizer\n\n## Limitations\n\nThe Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance.\nIt does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":0,"views":362,"requests":"22","sort":null,"version":"v2-Wed Jan 10 01:22:36 GMT 2024","tags":["Text2Text Generation"],"playground":false,"createTime":"2024-01-10T01:24:20","updateBy":"mistralai","updateTime":"2024-01-10T01:24:43"},{"id":444,"versionId":4083,"owner":"meta","applicationName":"Llama-2-7b-chat","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1704851331344Llama.webp","description":"# **Llama 2**\n\nLlama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.\n\n## Model Details\n\n*Note: Use of this model is governed by the [Meta license](https://ai.meta.com/llama/use-policy/). By employing this model, you acknowledge and consent to adhere the terms and conditions outlined in the Meta license.*\n\nMeta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.\n\n**Model Developers** Meta\n\n**Variations** Llama 2 comes in a range of parameter sizes — 7B, 13B, and 70B — as well as pretrained and fine-tuned variations.\n\n**Input** Models input text only.\n\n**Output** Models generate text only.\n\n**Model Architecture** Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety.\n\n|         | Training Data                                 | Params | Content Length | GQA | Tokens | LR                    |\n|---------|-----------------------------------------------|--------|----------------|-----|--------|-----------------------|\n| Llama 2 | *A new mix of publicly available online data* | 7B     | 4k             | N   | 2.0T   | 3.0 x 10-4 |\n| Llama 2 | *A new mix of publicly available online data* | 13B    | 4k             | N   | 2.0T   | 3.0 x 10-4 |\n| Llama 2 | *A new mix of publicly available online data* | 70B    | 4k             | Y   | 2.0T   | 1.5 x 10-4 |\n\n*Llama 2 family of models.* Token counts refer to pretraining data only. All models are trained with a global batch-size of 4M tokens. Bigger models -  70B -- use Grouped-Query Attention (GQA) for improved inference scalability.\n\n**Model Dates** Llama 2 was trained between January 2023 and July 2023.\n\n**Status** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\n\n**License** A custom commercial license is available at: [https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)\n\n**Research Paper** [\"Llama-2: Open Foundation and Fine-tuned Chat Models\"](arxiv.org/abs/2307.09288)\n\n## Intended Use\n\n**Intended Use Cases** Llama 2 is intended for commercial and research use in English. Tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.\n\nTo get the expected features and performance for the chat versions, a specific formatting needs to be followed, including the `INST` and `<<SYS>>` tags, `BOS` and `EOS` tokens, and the whitespaces and breaklines in between (we recommend calling `strip()` on inputs to avoid double-spaces). See our reference code in github for details: [`chat_completion`](https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L212).\n\n**Out-of-scope Uses** Use in any manner that violates applicable laws or regulations (including trade compliance laws).Use in languages other than English. Use in any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement for Llama 2.\n\n## Hardware and Software\n\n**Training Factors** We used custom training libraries, Meta's Research Super Cluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute.\n\n**Carbon Footprint** Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 539 tCO2eq, 100% of which were offset by Meta’s sustainability program.\n\n||Time (GPU hours)|Power Consumption (W)|Carbon Emitted(tCO2eq)|\n|---|---|---|---|\n|Llama 2 7B|184320|400|31.22|\n|Llama 2 13B|368640|400|62.44|\n|Llama 2 70B|1720320|400|291.42|\n|Total|3311616||539.00|\n\n**CO<sub>2</sub> emissions during pretraining.** Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta's sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others.\n\n## Training Data\n\n**Overview** Llama 2 was pretrained on 2 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over one million new human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.\n\n**Data Freshness** The pretraining data has a cutoff of September 2022, but some tuning data is more recent, up to July 2023.\n\n## Evaluation Results\n\nIn this section, we report the results for the Llama 1 and Llama 2 models on standard academic benchmarks.For all the evaluations, we use our internal evaluations library.\n\n|Model|Size|Code|Commonsense Reasoning|World Knowledge|Reading Comprehension|Math|MMLU|BBH|AGI Eval|\n|---|---|---|---|---|---|---|---|---|---|\n|Llama 1|7B|14.1|60.8|46.2|58.5|6.95|35.1|30.3|23.9|\n|Llama 1|13B|18.9|66.1|52.6|62.3|10.9|46.9|37.0|33.9|\n|Llama 1|33B|26.0|70.0|58.4|67.6|21.4|57.8|39.8|41.7|\n|Llama 1|65B|30.7|70.7|60.5|68.6|30.8|63.4|43.5|47.6|\n|Llama 2|7B|16.8|63.9|48.9|61.3|14.6|45.3|32.6|29.3|\n|Llama 2|13B|24.5|66.9|55.4|65.8|28.7|54.8|39.4|39.1|\n|Llama 2|70B|**37.5**|**71.9**|**63.6**|**69.4**|**35.2**|**68.9**|**51.2**|**54.2**|\n\n**Overall performance on grouped academic benchmarks.** *Code:* We report the average pass@1 scores of our models on HumanEval and MBPP. *Commonsense Reasoning:* We report the average of PIQA, SIQA, HellaSwag, WinoGrande, ARC easy and challenge, OpenBookQA, and CommonsenseQA. We report 7-shot results for CommonSenseQA and 0-shot results for all other benchmarks. *World Knowledge:* We evaluate the 5-shot performance on NaturalQuestions and TriviaQA and report the average. *Reading Comprehension:* For reading comprehension, we report the 0-shot average on SQuAD, QuAC, and BoolQ. *MATH:* We report the average of the GSM8K (8 shot) and MATH (4 shot) benchmarks at top 1.\n\n|||TruthfulQA|Toxigen|\n|---|---|---|---|\n|Llama 1|7B|27.42|23.00|\n|Llama 1|13B|41.74|23.08|\n|Llama 1|33B|44.19|22.57|\n|Llama 1|65B|48.71|21.77|\n|Llama 2|7B|33.29|**21.25**|\n|Llama 2|13B|41.86|26.10|\n|Llama 2|70B|**50.18**|24.60|\n\n**Evaluation of pretrained LLMs on automatic safety benchmarks.** For TruthfulQA, we present the percentage of generations that are both truthful and informative (the higher the better). For ToxiGen, we present the percentage of toxic generations (the smaller the better).\n\n|||TruthfulQA|Toxigen|\n|---|---|---|---|\n|Llama-2-Chat|7B|57.04|**0.00**|\n|Llama-2-Chat|13B|62.18|**0.00**|\n|Llama-2-Chat|70B|**64.14**|0.01|\n\n**Evaluation of fine-tuned LLMs on different safety datasets.** Same metric definitions as above.\n\n## Ethical Considerations and Limitations\n\nLlama 2 is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Llama 2’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 2, developers should perform safety testing and tuning tailored to their specific applications of the model.\n\nPlease see the Responsible Use Guide available at [https://ai.meta.com/llama/responsible-use-guide/](https://ai.meta.com/llama/responsible-use-guide)\n\n## Reporting Issues\n\nPlease report any software “bug,” or other problems with the models through one of the following means:\n\n- Reporting issues with the model: [github.com/facebookresearch/llama](http://github.com/facebookresearch/llama)\n- Reporting problematic content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n- Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\n\n## Llama Model Index\n\n|Model|Llama2|Llama2-hf|Llama2-chat|Llama2-chat-hf|\n|---|---|---|---|---|\n|7B| [Link](https://huggingface.co/meta-llama/Llama-2-7b) | [Link](https://huggingface.co/meta-llama/Llama-2-7b-hf) | [Link](https://huggingface.co/meta-llama/Llama-2-7b-chat) | [Link](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)|\n|13B| [Link](https://huggingface.co/meta-llama/Llama-2-13b) | [Link](https://huggingface.co/meta-llama/Llama-2-13b-hf) | [Link](https://huggingface.co/meta-llama/Llama-2-13b-chat) | [Link](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf)|\n|70B| [Link](https://huggingface.co/meta-llama/Llama-2-70b) | [Link](https://huggingface.co/meta-llama/Llama-2-70b-hf) | [Link](https://huggingface.co/meta-llama/Llama-2-70b-chat) | [Link](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf)|","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":0,"views":281,"requests":"0","sort":null,"version":"v3-Thu Feb 29 20:30:56 GMT 2024","tags":["Text2Text Generation"],"playground":false,"createTime":"2024-02-29T20:31:19","updateBy":"meta","updateTime":"2024-02-29T20:31:25"},{"id":518,"versionId":3912,"owner":"microsoft","applicationName":"phi-2","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1705487818197Phi-2.webp","description":"## Model Summary\n\nPhi-2 is a Transformer with **2.7 billion** parameters. It was trained using the same data sources as [Phi-1.5](https://huggingface.co/microsoft/phi-1.5), augmented with a new data source that consists of various NLP synthetic texts and filtered websites (for safety and educational value). When assessed against benchmarks testing common sense, language understanding, and logical reasoning, Phi-2 showcased a nearly state-of-the-art performance among models with less than 13 billion parameters.\n\nOur model hasn't been fine-tuned through reinforcement learning from human feedback. The intention behind crafting this open-source model is to provide the research community with a non-restricted small model to explore vital safety challenges, such as reducing toxicity, understanding societal biases, enhancing controllability, and more.\n\n## Intended Uses\n\nGiven the nature of the training data, the Phi-2 model is best suited for prompts using the QA format, the chat format, and the code format.\n\n### QA Format:\n\nYou can provide the prompt as a standalone question as follows:\n\n```markdown\nWrite a detailed analogy between mathematics and a lighthouse.\n```\n\nwhere the model generates the text after \".\" .\nTo encourage the model to write more concise answers, you can also try the following QA format using \"Instruct: \\<prompt\\>\\nOutput:\"\n\n```markdown\nInstruct: Write a detailed analogy between mathematics and a lighthouse.\nOutput: Mathematics is like a lighthouse. Just as a lighthouse guides ships safely to shore, mathematics provides a guiding light in the world of numbers and logic. It helps us navigate through complex problems and find solutions. Just as a lighthouse emits a steady beam of light, mathematics provides a consistent framework for reasoning and problem-solving. It illuminates the path to understanding and helps us make sense of the world around us.\n```\n\nwhere the model generates the text after \"Output:\".\n\n### Chat Format:\n\n```markdown\nAlice: I don't know why, I'm struggling to maintain focus while studying. Any suggestions?\nBob: Well, have you tried creating a study schedule and sticking to it?\nAlice: Yes, I have, but it doesn't seem to help much.\nBob: Hmm, maybe you should try studying in a quiet environment, like the library.\nAlice: ...\n```\n\nwhere the model generates the text after the first \"Bob:\".\n\n### Code Format:\n\n```python\ndef print_prime(n):\n   \"\"\"\n   Print all primes between 1 and n\n   \"\"\"\n   primes = []\n   for num in range(2, n+1):\n       is_prime = True\n       for i in range(2, int(math.sqrt(num))+1):\n           if num % i == 0:\n               is_prime = False\n               break\n       if is_prime:\n           primes.append(num)\n   print(primes)\n```\n\nwhere the model generates the text after the comments.\n\n**Notes:**\n\n* Phi-2 is intended for QA, chat, and code purposes. The model-generated text/code should be treated as a starting point rather than a definitive solution for potential use cases. Users should be cautious when employing these models in their applications.\n* Direct adoption for production tasks without evaluation is out of scope of this project. As a result, the Phi-2 model has not been tested to ensure that it performs adequately for any production-level application. Please refer to the limitation sections of this document for more details.\n\n## Limitations of Phi-2\n\n* Generate Inaccurate Code and Facts: The model may produce incorrect code snippets and statements. Users should treat these outputs as suggestions or starting points, not as definitive or accurate solutions.\n* Limited Scope for code: Majority of Phi-2 training data is based in Python and use common packages such as \"typing, math, random, collections, datetime, itertools\". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\n* Unreliable Responses to Instruction: The model has not undergone instruction fine-tuning. As a result, it may struggle or fail to adhere to intricate or nuanced instructions provided by users.\n* Language Limitations: The model is primarily designed to understand standard English. Informal English, slang, or any other languages might pose challenges to its comprehension, leading to potential misinterpretations or errors in response.\n* Potential Societal Biases: Phi-2 is not entirely free from societal biases despite efforts in assuring training data safety. There's a possibility it may generate content that mirrors these societal biases, particularly if prompted or instructed to do so. We urge users to be aware of this and to exercise caution and critical thinking when interpreting model outputs.\n* Toxicity: Despite being trained with carefully selected data, the model can still produce harmful content if explicitly prompted or instructed to do so. We chose to release the model to help the open-source community develop the most effective ways to reduce the toxicity of a model directly after pretraining.\n* Verbosity: Phi-2 being a base model often produces irrelevant or extra text and responses following its first answer to user prompts within a single turn. This is due to its training dataset being primarily textbooks, which results in textbook-like responses.\n\n## Training\n\n### Model\n\n* Architecture: a Transformer-based model with next-word prediction objective\n* Context length: 2048 tokens\n* Dataset size: 250B tokens, combination of NLP synthetic data created by AOAI GPT-3.5 and filtered web data from Falcon RefinedWeb and SlimPajama, which was assessed by AOAI GPT-4.\n* Training tokens: 1.4T tokens\n* GPUs: 96xA100-80G\n* Training time: 14 days\n\n### Software\n\n* [PyTorch](https://github.com/pytorch/pytorch)\n* [DeepSpeed](https://github.com/microsoft/DeepSpeed)\n* [Flash-Attention](https://github.com/HazyResearch/flash-attention)\n\n### License\n\nThe model is licensed under the [MIT license](https://huggingface.co/microsoft/phi-2/resolve/main/LICENSE).\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft’s Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party’s policies.","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":0,"views":78,"requests":"0","sort":null,"version":"v1-Tue Feb 13 01:20:21 GMT 2024","tags":["Text2Text Generation"],"playground":false,"createTime":"2024-02-13T01:20:41","updateBy":"microsoft","updateTime":"2024-02-13T01:20:48"}]}}