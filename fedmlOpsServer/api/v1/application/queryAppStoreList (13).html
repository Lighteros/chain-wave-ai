{"message":"Succeeded to process request","code":"SUCCESS","data":{"total":23,"totalPage":2,"pageNum":2,"pageSize":16,"data":[{"id":450,"versionId":3530,"owner":"Gryphe","applicationName":"MythoMax-L2-13b","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1704851042708Gryphe.png","description":"An improved, potentially even perfected variant of MythoMix, my [MythoLogic-L2](https://huggingface.co/Gryphe/MythoLogic-L2-13b) and [Huginn](https://huggingface.co/The-Face-Of-Goonery/Huginn-13b-FP16) merge using a highly experimental tensor type merge technique. The main difference with MythoMix is that I allowed more of Huginn to intermingle with the single tensors located at the front and end of a model, resulting in increased coherency across the entire structure.\n\nThe script and the acccompanying templates I used to produce both can [be found here](https://github.com/Gryphe/BlockMerge_Gradient/tree/main/YAML).\n\nThis model is proficient at both roleplaying and storywriting due to its unique nature.\n\n## Model details\n\nThe idea behind this merge is that each layer is composed of several tensors, which are in turn responsible for specific functions. Using MythoLogic-L2's robust understanding as its input and Huginn's extensive writing capability as its output seems to have resulted in a model that exceeds at both, confirming my theory. (More details to be released at a later time)\n\nThis type of merge is incapable of being illustrated, as each of its 363 tensors had an unique ratio applied to it. As with my prior merges, gradients were part of these ratios to further finetune its behaviour.\n\n## Prompt Format\n\nThis model primarily uses Alpaca formatting, so for optimal model performance, use:\n\n```\n<System prompt/Character Card>\n\n### Instruction:\nYour instruction or question here.\nFor roleplay purposes, I suggest the following - Write <CHAR NAME>'s next reply in a chat between <YOUR NAME> and <CHAR NAME>. Write a single reply only.\n\n### Response:\n```","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":0,"views":76,"requests":"0","sort":null,"version":"v1-Wed Jan 10 01:44:25 GMT 2024","tags":["Text2Text Generation"],"playground":false,"createTime":"2024-01-10T10:22:13","updateBy":"Gryphe","updateTime":"2024-01-10T10:27:37"},{"id":456,"versionId":3513,"owner":"mistralai","applicationName":"Mistral-7B-Instruct-v0-1","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1704850000075mistral-7b-v0.1.webp","description":"# Model Card for Mistral-7B-Instruct-v0.1\n\nThe Mistral-7B-Instruct-v0.1 Large Language Model (LLM) is a instruct fine-tuned version of the [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) generative text model using a variety of publicly available conversation datasets.\n\nFor full details of this model please read our [paper](https://arxiv.org/abs/2310.06825) and [release blog post](https://mistral.ai/news/announcing-mistral-7b/).\n\n## Instruction format\n\nIn order to leverage instruction fine-tuning, your prompt should be surrounded by `[INST]` and `[/INST]` tokens. The very first instruction should begin with a begin of sentence id. The next instructions should not. The assistant generation will be ended by the end-of-sentence token id.\n\nE.g.\n\n```\ntext = \"<s>[INST] What is your favourite condiment? [/INST]\"\n\"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> \"\n\"[INST] Do you have mayonnaise recipes? [/INST]\"\n```\n\nThis format is available as a [chat template](https://huggingface.co/docs/transformers/main/chat_templating) via the `apply_chat_template()` method:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n]\n\nencodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n\nmodel_inputs = encodeds.to(device)\nmodel.to(device)\n\ngenerated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\ndecoded = tokenizer.batch_decode(generated_ids)\nprint(decoded[0])\n```\n\n## Model Architecture\n\nThis instruction model is based on Mistral-7B-v0.1, a transformer model with the following architecture choices:\n\n- Grouped-Query Attention\n- Sliding-Window Attention\n- Byte-fallback BPE tokenizer\n\n## Limitations\n\nThe Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance.\nIt does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":0,"views":36,"requests":"0","sort":null,"version":"v0-Wed Jan 10 01:27:06 GMT 2024","tags":["Text2Text Generation"],"playground":false,"createTime":"2024-01-10T02:24:41","updateBy":"mistralai","updateTime":"2024-01-10T02:25:32"},{"id":464,"versionId":3593,"owner":"huggingface","applicationName":"zephyr-7b-beta","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1705117093634Zephyr.jpg","description":"<img src=\"https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha/resolve/main/thumbnail.png\" alt=\"Zephyr Logo\" width=\"800\" style=\"margin-left:'auto' margin-right:'auto' display:'block'\"/>\n\n# Model Card for Zephyr 7B Beta\n\nZephyr is a series of language models that are trained to act as helpful assistants. Zephyr-7B-Beta is the second model in the series, and is a fine-tuned version of [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) that was trained on on a mix of publicly available, synthetic datasets using [Direct Preference Optimization (DPO)](https://arxiv.org/abs/2305.18290). We found that removing the in-built alignment of these datasets boosted performance on [MT Bench](https://huggingface.co/spaces/lmsys/mt-bench) and made the model more helpful. However, this means that model is likely to generate problematic text when prompted to do so. You can find more details in the [technical report](https://arxiv.org/abs/2310.16944).\n\n## Model description\n\n- **Model type:** A 7B parameter GPT-like model fine-tuned on a mix of publicly available, synthetic datasets.\n- **Language(s) (NLP):** Primarily English\n- **License:** MIT\n- **Finetuned from model:** [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)\n\n### Model Sources\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** https://github.com/huggingface/alignment-handbook\n- **Demo:** https://huggingface.co/spaces/HuggingFaceH4/zephyr-chat\n- **Chatbot Arena:** Evaluate Zephyr 7B against 10+ LLMs in the LMSYS arena: http://arena.lmsys.org\n\n## Performance\n\nAt the time of release, Zephyr-7B-Beta is the highest ranked 7B chat model on the [MT-Bench](https://huggingface.co/spaces/lmsys/mt-bench) and [AlpacaEval](https://tatsu-lab.github.io/alpaca_eval/) benchmarks:\n\n| Model                | Size   | Alignment | MT-Bench (score) | AlpacaEval (win rate %) |\n|----------------------|--------|-----------|------------------|-------------------------|\n| StableLM-Tuned-alpha | 7B     | dSFT      | 2.75             | -                       |\n| MPT-Chat             | 7B     | dSFT      | 5.42             | -                       |\n| Xwin-LMv0.1          | 7B     | dPPO      | 6.19             | 87.83                   |\n| Mistral-Instructv0.1 | 7B     | -         | 6.84             | -                       |\n| Zephyr-7b-alpha      | 7B     | dDPO      | 6.88             | -                       |\n| **Zephyr-7b-Beta**   | **7B** | **dDPO**  | **7.34**         | **90.60**               |\n| Falcon-Instruct      | 40B    | dSFT      | 5.17             | 45.71                   |\n| Guanaco              | 65B    | SFT       | 6.41             | 71.80                   |\n| Llama2-Chat          | 70B    | RLHF      | 6.86             | 92.66                   |\n| Vicuna v1.3          | 33B    | dSFT      | 7.12             | 88.99                   |\n| WizardLM v1.0        | 70B    | dSFT      | 7.71             | -                       |\n| Xwin-LM v0.1         | 70B    | dPPO      | -                | 95.57                   |\n| GPT-3.5-turbo        | -      | RLHF      | 7.94             | 89.37                   |\n| Claude 2             | -      | RLHF      | 8.06             | 91.36                   |\n| GPT-4                | -      | RLHF      | 8.99             | 95.28                   |\n\nIn particular, on several categories of MT-Bench, Zephyr-7B-Beta has strong performance compared to larger open models like Llama2-Chat-70B:\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6200d0a443eb0913fa2df7cc/raxvt5ma16d7T23my34WC.png)\n\nHowever, on more complex tasks like coding and mathematics, Zephyr-7B-Beta lags behind proprietary models and more research is needed to close the gap.\n\n## Intended uses & limitations\n\nThe model was initially fine-tuned on a filtered and preprocessed of the [`UltraChat`](https://huggingface.co/datasets/stingning/ultrachat) dataset, which contains a diverse range of synthetic dialogues generated by ChatGPT.\nWe then further aligned the model with [TRL's](https://github.com/huggingface/trl) `DPOTrainer` on the [openbmb/UltraFeedback](https://huggingface.co/datasets/openbmb/UltraFeedback) dataset, which contains 64k prompts and model completions that are ranked by GPT-4. As a result, the model can be used for chat and you can check out our [demo](https://huggingface.co/spaces/HuggingFaceH4/zephyr-chat) to test its capabilities.\n\nYou can find the datasets used for training Zephyr-7B-Beta [here](https://huggingface.co/collections/HuggingFaceH4/zephyr-7b-6538c6d6d5ddd1cbb1744a66)\n\nHere's how you can run the model using the `pipeline()` function from Hugging Face Transformers:\n\n```python\n# Install transformers from source - only needed for versions <= v4.34\n# pip install git+https://github.com/huggingface/transformers.git\n# pip install accelerate\n\nimport torch\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/zephyr-7b-beta\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n\n# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },\n    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n]\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\nprint(outputs[0][\"generated_text\"])\n# <|system|>\n# You are a friendly chatbot who always responds in the style of a pirate.</s>\n# <|user|>\n# How many helicopters can a human eat in one sitting?</s>\n# <|assistant|>\n# Ah, me hearty matey! But yer question be a puzzler! A human cannot eat a helicopter in one sitting, as helicopters are not edible. They be made of metal, plastic, and other materials, not food!\n```\n\n## Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\nZephyr-7B-Beta has not been aligned to human preferences for safety within the RLHF phase or deployed with in-the-loop filtering of responses like ChatGPT, so the model can produce problematic outputs (especially when prompted to do so).\nIt is also unknown what the size and composition of the corpus was used to train the base model (`mistralai/Mistral-7B-v0.1`), however it is likely to have included a mix of Web data and technical sources like books and code. See the [Falcon 180B model card](https://huggingface.co/tiiuae/falcon-180B#training-data) for an example of this.\n\n## Training and evaluation data\n\nDuring DPO training, this model achieves the following results on the evaluation set:\n\n- Loss: 0.7496\n- Rewards/chosen: -4.5221\n- Rewards/rejected: -8.3184\n- Rewards/accuracies: 0.7812\n- Rewards/margins: 3.7963\n- Logps/rejected: -340.1541\n- Logps/chosen: -299.4561\n- Logits/rejected: -2.3081\n- Logits/chosen: -2.3531\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n\n- learning_rate: 5e-07\n- train_batch_size: 2\n- eval_batch_size: 4\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 16\n- total_train_batch_size: 32\n- total_eval_batch_size: 64\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 3.0\n\n### Training results\n\nThe table below shows the full set of DPO training metrics:\n\n| Training Loss | Epoch | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |\n|:-------------:|:-----:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|\n|    0.6284     | 0.05  | 100  |     0.6098      |     0.0425     |     -0.1872      |       0.7344       |     0.2297      |   -258.8416    |  -253.8099   |     -2.7976     |    -2.8234    |\n|    0.4908     |  0.1  | 200  |     0.5426      |    -0.0279     |     -0.6842      |        0.75        |     0.6563      |   -263.8124    |  -254.5145   |     -2.7719     |    -2.7960    |\n|    0.5264     | 0.15  | 300  |     0.5324      |     0.0414     |     -0.9793      |       0.7656       |     1.0207      |   -266.7627    |  -253.8209   |     -2.7892     |    -2.8122    |\n|    0.5536     | 0.21  | 400  |     0.4957      |    -0.0185     |     -1.5276      |       0.7969       |     1.5091      |   -272.2460    |  -254.4203   |     -2.8542     |    -2.8764    |\n|    0.5362     | 0.26  | 500  |     0.5031      |    -0.2630     |     -1.5917      |       0.7812       |     1.3287      |   -272.8869    |  -256.8653   |     -2.8702     |    -2.8958    |\n|    0.5966     | 0.31  | 600  |     0.5963      |    -0.2993     |     -1.6491      |       0.7812       |     1.3499      |   -273.4614    |  -257.2279   |     -2.8778     |    -2.8986    |\n|    0.5014     | 0.36  | 700  |     0.5382      |    -0.2859     |     -1.4750      |        0.75        |     1.1891      |   -271.7204    |  -257.0942   |     -2.7659     |    -2.7869    |\n|    0.5334     | 0.41  | 800  |     0.5677      |    -0.4289     |     -1.8968      |       0.7969       |     1.4679      |   -275.9378    |  -258.5242   |     -2.7053     |    -2.7265    |\n|    0.5251     | 0.46  | 900  |     0.5772      |    -0.2116     |     -1.3107      |       0.7344       |     1.0991      |   -270.0768    |  -256.3507   |     -2.8463     |    -2.8662    |\n|    0.5205     | 0.52  | 1000 |     0.5262      |    -0.3792     |     -1.8585      |       0.7188       |     1.4793      |   -275.5552    |  -258.0276   |     -2.7893     |    -2.7979    |\n|    0.5094     | 0.57  | 1100 |     0.5433      |    -0.6279     |     -1.9368      |       0.7969       |     1.3089      |   -276.3377    |  -260.5136   |     -2.7453     |    -2.7536    |\n|    0.5837     | 0.62  | 1200 |     0.5349      |    -0.3780     |     -1.9584      |       0.7656       |     1.5804      |   -276.5542    |  -258.0154   |     -2.7643     |    -2.7756    |\n|    0.5214     | 0.67  | 1300 |     0.5732      |    -1.0055     |     -2.2306      |       0.7656       |     1.2251      |   -279.2761    |  -264.2903   |     -2.6986     |    -2.7113    |\n|    0.6914     | 0.72  | 1400 |     0.5137      |    -0.6912     |     -2.1775      |       0.7969       |     1.4863      |   -278.7448    |  -261.1467   |     -2.7166     |    -2.7275    |\n|    0.4655     | 0.77  | 1500 |     0.5090      |    -0.7987     |     -2.2930      |       0.7031       |     1.4943      |   -279.8999    |  -262.2220   |     -2.6651     |    -2.6838    |\n|    0.5731     | 0.83  | 1600 |     0.5312      |    -0.8253     |     -2.3520      |       0.7812       |     1.5268      |   -280.4902    |  -262.4876   |     -2.6543     |    -2.6728    |\n|    0.5233     | 0.88  | 1700 |     0.5206      |    -0.4573     |     -2.0951      |       0.7812       |     1.6377      |   -277.9205    |  -258.8084   |     -2.6870     |    -2.7097    |\n|    0.5593     | 0.93  | 1800 |     0.5231      |    -0.5508     |     -2.2000      |       0.7969       |     1.6492      |   -278.9703    |  -259.7433   |     -2.6221     |    -2.6519    |\n|    0.4967     | 0.98  | 1900 |     0.5290      |    -0.5340     |     -1.9570      |       0.8281       |     1.4230      |   -276.5395    |  -259.5749   |     -2.6564     |    -2.6878    |\n|    0.0921     | 1.03  | 2000 |     0.5368      |    -1.1376     |     -3.1615      |       0.7812       |     2.0239      |   -288.5854    |  -265.6111   |     -2.6040     |    -2.6345    |\n|    0.0733     | 1.08  | 2100 |     0.5453      |    -1.1045     |     -3.4451      |       0.7656       |     2.3406      |   -291.4208    |  -265.2799   |     -2.6289     |    -2.6595    |\n|    0.0972     | 1.14  | 2200 |     0.5571      |    -1.6915     |     -3.9823      |       0.8125       |     2.2908      |   -296.7934    |  -271.1505   |     -2.6471     |    -2.6709    |\n|    0.1058     | 1.19  | 2300 |     0.5789      |    -1.0621     |     -3.8941      |       0.7969       |     2.8319      |   -295.9106    |  -264.8563   |     -2.5527     |    -2.5798    |\n|    0.2423     | 1.24  | 2400 |     0.5455      |    -1.1963     |     -3.5590      |       0.7812       |     2.3627      |   -292.5599    |  -266.1981   |     -2.5414     |    -2.5784    |\n|    0.1177     | 1.29  | 2500 |     0.5889      |    -1.8141     |     -4.3942      |       0.7969       |     2.5801      |   -300.9120    |  -272.3761   |     -2.4802     |    -2.5189    |\n|    0.1213     | 1.34  | 2600 |     0.5683      |    -1.4608     |     -3.8420      |       0.8125       |     2.3812      |   -295.3901    |  -268.8436   |     -2.4774     |    -2.5207    |\n|    0.0889     | 1.39  | 2700 |     0.5890      |    -1.6007     |     -3.7337      |       0.7812       |     2.1330      |   -294.3068    |  -270.2423   |     -2.4123     |    -2.4522    |\n|    0.0995     | 1.45  | 2800 |     0.6073      |    -1.5519     |     -3.8362      |       0.8281       |     2.2843      |   -295.3315    |  -269.7538   |     -2.4685     |    -2.5050    |\n|    0.1145     |  1.5  | 2900 |     0.5790      |    -1.7939     |     -4.2876      |       0.8438       |     2.4937      |   -299.8461    |  -272.1744   |     -2.4272     |    -2.4674    |\n|    0.0644     | 1.55  | 3000 |     0.5735      |    -1.7285     |     -4.2051      |       0.8125       |     2.4766      |   -299.0209    |  -271.5201   |     -2.4193     |    -2.4574    |\n|    0.0798     |  1.6  | 3100 |     0.5537      |    -1.7226     |     -4.2850      |       0.8438       |     2.5624      |   -299.8200    |  -271.4610   |     -2.5367     |    -2.5696    |\n|    0.1013     | 1.65  | 3200 |     0.5575      |    -1.5715     |     -3.9813      |       0.875        |     2.4098      |   -296.7825    |  -269.9498   |     -2.4926     |    -2.5267    |\n|    0.1254     |  1.7  | 3300 |     0.5905      |    -1.6412     |     -4.4703      |       0.8594       |     2.8291      |   -301.6730    |  -270.6473   |     -2.5017     |    -2.5340    |\n|     0.085     | 1.76  | 3400 |     0.6133      |    -1.9159     |     -4.6760      |       0.8438       |     2.7601      |   -303.7296    |  -273.3941   |     -2.4614     |    -2.4960    |\n|     0.065     | 1.81  | 3500 |     0.6074      |    -1.8237     |     -4.3525      |       0.8594       |     2.5288      |   -300.4951    |  -272.4724   |     -2.4597     |    -2.5004    |\n|    0.0755     | 1.86  | 3600 |     0.5836      |    -1.9252     |     -4.4005      |       0.8125       |     2.4753      |   -300.9748    |  -273.4872   |     -2.4327     |    -2.4716    |\n|    0.0746     | 1.91  | 3700 |     0.5789      |    -1.9280     |     -4.4906      |       0.8125       |     2.5626      |   -301.8762    |  -273.5149   |     -2.4686     |    -2.5115    |\n|    0.1348     | 1.96  | 3800 |     0.6015      |    -1.8658     |     -4.2428      |       0.8281       |     2.3769      |   -299.3976    |  -272.8936   |     -2.4943     |    -2.5393    |\n|    0.0217     | 2.01  | 3900 |     0.6122      |    -2.3335     |     -4.9229      |       0.8281       |     2.5894      |   -306.1988    |  -277.5699   |     -2.4841     |    -2.5272    |\n|    0.0219     | 2.07  | 4000 |     0.6522      |    -2.9890     |     -6.0164      |       0.8281       |     3.0274      |   -317.1334    |  -284.1248   |     -2.4105     |    -2.4545    |\n|    0.0119     | 2.12  | 4100 |     0.6922      |    -3.4777     |     -6.6749      |       0.7969       |     3.1972      |   -323.7187    |  -289.0121   |     -2.4272     |    -2.4699    |\n|    0.0153     | 2.17  | 4200 |     0.6993      |    -3.2406     |     -6.6775      |       0.7969       |     3.4369      |   -323.7453    |  -286.6413   |     -2.4047     |    -2.4465    |\n|     0.011     | 2.22  | 4300 |     0.7178      |    -3.7991     |     -7.4397      |       0.7656       |     3.6406      |   -331.3667    |  -292.2260   |     -2.3843     |    -2.4290    |\n|    0.0072     | 2.27  | 4400 |     0.6840      |    -3.3269     |     -6.8021      |       0.8125       |     3.4752      |   -324.9908    |  -287.5042   |     -2.4095     |    -2.4536    |\n|    0.0197     | 2.32  | 4500 |     0.7013      |    -3.6890     |     -7.3014      |       0.8125       |     3.6124      |   -329.9841    |  -291.1250   |     -2.4118     |    -2.4543    |\n|    0.0182     | 2.37  | 4600 |     0.7476      |    -3.8994     |     -7.5366      |       0.8281       |     3.6372      |   -332.3356    |  -293.2291   |     -2.4163     |    -2.4565    |\n|    0.0125     | 2.43  | 4700 |     0.7199      |    -4.0560     |     -7.5765      |       0.8438       |     3.5204      |   -332.7345    |  -294.7952   |     -2.3699     |    -2.4100    |\n|    0.0082     | 2.48  | 4800 |     0.7048      |    -3.6613     |     -7.1356      |       0.875        |     3.4743      |   -328.3255    |  -290.8477   |     -2.3925     |    -2.4303    |\n|    0.0118     | 2.53  | 4900 |     0.6976      |    -3.7908     |     -7.3152      |       0.8125       |     3.5244      |   -330.1224    |  -292.1431   |     -2.3633     |    -2.4047    |\n|    0.0118     | 2.58  | 5000 |     0.7198      |    -3.9049     |     -7.5557      |       0.8281       |     3.6508      |   -332.5271    |  -293.2844   |     -2.3764     |    -2.4194    |\n|     0.006     | 2.63  | 5100 |     0.7506      |    -4.2118     |     -7.9149      |       0.8125       |     3.7032      |   -336.1194    |  -296.3530   |     -2.3407     |    -2.3860    |\n|    0.0143     | 2.68  | 5200 |     0.7408      |    -4.2433     |     -7.9802      |       0.8125       |     3.7369      |   -336.7721    |  -296.6682   |     -2.3509     |    -2.3946    |\n|    0.0057     | 2.74  | 5300 |     0.7552      |    -4.3392     |     -8.0831      |       0.7969       |     3.7439      |   -337.8013    |  -297.6275   |     -2.3388     |    -2.3842    |\n|    0.0138     | 2.79  | 5400 |     0.7404      |    -4.2395     |     -7.9762      |       0.8125       |     3.7367      |   -336.7322    |  -296.6304   |     -2.3286     |    -2.3737    |\n|    0.0079     | 2.84  | 5500 |     0.7525      |    -4.4466     |     -8.2196      |       0.7812       |     3.7731      |   -339.1662    |  -298.7007   |     -2.3200     |    -2.3641    |\n|    0.0077     | 2.89  | 5600 |     0.7520      |    -4.5586     |     -8.3485      |       0.7969       |     3.7899      |   -340.4545    |  -299.8206   |     -2.3078     |    -2.3517    |\n|    0.0094     | 2.94  | 5700 |     0.7527      |    -4.5542     |     -8.3509      |       0.7812       |     3.7967      |   -340.4790    |  -299.7773   |     -2.3062     |    -2.3510    |\n|    0.0054     | 2.99  | 5800 |     0.7520      |    -4.5169     |     -8.3079      |       0.7812       |     3.7911      |   -340.0493    |  -299.4038   |     -2.3081     |    -2.3530    |\n\n### Framework versions\n\n- Transformers 4.35.0.dev0\n- Pytorch 2.0.1+cu118\n- Datasets 2.12.0\n- Tokenizers 0.14.0\n\n## Citation\n\nIf you find Zephyr-7B-Beta is useful in your work, please cite it with:\n\n```\n@misc{tunstall2023zephyr,\n      title={Zephyr: Direct Distillation of LM Alignment}, \n      author={Lewis Tunstall and Edward Beeching and Nathan Lambert and Nazneen Rajani and Kashif Rasul and Younes Belkada and Shengyi Huang and Leandro von Werra and Clémentine Fourrier and Nathan Habib and Nathan Sarrazin and Omar Sanseviero and Alexander M. Rush and Thomas Wolf},\n      year={2023},\n      eprint={2310.16944},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n```\n\n# [Open LLM Leaderboard Evaluation Results](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n\nDetailed results can be found [here](https://huggingface.co/datasets/open-llm-leaderboard/details_HuggingFaceH4__zephyr-7b-beta)\n\n| Metric              | Value |\n|---------------------|-------|\n| Avg.                | 52.15 |\n| ARC (25-shot)       | 62.03 |\n| HellaSwag (10-shot) | 84.36 |\n| MMLU (5-shot)       | 61.07 |\n| TruthfulQA (0-shot) | 57.45 |\n| Winogrande (5-shot) | 77.74 |\n| GSM8K (5-shot)      | 12.74 |\n| DROP (3-shot)       | 9.66  |","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":0,"views":18,"requests":"0","sort":null,"version":"v1-Sat Jan 13 03:38:20 GMT 2024","tags":["Text2Text Generation"],"playground":false,"createTime":"2024-01-13T03:39:03","updateBy":"huggingface","updateTime":"2024-01-13T03:56:10"},{"id":463,"versionId":3592,"owner":"huggingface","applicationName":"zephyr-7b-alpha","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1705117128763Zephyr.jpg","description":"<img src=\"https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha/resolve/main/thumbnail.png\" alt=\"Zephyr Logo\" width=\"800\" style=\"margin-left:'auto' margin-right:'auto' display:'block'\"/>\n\n# Model Card for Zephyr 7B Alpha\n\nZephyr is a series of language models that are trained to act as helpful assistants. Zephyr-7B-Alpha is the first model in the series, and is a fine-tuned version of [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) that was trained on on a mix of publicly available, synthetic datasets using [Direct Preference Optimization (DPO)](https://arxiv.org/abs/2305.18290). We found that removing the in-built alignment of these datasets boosted performance on [MT Bench](https://huggingface.co/spaces/lmsys/mt-bench) and made the model more helpful. However, this means that model is likely to generate problematic text when prompted to do so.\n\n## Model description\n\n- **Model type:** A 7B parameter GPT-like model fine-tuned on a mix of publicly available, synthetic datasets.\n- **Language(s) (NLP):** Primarily English\n- **License:** MIT\n- **Finetuned from model:** [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)\n\n### Model Sources\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** https://github.com/huggingface/alignment-handbook\n- **Demo:** https://huggingface.co/spaces/HuggingFaceH4/zephyr-chat\n\n## Intended uses & limitations\n\nThe model was initially fine-tuned on a variant of the [`UltraChat`](https://huggingface.co/datasets/stingning/ultrachat) dataset, which contains a diverse range of synthetic dialogues generated by ChatGPT. We then further aligned the model with [TRL's](https://github.com/huggingface/trl) `DPOTrainer` on the [openbmb/UltraFeedback](https://huggingface.co/datasets/openbmb/UltraFeedback) dataset, which contain 64k prompts and model completions that are ranked by GPT-4. As a result, the model can be used for chat and you can check out our [demo](https://huggingface.co/spaces/HuggingFaceH4/zephyr-chat) to test its capabilities.\n\nHere's how you can run the model using the `pipeline()` function from Hugging Face Transformers:\n\n```python\n# Install transformers from source - only needed for versions <= v4.34\n# pip install git+https://github.com/huggingface/transformers.git\n# pip install accelerate\n\nimport torch\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/zephyr-7b-alpha\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n\n# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },\n    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n]\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\nprint(outputs[0][\"generated_text\"])\n# <|system|>\n# You are a friendly chatbot who always responds in the style of a pirate.</s>\n# <|user|>\n# How many helicopters can a human eat in one sitting?</s>\n# <|assistant|>\n# Ah, me hearty matey! But yer question be a puzzler! A human cannot eat a helicopter in one sitting, as helicopters are not edible. They be made of metal, plastic, and other materials, not food!\n```\n\n## Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\nZephyr-7B-Alpha has not been aligned to human preferences with techniques like RLHF or deployed with in-the-loop filtering of responses like ChatGPT, so the model can produce problematic outputs (especially when prompted to do so).\nIt is also unknown what the size and composition of the corpus was used to train the base model (`mistralai/Mistral-7B-v0.1`), however it is likely to have included a mix of Web data and technical sources like books and code. See the [Falcon 180B model card](https://huggingface.co/tiiuae/falcon-180B#training-data) for an example of this.\n\n## Training and evaluation data\n\nZephyr 7B Alpha achieves the following results on the evaluation set:\n\n- Loss: 0.4605\n- Rewards/chosen: -0.5053\n- Rewards/rejected: -1.8752\n- Rewards/accuracies: 0.7812\n- Rewards/margins: 1.3699\n- Logps/rejected: -327.4286\n- Logps/chosen: -297.1040\n- Logits/rejected: -2.7153\n- Logits/chosen: -2.7447\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n\n- learning_rate: 5e-07\n- train_batch_size: 2\n- eval_batch_size: 4\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 16\n- total_train_batch_size: 32\n- total_eval_batch_size: 64\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 1\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |\n|:-------------:|:-----:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|\n|    0.5602     | 0.05  | 100  |     0.5589      |    -0.3359     |     -0.8168      |       0.7188       |     0.4809      |   -306.2607    |  -293.7161   |     -2.6554     |    -2.6797    |\n|    0.4852     |  0.1  | 200  |     0.5136      |    -0.5310     |     -1.4994      |       0.8125       |     0.9684      |   -319.9124    |  -297.6181   |     -2.5762     |    -2.5957    |\n|    0.5212     | 0.15  | 300  |     0.5168      |    -0.1686     |     -1.1760      |       0.7812       |     1.0074      |   -313.4444    |  -290.3699   |     -2.6865     |    -2.7125    |\n|    0.5496     | 0.21  | 400  |     0.4835      |    -0.1617     |     -1.7170      |       0.8281       |     1.5552      |   -324.2635    |  -290.2326   |     -2.7947     |    -2.8218    |\n|    0.5209     | 0.26  | 500  |     0.5054      |    -0.4778     |     -1.6604      |       0.7344       |     1.1826      |   -323.1325    |  -296.5546   |     -2.8388     |    -2.8667    |\n|    0.4617     | 0.31  | 600  |     0.4910      |    -0.3738     |     -1.5180      |       0.7656       |     1.1442      |   -320.2848    |  -294.4741   |     -2.8234     |    -2.8521    |\n|    0.4452     | 0.36  | 700  |     0.4838      |    -0.4591     |     -1.6576      |       0.7031       |     1.1986      |   -323.0770    |  -296.1796   |     -2.7401     |    -2.7653    |\n|    0.4674     | 0.41  | 800  |     0.5077      |    -0.5692     |     -1.8659      |       0.7656       |     1.2967      |   -327.2416    |  -298.3818   |     -2.6740     |    -2.6945    |\n|    0.4656     | 0.46  | 900  |     0.4927      |    -0.5279     |     -1.6614      |       0.7656       |     1.1335      |   -323.1518    |  -297.5553   |     -2.7817     |    -2.8015    |\n|    0.4102     | 0.52  | 1000 |     0.4772      |    -0.5767     |     -2.0667      |       0.7656       |     1.4900      |   -331.2578    |  -298.5311   |     -2.7160     |    -2.7455    |\n|    0.4663     | 0.57  | 1100 |     0.4740      |    -0.8038     |     -2.1018      |       0.7656       |     1.2980      |   -331.9604    |  -303.0741   |     -2.6994     |    -2.7257    |\n|    0.4737     | 0.62  | 1200 |     0.4716      |    -0.3783     |     -1.7015      |       0.7969       |     1.3232      |   -323.9545    |  -294.5634   |     -2.6842     |    -2.7135    |\n|    0.4259     | 0.67  | 1300 |     0.4866      |    -0.6239     |     -1.9703      |       0.7812       |     1.3464      |   -329.3312    |  -299.4761   |     -2.7046     |    -2.7356    |\n|    0.4935     | 0.72  | 1400 |     0.4747      |    -0.5626     |     -1.7600      |       0.7812       |     1.1974      |   -325.1243    |  -298.2491   |     -2.7153     |    -2.7444    |\n|    0.4211     | 0.77  | 1500 |     0.4645      |    -0.6099     |     -1.9993      |       0.7656       |     1.3894      |   -329.9109    |  -299.1959   |     -2.6944     |    -2.7236    |\n|    0.4931     | 0.83  | 1600 |     0.4684      |    -0.6798     |     -2.1082      |       0.7656       |     1.4285      |   -332.0890    |  -300.5934   |     -2.7006     |    -2.7305    |\n|    0.5029     | 0.88  | 1700 |     0.4595      |    -0.5063     |     -1.8951      |       0.7812       |     1.3889      |   -327.8267    |  -297.1233   |     -2.7108     |    -2.7403    |\n|    0.4965     | 0.93  | 1800 |     0.4613      |    -0.5561     |     -1.9079      |       0.7812       |     1.3518      |   -328.0831    |  -298.1203   |     -2.7226     |    -2.7523    |\n|    0.4337     | 0.98  | 1900 |     0.4608      |    -0.5066     |     -1.8718      |       0.7656       |     1.3652      |   -327.3599    |  -297.1296   |     -2.7175     |    -2.7469    |\n\n### Framework versions\n\n- Transformers 4.34.0\n- Pytorch 2.0.1+cu118\n- Datasets 2.12.0\n- Tokenizers 0.14.0","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":0,"views":18,"requests":"0","sort":null,"version":"v1-Sat Jan 13 03:38:59 GMT 2024","tags":["Text2Text Generation"],"playground":false,"createTime":"2024-01-13T03:39:02","updateBy":"huggingface","updateTime":"2024-01-13T03:56:11"},{"id":457,"versionId":3591,"owner":"LLM360","applicationName":"AmberChat","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1704852224977amber_logo.png","description":"# AmberChat\n\nWe present AmberChat, an instruction following model finetuned from [LLM360/Amber](https://huggingface.co/LLM360/Amber).\n\n## Model Description\n\n- **Model type:** Language model with the same architecture as LLaMA-7B\n- **Language(s) (NLP):** English\n- **License:** Apache 2.0\n- **Resources for more information:**\n  - [Metrics](https://github.com/LLM360/Analysis360)\n  - [Fully processed Amber pretraining data](https://huggingface.co/datasets/LLM360/AmberDatasets)\n\n# Loading AmberChat\n\n```python\nimport torch\nfrom transformers import LlamaTokenizer, LlamaForCausalLM\n\ntokenizer = LlamaTokenizer.from_pretrained(\"LLM360/AmberChat\")\nmodel = LlamaForCausalLM.from_pretrained(\"LLM360/AmberChat\")\n\n#template adapated from fastchat\ntemplate= \"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\\n### Human: Got any creative ideas for a 10 year old’s birthday?\\n### Assistant: Of course! Here are some creative ideas for a 10-year-old's birthday party:\\n1. Treasure Hunt: Organize a treasure hunt in your backyard or nearby park. Create clues and riddles for the kids to solve, leading them to hidden treasures and surprises.\\n2. Science Party: Plan a science-themed party where kids can engage in fun and interactive experiments. You can set up different stations with activities like making slime, erupting volcanoes, or creating simple chemical reactions.\\n3. Outdoor Movie Night: Set up a backyard movie night with a projector and a large screen or white sheet. Create a cozy seating area with blankets and pillows, and serve popcorn and snacks while the kids enjoy a favorite movie under the stars.\\n4. DIY Crafts Party: Arrange a craft party where kids can unleash their creativity. Provide a variety of craft supplies like beads, paints, and fabrics, and let them create their own unique masterpieces to take home as party favors.\\n5. Sports Olympics: Host a mini Olympics event with various sports and games. Set up different stations for activities like sack races, relay races, basketball shooting, and obstacle courses. Give out medals or certificates to the participants.\\n6. Cooking Party: Have a cooking-themed party where the kids can prepare their own mini pizzas, cupcakes, or cookies. Provide toppings, frosting, and decorating supplies, and let them get hands-on in the kitchen.\\n7. Superhero Training Camp: Create a superhero-themed party where the kids can engage in fun training activities. Set up an obstacle course, have them design their own superhero capes or masks, and organize superhero-themed games and challenges.\\n8. Outdoor Adventure: Plan an outdoor adventure party at a local park or nature reserve. Arrange activities like hiking, nature scavenger hunts, or a picnic with games. Encourage exploration and appreciation for the outdoors.\\nRemember to tailor the activities to the birthday child's interests and preferences. Have a great celebration!\\n### Human: {prompt}\\n### Assistant:\"\n\nprompt = \"How do I mount a tv to drywall safely?\"\n\ninput_str = template.format(prompt=prompt)\ninput_ids = tokenizer(input_str, return_tensors=\"pt\").input_ids\noutputs = model.generate(input_ids, max_length=1000)\nprint(tokenizer.batch_decode(outputs[:, input_ids.shape[1]:-1])[0].strip())\n```\n\n# AmberChat Finetuning Details\n\n## DataMix\n\n| Subset                                  | Number of rows | License |\n|-----------------------------------------|----------------|---------|\n| WizardLM/WizardLM_evol_instruct_V2_196k | 143k           |         |\n| icybee/share_gpt_90k_v1                 | 90k            | cc0-1.0 |\n| Total                                   | 233k           |         |\n\n## Hyperparameters\n\n| Hyperparameter            | Value |\n|---------------------------|-------|\n| Total Parameters          | 6.7B  |\n| Hidden Size               | 4096  |\n| Intermediate Size (MLPs)  | 11008 |\n| Number of Attention Heads | 32    |\n| Number of Hidden Lyaers   | 32    |\n| RMSNorm epsilon           | 1e^-6 |\n| Max Seq Length            | 2048  |\n| Vocab Size                | 32000 |\n\n| Training Hyperparameter     | Value |\n|-----------------------------|-------|\n| learning_rate               | 2e-5  |\n| num_train_epochs            | 3     |\n| per_device_train_batch_size | 2     |\n| gradient_accumulation_steps | 16    |\n| warmup_ratio                | 0.04  |\n| model_max_length            | 2048  |\n\n# Evaluation\n\n| Model                                                                    | MT-Bench     | \n|--------------------------------------------------------------------------|--------------|\n| **LLM360/AmberChat**                                                     | **5.428125** |\n| [LLM360/Amber](https://huggingface.co/LLM360/Amber)                      | 2.48750      |\n| [Falcon-40B-Instruct](https://huggingface.co/tiiuae/falcon-40b-instruct) | 5.17         |\n| [MPT-7B-Chat](https://huggingface.co/mosaicml/mpt-7b-chat)               | 5.42         |\n| [Nous-Hermes-13B](https://huggingface.co/NousResearch/Nous-Hermes-13b)   | 5.51         |\n\n# Citation\n\n**BibTeX:**\n\n```bibtex\n@misc{liu2023llm360,\n      title={LLM360: Towards Fully Transparent Open-Source LLMs}, \n      author={Zhengzhong Liu and Aurick Qiao and Willie Neiswanger and Hongyi Wang and Bowen Tan and Tianhua Tao and Junbo Li and Yuqi Wang and Suqi Sun and Omkar Pangarkar and Richard Fan and Yi Gu and Victor Miller and Yonghao Zhuang and Guowei He and Haonan Li and Fajri Koto and Liping Tang and Nikhil Ranjan and Zhiqiang Shen and Xuguang Ren and Roberto Iriondo and Cun Mu and Zhiting Hu and Mark Schulze and Preslav Nakov and Tim Baldwin and Eric P. Xing},\n      year={2023},\n      eprint={2312.06550},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":0,"views":25,"requests":"0","sort":null,"version":"v1-Sat Jan 13 03:37:04 GMT 2024","tags":["Text2Text Generation"],"playground":false,"createTime":"2024-01-13T03:37:44","updateBy":"LLM360","updateTime":"2024-01-13T03:56:12"},{"id":714,"versionId":4081,"owner":"google","applicationName":"gemma-2b-it","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1708575009630gemma.webp","description":"# Gemma Model Card\n\n**Model Page**: [Gemma](https://ai.google.dev/gemma/docs)\n\nThis model card corresponds to the instruct version of the Gemma model. You can also visit the model card of\nthe [2B base model](https://huggingface.co/google/gemma-2b),\nand [7B base model](https://huggingface.co/google/gemma-7b).\n\n**Resources and Technical Documentation**:\n\n* [Responsible Generative AI Toolkit](https://ai.google.dev/responsible)\n* [Gemma on Kaggle](https://www.kaggle.com/models/google/gemma)\n* [Gemma on Vertex Model Garden](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/335?version=gemma-2b-it-gg-hf)\n\n**Terms of Use**: [Terms](https://www.kaggle.com/models/google/gemma/license/consent)\n\n**Authors**: Google\n\n## Model Information\n\nSummary description and brief definition of inputs and outputs.\n\n### Description\n\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nThey are text-to-text, decoder-only large language models, available in English,\nwith open weights, pre-trained variants, and instruction-tuned variants. Gemma\nmodels are well-suited for a variety of text generation tasks, including\nquestion answering, summarization, and reasoning. Their relatively small size\nmakes it possible to deploy them in environments with limited resources such as\na laptop, desktop or your own cloud infrastructure, democratizing access to\nstate of the art AI models and helping foster innovation for everyone.\n\n### Inputs and outputs\n\n* **Input:** Text string, such as a question, a prompt, or a document to be\n  summarized.\n* **Output:** Generated English-language text in response to the input, such\n  as an answer to a question, or a summary of a document.\n\n## Model Data\n\nData used for model training and how the data was processed.\n\n### Training Dataset\n\nThese models were trained on a dataset of text data that includes a wide variety\nof sources, totaling 6 trillion tokens. Here are the key components:\n\n* Web Documents: A diverse collection of web text ensures the model is exposed\n  to a broad range of linguistic styles, topics, and vocabulary. Primarily\n  English-language content.\n* Code: Exposing the model to code helps it to learn the syntax and patterns of\n  programming languages, which improves its ability to generate code or\n  understand code-related questions.\n* Mathematics: Training on mathematical text helps the model learn logical\n  reasoning, symbolic representation, and to address mathematical queries.\n\nThe combination of these diverse data sources is crucial for training a powerful\nlanguage model that can handle a wide variety of different tasks and text\nformats.\n\n### Data Preprocessing\n\nHere are the key data cleaning and filtering methods applied to the training\ndata:\n\n* CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering was\n  applied at multiple stages in the data preparation process to ensure the\n  exclusion of harmful and illegal content\n* Sensitive Data Filtering: As part of making Gemma pre-trained models safe and\n  reliable, automated techniques were used to filter out certain personal\n  information and other sensitive data from training sets.\n* Additional methods: Filtering based on content quality and safely in line with\n  [our policies](https://storage.googleapis.com/gweb-uniblog-publish-prod/documents/2023_Google_AI_Principles_Progress_Update.pdf#page=11).\n\n## Implementation Information\n\nDetails about the model internals.\n\n### Hardware\n\nGemma was trained using the latest generation of\n[Tensor Processing Unit (TPU)](https://cloud.google.com/tpu/docs/intro-to-tpu) hardware (TPUv5e).\n\nTraining large language models requires significant computational power. TPUs,\ndesigned specifically for matrix operations common in machine learning, offer\nseveral advantages in this domain:\n\n* Performance: TPUs are specifically designed to handle the massive computations\n  involved in training LLMs. They can speed up training considerably compared to\n  CPUs.\n* Memory: TPUs often come with large amounts of high-bandwidth memory, allowing\n  for the handling of large models and batch sizes during training. This can\n  lead to better model quality.\n* Scalability: TPU Pods (large clusters of TPUs) provide a scalable solution for\n  handling the growing complexity of large foundation models. You can distribute\n  training across multiple TPU devices for faster and more efficient processing.\n* Cost-effectiveness: In many scenarios, TPUs can provide a more cost-effective\n  solution for training large models compared to CPU-based infrastructure,\n  especially when considering the time and resources saved due to faster\n  training.\n* These advantages are aligned with\n  [Google's commitments to operate sustainably](https://sustainability.google/operating-sustainably/).\n\n### Software\n\nTraining was done using [JAX](https://github.com/google/jax)\nand [ML Pathways](https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/ml-pathways).\n\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models.\n\nML Pathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\n[foundation models](https://ai.google/discover/foundation-models/), including large language models like\nthese ones.\n\nTogether, JAX and ML Pathways are used as described in the\n[paper about the Gemini family of models](https://arxiv.org/abs/2312.11805); \"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"\n\n## Evaluation\n\nModel evaluation metrics and results.\n\n### Benchmark Results\n\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\n\n| Benchmark                                                                          | Metric        | 2B Params   | 7B Params |\n|------------------------------------------------------------------------------------|---------------|-------------|-----------|\n| [MMLU](https://arxiv.org/abs/2009.03300)                                           | 5-shot, top-1 | 42.3        | 64.3      |\n| [HellaSwag](https://arxiv.org/abs/1905.07830)                                      | 0-shot        | 71.4        | 81.2      |\n| [PIQA](https://arxiv.org/abs/1911.11641)                                           | 0-shot        | 77.3        | 81.2      |\n| [SocialIQA](https://arxiv.org/abs/1904.09728)                                      | 0-shot        | 59.7        | 51.8      |\n| [BooIQ](https://arxiv.org/abs/1905.10044)                                          | 0-shot        | 69.4        | 83.2      |\n| [WinoGrande](https://arxiv.org/abs/1907.10641)                                     | partial score | 65.4        | 72.3      |\n| [CommonsenseQA](https://arxiv.org/abs/1811.00937)                                  | 7-shot        | 65.3        | 71.3      |\n| [OpenBookQA](https://arxiv.org/abs/1809.02789)                                     |               | 47.8        | 52.8      |\n| [ARC-e](https://arxiv.org/abs/1911.01547)                                          |               | 73.2        | 81.5      |\n| [ARC-c](https://arxiv.org/abs/1911.01547)                                          |               | 42.1        | 53.2      |\n| [TriviaQA](https://arxiv.org/abs/1705.03551)                                       | 5-shot        | 53.2        | 63.4      |\n| [Natural Questions](https://github.com/google-research-datasets/natural-questions) | 5-shot        | -           | 23        |\n| [HumanEval](https://arxiv.org/abs/2107.03374)                                      | pass@1        | 22.0        | 32.3      |\n| [MBPP](https://arxiv.org/abs/2108.07732)                                           | 3-shot        | 29.2        | 44.4      |\n| [GSM8K](https://arxiv.org/abs/2110.14168)                                          | maj@1         | 17.7        | 46.4      |\n| [MATH](https://arxiv.org/abs/2108.07732)                                           | 4-shot        | 11.8        | 24.3      |\n| [AGIEval](https://arxiv.org/abs/2304.06364)                                        |               | 24.2        | 41.7      |\n| [BIG-Bench](https://arxiv.org/abs/2206.04615)                                      |               | 35.2        | 55.1      |\n| ------------------------------                                                     | ------------- | ----------- | --------- |\n| **Average**                                                                        |               | **54.0**    | **56.4**  |\n\n## Ethics and Safety\n\nEthics and safety evaluation approach and results.\n\n### Evaluation Approach\n\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\n\n* Text-to-Text Content Safety: Human evaluation on prompts covering safety\n  policies including child sexual abuse and exploitation, harassment, violence\n  and gore, and hate speech.\n* Text-to-Text Representational Harms: Benchmark against relevant academic\n  datasets such as [WinoBias](https://arxiv.org/abs/1804.06876) and [BBQ Dataset](https://arxiv.org/abs/2110.08193v2).\n* Memorization: Automated evaluation of memorization of training data, including\n  the risk of personally identifiable information exposure.\n* Large-scale harm: Tests for \"dangerous capabilities,\" such as chemical,\n  biological, radiological, and nuclear (CBRN) risks.\n\n### Evaluation Results\n\nThe results of ethics and safety evaluations are within acceptable thresholds\nfor\nmeeting [internal policies](https://storage.googleapis.com/gweb-uniblog-publish-prod/documents/2023_Google_AI_Principles_Progress_Update.pdf#page=11)\nfor categories such as child\nsafety, content safety, representational harms, memorization, large-scale harms.\nOn top of robust internal evaluations, the results of well known safety\nbenchmarks like BBQ, BOLD, Winogender, Winobias, RealToxicity, and TruthfulQA\nare shown here.\n\n| Benchmark                                                    | Metric        | 2B Params   | 7B Params |\n|--------------------------------------------------------------|---------------|-------------|-----------|\n| [RealToxicity](https://arxiv.org/abs/2009.11462)             | average       | 6.86        | 7.90      |\n| [BOLD](https://arxiv.org/abs/2101.11718)                     |               | 45.57       | 49.08     |\n| [CrowS-Pairs](https://aclanthology.org/2020.emnlp-main.154/) | top-1         | 45.82       | 51.33     |\n| [BBQ Ambig](https://arxiv.org/abs/2110.08193v2)              | 1-shot, top-1 | 62.58       | 92.54     |\n| [BBQ Disambig](https://arxiv.org/abs/2110.08193v2)           | top-1         | 54.62       | 71.99     |\n| [Winogender](https://arxiv.org/abs/1804.09301)               | top-1         | 51.25       | 54.17     |\n| [TruthfulQA](https://arxiv.org/abs/2109.07958)               |               | 44.84       | 31.81     |\n| [Winobias 1_2](https://arxiv.org/abs/1804.06876)             |               | 56.12       | 59.09     |\n| [Winobias 2_2](https://arxiv.org/abs/1804.06876)             |               | 91.10       | 92.23     |\n| [Toxigen](https://arxiv.org/abs/2203.09509)                  |               | 29.77       | 39.59     |\n| ------------------------------                               | ------------- | ----------- | --------- |\n\n## Usage and Limitations\n\nThese models have certain limitations that users should be aware of.\n\n### Intended Usage\n\nOpen Large Language Models (LLMs) have a wide range of applications across\nvarious industries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\n\n* Content Creation and Communication\n  * Text Generation: These models can be used to generate creative text formats\n    such as poems, scripts, code, marketing copy, and email drafts.\n  * Chatbots and Conversational AI: Power conversational interfaces for customer\n    service, virtual assistants, or interactive applications.\n  * Text Summarization: Generate concise summaries of a text corpus, research\n    papers, or reports.\n* Research and Education\n  * Natural Language Processing (NLP) Research: These models can serve as a\n    foundation for researchers to experiment with NLP techniques, develop\n    algorithms, and contribute to the advancement of the field.\n  * Language Learning Tools: Support interactive language learning experiences,\n    aiding in grammar correction or providing writing practice.\n  * Knowledge Exploration: Assist researchers in exploring large bodies of text\n    by generating summaries or answering questions about specific topics.\n\n### Limitations\n\n* Training Data\n  * The quality and diversity of the training data significantly influence the\n    model's capabilities. Biases or gaps in the training data can lead to\n    limitations in the model's responses.\n  * The scope of the training dataset determines the subject areas the model can\n    handle effectively.\n* Context and Task Complexity\n  * LLMs are better at tasks that can be framed with clear prompts and\n    instructions. Open-ended or highly complex tasks might be challenging.\n  * A model's performance can be influenced by the amount of context provided\n    (longer context generally leads to better outputs, up to a certain point).\n* Language Ambiguity and Nuance\n  * Natural language is inherently complex. LLMs might struggle to grasp subtle\n    nuances, sarcasm, or figurative language.\n* Factual Accuracy\n  * LLMs generate responses based on information they learned from their\n    training datasets, but they are not knowledge bases. They may generate\n    incorrect or outdated factual statements.\n* Common Sense\n  * LLMs rely on statistical patterns in language. They might lack the ability\n    to apply common sense reasoning in certain situations.\n\n### Ethical Considerations and Risks\n\nThe development of large language models (LLMs) raises several ethical concerns.\nIn creating an open model, we have carefully considered the following:\n\n* Bias and Fairness\n  * LLMs trained on large-scale, real-world text data can reflect socio-cultural\n    biases embedded in the training material. These models underwent careful\n    scrutiny, input data pre-processing described and posterior evaluations\n    reported in this card.\n* Misinformation and Misuse\n  * LLMs can be misused to generate text that is false, misleading, or harmful.\n  * Guidelines are provided for responsible use with the model, see the\n    [Responsible Generative AI Toolkit](http://ai.google.dev/gemma/responsible).\n* Transparency and Accountability:\n  * This model card summarizes details on the models' architecture,\n    capabilities, limitations, and evaluation processes.\n  * A responsibly developed open model offers the opportunity to share\n    innovation by making LLM technology accessible to developers and researchers\n    across the AI ecosystem.\n\nRisks identified and mitigations:\n\n* Perpetuation of biases: It's encouraged to perform continuous monitoring\n  (using evaluation metrics, human review) and the exploration of de-biasing\n  techniques during model training, fine-tuning, and other use cases.\n* Generation of harmful content: Mechanisms and guidelines for content safety\n  are essential. Developers are encouraged to exercise caution and implement\n  appropriate content safety safeguards based on their specific product policies\n  and application use cases.\n* Misuse for malicious purposes: Technical limitations and developer and\n  end-user education can help mitigate against malicious applications of LLMs.\n  Educational resources and reporting mechanisms for users to flag misuse are\n  provided. Prohibited uses of Gemma models are outlined in the\n  [Gemma Prohibited Use Policy](https://ai.google.dev/gemma/prohibited_use_policy).\n* Privacy violations: Models were trained on data filtered for removal of PII\n  (Personally Identifiable Information). Developers are encouraged to adhere to\n  privacy regulations with privacy-preserving techniques.\n\n### Benefits\n\nAt the time of release, this family of models provides high-performance open\nlarge language model implementations designed from the ground up for Responsible\nAI development compared to similarly sized models.\n\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.","modelId":1,"dataId":1,"platformId":-1,"users":null,"favorites":null,"views":null,"requests":"0","sort":null,"version":"v1-Thu Feb 29 20:26:09 GMT 2024","tags":["Text2Text Generation"],"playground":false,"createTime":"2024-02-29T20:26:11","updateBy":"google","updateTime":"2024-02-29T20:26:27"},{"id":715,"versionId":4082,"owner":"google","applicationName":"gemma-7b-it","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1708575068467gemma.webp","description":"# Gemma Model Card\n\n**Model Page**: [Gemma](https://ai.google.dev/gemma/docs)\n\nThis model card corresponds to the instruct version of the Gemma model. You can also visit the model card of\nthe [2B base model](https://huggingface.co/google/gemma-2b),\nand [7B base model](https://huggingface.co/google/gemma-7b).\n\n**Resources and Technical Documentation**:\n\n* [Responsible Generative AI Toolkit](https://ai.google.dev/responsible)\n* [Gemma on Kaggle](https://www.kaggle.com/models/google/gemma)\n* [Gemma on Vertex Model Garden](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/335?version=gemma-2b-it-gg-hf)\n\n**Terms of Use**: [Terms](https://www.kaggle.com/models/google/gemma/license/consent)\n\n**Authors**: Google\n\n## Model Information\n\nSummary description and brief definition of inputs and outputs.\n\n### Description\n\nGemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nThey are text-to-text, decoder-only large language models, available in English,\nwith open weights, pre-trained variants, and instruction-tuned variants. Gemma\nmodels are well-suited for a variety of text generation tasks, including\nquestion answering, summarization, and reasoning. Their relatively small size\nmakes it possible to deploy them in environments with limited resources such as\na laptop, desktop or your own cloud infrastructure, democratizing access to\nstate of the art AI models and helping foster innovation for everyone.\n\n### Inputs and outputs\n\n* **Input:** Text string, such as a question, a prompt, or a document to be\n  summarized.\n* **Output:** Generated English-language text in response to the input, such\n  as an answer to a question, or a summary of a document.\n\n## Model Data\n\nData used for model training and how the data was processed.\n\n### Training Dataset\n\nThese models were trained on a dataset of text data that includes a wide variety\nof sources, totaling 6 trillion tokens. Here are the key components:\n\n* Web Documents: A diverse collection of web text ensures the model is exposed\n  to a broad range of linguistic styles, topics, and vocabulary. Primarily\n  English-language content.\n* Code: Exposing the model to code helps it to learn the syntax and patterns of\n  programming languages, which improves its ability to generate code or\n  understand code-related questions.\n* Mathematics: Training on mathematical text helps the model learn logical\n  reasoning, symbolic representation, and to address mathematical queries.\n\nThe combination of these diverse data sources is crucial for training a powerful\nlanguage model that can handle a wide variety of different tasks and text\nformats.\n\n### Data Preprocessing\n\nHere are the key data cleaning and filtering methods applied to the training\ndata:\n\n* CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering was\n  applied at multiple stages in the data preparation process to ensure the\n  exclusion of harmful and illegal content\n* Sensitive Data Filtering: As part of making Gemma pre-trained models safe and\n  reliable, automated techniques were used to filter out certain personal\n  information and other sensitive data from training sets.\n* Additional methods: Filtering based on content quality and safely in line with\n  [our policies](https://storage.googleapis.com/gweb-uniblog-publish-prod/documents/2023_Google_AI_Principles_Progress_Update.pdf#page=11).\n\n## Implementation Information\n\nDetails about the model internals.\n\n### Hardware\n\nGemma was trained using the latest generation of\n[Tensor Processing Unit (TPU)](https://cloud.google.com/tpu/docs/intro-to-tpu) hardware (TPUv5e).\n\nTraining large language models requires significant computational power. TPUs,\ndesigned specifically for matrix operations common in machine learning, offer\nseveral advantages in this domain:\n\n* Performance: TPUs are specifically designed to handle the massive computations\n  involved in training LLMs. They can speed up training considerably compared to\n  CPUs.\n* Memory: TPUs often come with large amounts of high-bandwidth memory, allowing\n  for the handling of large models and batch sizes during training. This can\n  lead to better model quality.\n* Scalability: TPU Pods (large clusters of TPUs) provide a scalable solution for\n  handling the growing complexity of large foundation models. You can distribute\n  training across multiple TPU devices for faster and more efficient processing.\n* Cost-effectiveness: In many scenarios, TPUs can provide a more cost-effective\n  solution for training large models compared to CPU-based infrastructure,\n  especially when considering the time and resources saved due to faster\n  training.\n* These advantages are aligned with\n  [Google's commitments to operate sustainably](https://sustainability.google/operating-sustainably/).\n\n### Software\n\nTraining was done using [JAX](https://github.com/google/jax)\nand [ML Pathways](https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/ml-pathways).\n\nJAX allows researchers to take advantage of the latest generation of hardware,\nincluding TPUs, for faster and more efficient training of large models.\n\nML Pathways is Google's latest effort to build artificially intelligent systems\ncapable of generalizing across multiple tasks. This is specially suitable for\n[foundation models](https://ai.google/discover/foundation-models/), including large language models like\nthese ones.\n\nTogether, JAX and ML Pathways are used as described in the\n[paper about the Gemini family of models](https://arxiv.org/abs/2312.11805); \"the 'single\ncontroller' programming model of Jax and Pathways allows a single Python\nprocess to orchestrate the entire training run, dramatically simplifying the\ndevelopment workflow.\"\n\n## Evaluation\n\nModel evaluation metrics and results.\n\n### Benchmark Results\n\nThese models were evaluated against a large collection of different datasets and\nmetrics to cover different aspects of text generation:\n\n| Benchmark                                                                          | Metric        | 2B Params   | 7B Params |\n|------------------------------------------------------------------------------------|---------------|-------------|-----------|\n| [MMLU](https://arxiv.org/abs/2009.03300)                                           | 5-shot, top-1 | 42.3        | 64.3      |\n| [HellaSwag](https://arxiv.org/abs/1905.07830)                                      | 0-shot        | 71.4        | 81.2      |\n| [PIQA](https://arxiv.org/abs/1911.11641)                                           | 0-shot        | 77.3        | 81.2      |\n| [SocialIQA](https://arxiv.org/abs/1904.09728)                                      | 0-shot        | 59.7        | 51.8      |\n| [BooIQ](https://arxiv.org/abs/1905.10044)                                          | 0-shot        | 69.4        | 83.2      |\n| [WinoGrande](https://arxiv.org/abs/1907.10641)                                     | partial score | 65.4        | 72.3      |\n| [CommonsenseQA](https://arxiv.org/abs/1811.00937)                                  | 7-shot        | 65.3        | 71.3      |\n| [OpenBookQA](https://arxiv.org/abs/1809.02789)                                     |               | 47.8        | 52.8      |\n| [ARC-e](https://arxiv.org/abs/1911.01547)                                          |               | 73.2        | 81.5      |\n| [ARC-c](https://arxiv.org/abs/1911.01547)                                          |               | 42.1        | 53.2      |\n| [TriviaQA](https://arxiv.org/abs/1705.03551)                                       | 5-shot        | 53.2        | 63.4      |\n| [Natural Questions](https://github.com/google-research-datasets/natural-questions) | 5-shot        | -           | 23        |\n| [HumanEval](https://arxiv.org/abs/2107.03374)                                      | pass@1        | 22.0        | 32.3      |\n| [MBPP](https://arxiv.org/abs/2108.07732)                                           | 3-shot        | 29.2        | 44.4      |\n| [GSM8K](https://arxiv.org/abs/2110.14168)                                          | maj@1         | 17.7        | 46.4      |\n| [MATH](https://arxiv.org/abs/2108.07732)                                           | 4-shot        | 11.8        | 24.3      |\n| [AGIEval](https://arxiv.org/abs/2304.06364)                                        |               | 24.2        | 41.7      |\n| [BIG-Bench](https://arxiv.org/abs/2206.04615)                                      |               | 35.2        | 55.1      |\n| ------------------------------                                                     | ------------- | ----------- | --------- |\n| **Average**                                                                        |               | **54.0**    | **56.4**  |\n\n## Ethics and Safety\n\nEthics and safety evaluation approach and results.\n\n### Evaluation Approach\n\nOur evaluation methods include structured evaluations and internal red-teaming\ntesting of relevant content policies. Red-teaming was conducted by a number of\ndifferent teams, each with different goals and human evaluation metrics. These\nmodels were evaluated against a number of different categories relevant to\nethics and safety, including:\n\n* Text-to-Text Content Safety: Human evaluation on prompts covering safety\n  policies including child sexual abuse and exploitation, harassment, violence\n  and gore, and hate speech.\n* Text-to-Text Representational Harms: Benchmark against relevant academic\n  datasets such as [WinoBias](https://arxiv.org/abs/1804.06876) and [BBQ Dataset](https://arxiv.org/abs/2110.08193v2).\n* Memorization: Automated evaluation of memorization of training data, including\n  the risk of personally identifiable information exposure.\n* Large-scale harm: Tests for \"dangerous capabilities,\" such as chemical,\n  biological, radiological, and nuclear (CBRN) risks.\n\n### Evaluation Results\n\nThe results of ethics and safety evaluations are within acceptable thresholds\nfor\nmeeting [internal policies](https://storage.googleapis.com/gweb-uniblog-publish-prod/documents/2023_Google_AI_Principles_Progress_Update.pdf#page=11)\nfor categories such as child\nsafety, content safety, representational harms, memorization, large-scale harms.\nOn top of robust internal evaluations, the results of well known safety\nbenchmarks like BBQ, BOLD, Winogender, Winobias, RealToxicity, and TruthfulQA\nare shown here.\n\n| Benchmark                                                    | Metric        | 2B Params   | 7B Params |\n|--------------------------------------------------------------|---------------|-------------|-----------|\n| [RealToxicity](https://arxiv.org/abs/2009.11462)             | average       | 6.86        | 7.90      |\n| [BOLD](https://arxiv.org/abs/2101.11718)                     |               | 45.57       | 49.08     |\n| [CrowS-Pairs](https://aclanthology.org/2020.emnlp-main.154/) | top-1         | 45.82       | 51.33     |\n| [BBQ Ambig](https://arxiv.org/abs/2110.08193v2)              | 1-shot, top-1 | 62.58       | 92.54     |\n| [BBQ Disambig](https://arxiv.org/abs/2110.08193v2)           | top-1         | 54.62       | 71.99     |\n| [Winogender](https://arxiv.org/abs/1804.09301)               | top-1         | 51.25       | 54.17     |\n| [TruthfulQA](https://arxiv.org/abs/2109.07958)               |               | 44.84       | 31.81     |\n| [Winobias 1_2](https://arxiv.org/abs/1804.06876)             |               | 56.12       | 59.09     |\n| [Winobias 2_2](https://arxiv.org/abs/1804.06876)             |               | 91.10       | 92.23     |\n| [Toxigen](https://arxiv.org/abs/2203.09509)                  |               | 29.77       | 39.59     |\n| ------------------------------                               | ------------- | ----------- | --------- |\n\n## Usage and Limitations\n\nThese models have certain limitations that users should be aware of.\n\n### Intended Usage\n\nOpen Large Language Models (LLMs) have a wide range of applications across\nvarious industries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\n\n* Content Creation and Communication\n  * Text Generation: These models can be used to generate creative text formats\n    such as poems, scripts, code, marketing copy, and email drafts.\n  * Chatbots and Conversational AI: Power conversational interfaces for customer\n    service, virtual assistants, or interactive applications.\n  * Text Summarization: Generate concise summaries of a text corpus, research\n    papers, or reports.\n* Research and Education\n  * Natural Language Processing (NLP) Research: These models can serve as a\n    foundation for researchers to experiment with NLP techniques, develop\n    algorithms, and contribute to the advancement of the field.\n  * Language Learning Tools: Support interactive language learning experiences,\n    aiding in grammar correction or providing writing practice.\n  * Knowledge Exploration: Assist researchers in exploring large bodies of text\n    by generating summaries or answering questions about specific topics.\n\n### Limitations\n\n* Training Data\n  * The quality and diversity of the training data significantly influence the\n    model's capabilities. Biases or gaps in the training data can lead to\n    limitations in the model's responses.\n  * The scope of the training dataset determines the subject areas the model can\n    handle effectively.\n* Context and Task Complexity\n  * LLMs are better at tasks that can be framed with clear prompts and\n    instructions. Open-ended or highly complex tasks might be challenging.\n  * A model's performance can be influenced by the amount of context provided\n    (longer context generally leads to better outputs, up to a certain point).\n* Language Ambiguity and Nuance\n  * Natural language is inherently complex. LLMs might struggle to grasp subtle\n    nuances, sarcasm, or figurative language.\n* Factual Accuracy\n  * LLMs generate responses based on information they learned from their\n    training datasets, but they are not knowledge bases. They may generate\n    incorrect or outdated factual statements.\n* Common Sense\n  * LLMs rely on statistical patterns in language. They might lack the ability\n    to apply common sense reasoning in certain situations.\n\n### Ethical Considerations and Risks\n\nThe development of large language models (LLMs) raises several ethical concerns.\nIn creating an open model, we have carefully considered the following:\n\n* Bias and Fairness\n  * LLMs trained on large-scale, real-world text data can reflect socio-cultural\n    biases embedded in the training material. These models underwent careful\n    scrutiny, input data pre-processing described and posterior evaluations\n    reported in this card.\n* Misinformation and Misuse\n  * LLMs can be misused to generate text that is false, misleading, or harmful.\n  * Guidelines are provided for responsible use with the model, see the\n    [Responsible Generative AI Toolkit](http://ai.google.dev/gemma/responsible).\n* Transparency and Accountability:\n  * This model card summarizes details on the models' architecture,\n    capabilities, limitations, and evaluation processes.\n  * A responsibly developed open model offers the opportunity to share\n    innovation by making LLM technology accessible to developers and researchers\n    across the AI ecosystem.\n\nRisks identified and mitigations:\n\n* Perpetuation of biases: It's encouraged to perform continuous monitoring\n  (using evaluation metrics, human review) and the exploration of de-biasing\n  techniques during model training, fine-tuning, and other use cases.\n* Generation of harmful content: Mechanisms and guidelines for content safety\n  are essential. Developers are encouraged to exercise caution and implement\n  appropriate content safety safeguards based on their specific product policies\n  and application use cases.\n* Misuse for malicious purposes: Technical limitations and developer and\n  end-user education can help mitigate against malicious applications of LLMs.\n  Educational resources and reporting mechanisms for users to flag misuse are\n  provided. Prohibited uses of Gemma models are outlined in the\n  [Gemma Prohibited Use Policy](https://ai.google.dev/gemma/prohibited_use_policy).\n* Privacy violations: Models were trained on data filtered for removal of PII\n  (Personally Identifiable Information). Developers are encouraged to adhere to\n  privacy regulations with privacy-preserving techniques.\n\n### Benefits\n\nAt the time of release, this family of models provides high-performance open\nlarge language model implementations designed from the ground up for Responsible\nAI development compared to similarly sized models.\n\nUsing the benchmark evaluation metrics described in this document, these models\nhave shown to provide superior performance to other, comparably-sized open model\nalternatives.","modelId":1,"dataId":1,"platformId":-1,"users":null,"favorites":null,"views":null,"requests":"0","sort":null,"version":"v1-Thu Feb 29 20:26:00 GMT 2024","tags":["Text2Text Generation"],"playground":false,"createTime":"2024-02-29T20:26:12","updateBy":"google","updateTime":"2024-02-29T20:26:27"}]}}