{"message":"Succeeded to process
request","code":"SUCCESS","data":{"total":5,"totalPage":1,"pageNum":1,"pageSize":16,"data":[{"id":980,"versionId":4701,"owner":"deepseek-ai","applicationName":"deepseek-coder-6-7b-instruct","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1711747895103Uk1zNOj4_400x400.jpg","description":"###
1. Introduction of Deepseek Coder\n\nDeepseek Coder is composed of a series of
code language models, each trained from scratch on 2T tokens, with a composition
of 87% code and 13% natural language in both English and Chinese. We provide
various sizes of the code model, ranging from 1B to 33B versions. Each model is
pre-trained on project-level code corpus by employing a window size of 16K and a
extra fill-in-the-blank task, to support project-level code completion and
infilling. For coding capabilities, Deepseek Coder achieves state-of-the-art
performance among open-source code models on multiple programming languages and
various benchmarks.\n\n- **Massive Training Data**: Trained from scratch fon 2T
tokens, including 87% code and 13% linguistic data in both English and Chinese
languages.\n- **Highly Flexible & Scalable**: Offered in model sizes of 1.3B,
5.7B, 6.7B, and 33B, enabling users to choose the setup most suitable for their
requirements.\n- **Superior Model Performance**: State-of-the-art performance
among publicly available code models on HumanEval, MultiPL-E, MBPP, DS-1000, and
APPS benchmarks.\n- **Advanced Code Completion Capabilities**: A window size of
16K and a fill-in-the-blank task, supporting project-level code completion and
infilling tasks.\n\n### 2. Model Summary\n\ndeepseek-coder-6.7b-instruct is a
6.7B parameter model initialized from deepseek-coder-6.7b-base and fine-tuned on
2B tokens of instruction data.\n\n- **Home Page:**
[DeepSeek](https://deepseek.com/)\n- **Repository:**
[deepseek-ai/deepseek-coder](https://github.com/deepseek-ai/deepseek-coder)\n-
**Chat With DeepSeek Coder:**
[DeepSeek-Coder](https://coder.deepseek.com/)\n\n### 3. How to Use\n\nHere give
some examples of how to use our model.\n\n#### Chat Model
Inference\n\n```python\nfrom transformers import AutoTokenizer,
AutoModelForCausalLM\ntokenizer =
AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\",
trust_remote_code=True)\nmodel =
AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\",
trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\nmessages=[\n {
'role': 'user', 'content': \"write a quick sort algorithm in
python.\"}\n]\ninputs = tokenizer.apply_chat_template(messages,
add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n#
tokenizer.eos_token_id is the id of <|EOT|> token\noutputs =
model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50,
top_p=0.95, num_return_sequences=1,
eos_token_id=tokenizer.eos_token_id)\nprint(tokenizer.decode(outputs[0][len(inputs[0]):],
skip_special_tokens=True))\n```\n\n### 4. License\n\nThis code repository is
licensed under the MIT License. The use of DeepSeek Coder models is subject to
the Model License. DeepSeek Coder supports commercial use.\n\nSee the
[LICENSE-MODEL](https://github.com/deepseek-ai/deepseek-coder/blob/main/LICENSE-MODEL)
for more details.\n\n### 5. Contact\n\nIf you have any questions, please raise
an issue or contact us at
[agi_code@deepseek.com](mailto:agi_code@deepseek.com).","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":1,"views":154,"requests":"0","sort":null,"version":"v1-Fri
Mar 29 21:31:53 GMT 2024","tags":["Code
Generation"],"playground":false,"createTime":"2024-03-29T21:32:13","updateBy":"deepseek-ai","updateTime":"2024-03-29T21:48:34"},{"id":843,"versionId":4391,"owner":"defog","applicationName":"sqlcoder-70b-alpha","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1709766210228-L2fZf-T4VV81Na2q8luu.png.webp","description":"---\nlicense:
cc-by-sa-4.0\nlibrary_name: transformers\npipeline_tag:
text-generation\n---\n\n# Model Card for SQLCoder-70B-Alpha\n\nA capable large
language model for natural language to SQL generation. Outperforms all
generalist models (including GPT-4) on text to
SQL.\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/603bbad3fd770a9997b57cb6/3BVMV2z6FTEEPF1hJ2qu1.png)\n\n##
Model Details\n\n### Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\nThis
is the model card of a \uD83E\uDD17 transformers model that has been pushed on
the Hub. This model card has been automatically generated.\n\n- **Developed
by:** [Defog, Inc](https://defog.ai)\n- **Model type:** [Text to SQL]\n-
**License:** [CC-by-SA-4.0]\n- **Finetuned from model:** [CodeLlama-70B]\n\n###
Model Sources [optional]\n\n-
[**HuggingFace:**](https://huggingface.co/defog/sqlcoder-70b-alpha)\n-
[**GitHub:**](https://github.com/defog-ai/sqlcoder)\n-
[**Demo:**](https://defog.ai/sqlcoder-demo/)\n\n## Uses\n\nThis model is
intended to be used by non-technical users to understand data inside their SQL
databases. It is meant as an analytics tool, and not as a database admin
tool.\n\nThis model has not been trained to reject malicious requests from users
with write access to databases, and should only be used by users with read-only
access.\n\n## How to Get Started with the Model\n\nUse the code
[here](https://github.com/defog-ai/sqlcoder/blob/main/inference.py) to get
started with the model.\n\n## Evaluation\n\nThis model was evaluated on
[SQL-Eval](https://github.com/defog-ai/sql-eval), a PostgreSQL based evaluation
framework developed by Defog for testing and alignment of model
capabilities.\n\nYou can read more about the methodology behind SQLEval
[here](https://defog.ai/blog/open-sourcing-sqleval/).\n\n### Results\n\nWe
classified each generated question into one of 6 categories. The table displays
the percentage of questions answered correctly by each model, broken down by
category.\n\n| | date | group_by | order_by | ratio | join | where |\n|
------------- | ---- | -------- | -------- | ----- | ---- | ----- |\n|
sqlcoder-70b | 96 | 91.4 | 97.1 | 85.7 | 97.1 | 91.4 |\n| sqlcoder-34b | 80 |
94.3 | 85.7 | 77.1 | 85.7 | 80 |\n| gpt-4 | 64 | 94.3 | 88.6 | 74.2 | 85.7 | 80
|\n| sqlcoder2-15b | 76 | 80 | 77.1 | 60 | 77.1 | 77.1 |\n| sqlcoder-7b | 64 |
82.9 | 74.3 | 54.3 | 74.3 | 74.3 |\n| gpt-3.5 | 68 | 77.1 | 74.2 | 34.3 | 65.7 |
71.4 |\n| claude-2 | 52 | 71.4 | 74.3 | 57.1 | 65.7 | 62.9 |\n\n## Using
SQLCoder\n\n## Model Card Authors\n\n- [Rishabh
Srivastava](https://twitter.com/rishdotblog)\n- [Wendy
Aw](https://www.linkedin.com/in/wendyaw/)\n- [Wong Jing
Ping](https://www.linkedin.com/in/jing-ping-wong/)\n\n## Model Card
Contact\n\nContact us on X at [@defogdata](https://twitter.com/defogdata), or on
email at
[founders@defog.ai](mailto:founders@defog.ai)","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":1,"views":130,"requests":"0","sort":null,"version":"v13-Tue
Mar 12 08:15:50 GMT 2024","tags":["Code
Generation"],"playground":false,"createTime":"2024-03-13T07:47:22","updateBy":"defog","updateTime":"2024-03-13T07:47:41"},{"id":467,"versionId":3524,"owner":"meta","applicationName":"CodeLlama-7b-Instruct","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1704872459434CodeLlama.webp","description":"#
**Code Llama**\n\nCode Llama is a collection of pretrained and fine-tuned
generative text models ranging in scale from 7 billion to 34 billion parameters.
This is the repository for the 7B instruct-tuned version in the Hugging Face
Transformers format. This model is designed for general code synthesis and
understanding. Links to other models can be found in the index at the
bottom.\n\n| | Base Model | Python | Instruct
|\n|-----|---------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|\n|
7B |
[codellama/CodeLlama-7b-hf](https://huggingface.co/codellama/CodeLlama-7b-hf) |
[codellama/CodeLlama-7b-Python-hf](https://huggingface.co/codellama/CodeLlama-7b-Python-hf)
|
[codellama/CodeLlama-7b-Instruct-hf](https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf)
|\n| 13B |
[codellama/CodeLlama-13b-hf](https://huggingface.co/codellama/CodeLlama-13b-hf)
|
[codellama/CodeLlama-13b-Python-hf](https://huggingface.co/codellama/CodeLlama-13b-Python-hf)
|
[codellama/CodeLlama-13b-Instruct-hf](https://huggingface.co/codellama/CodeLlama-13b-Instruct-hf)
|\n| 34B |
[codellama/CodeLlama-34b-hf](https://huggingface.co/codellama/CodeLlama-34b-hf)
|
[codellama/CodeLlama-34b-Python-hf](https://huggingface.co/codellama/CodeLlama-34b-Python-hf)
|
[codellama/CodeLlama-34b-Instruct-hf](https://huggingface.co/codellama/CodeLlama-34b-Instruct-hf)
|\n\n## Model Use\n\nTo use this model, please make sure to install transformers
from `main` until the next version is released:\n\n```bash\npip install
git+https://github.com/huggingface/transformers.git@main
accelerate\n```\n\nModel capabilities:\n\n- [x] Code completion.\n- [x]
Infilling.\n- [x] Instructions / chat.\n- [ ] Python specialist.\n\n## Model
Details\n\n*Note: Use of this model is governed by the Meta license. Meta
developed and publicly released the Code Llama family of large language models
(LLMs).\n\n**Model Developers** Meta\n\n**Variations** Code Llama comes in three
model sizes, and three variants:\n\n* Code Llama: base models designed for
general code synthesis and understanding\n* Code Llama - Python: designed
specifically for Python\n* Code Llama - Instruct: for instruction following and
safer deployment\n\nAll variants are available in sizes of 7B, 13B and 34B
parameters.\n\n**This repository contains the Instruct version of the 7B
parameters model.**\n\n**Input** Models input text only.\n\n**Output** Models
generate text only.\n\n**Model Architecture** Code Llama is an auto-regressive
language model that uses an optimized transformer architecture.\n\n**Model
Dates** Code Llama and its variants have been trained between January 2023 and
July 2023.\n\n**Status** This is a static model trained on an offline dataset.
Future versions of Code Llama - Instruct will be released as we improve model
safety with community feedback.\n\n**License** A custom commercial license is
available at:
[https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)\n\n**Research
Paper** More information can be found in the paper \"[Code Llama: Open
Foundation Models for
Code](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/)\"
or its [arXiv page](https://arxiv.org/abs/2308.12950).\n\n## Intended
Use\n\n**Intended Use Cases** Code Llama and its variants is intended for
commercial and research use in English and relevant programming languages. The
base model Code Llama can be adapted for a variety of code synthesis and
understanding tasks, Code Llama - Python is designed specifically to handle the
Python programming language, and Code Llama - Instruct is intended to be safer
to use for code assistant and generation applications.\n\n**Out-of-Scope Uses**
Use in any manner that violates applicable laws or regulations (including trade
compliance laws). Use in languages other than English. Use in any other way that
is prohibited by the Acceptable Use Policy and Licensing Agreement for Code
Llama and its variants.\n\n## Hardware and Software\n\n**Training Factors** We
used custom training libraries. The training and fine-tuning of the released
models have been performed Meta’s Research Super Cluster.\n\n**Carbon
Footprint** In aggregate, training all 9 Code Llama models required 400K GPU
hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated
total emissions were 65.3 tCO2eq, 100% of which were offset by Meta’s
sustainability program.\n\n## Training Data\n\nAll experiments reported here and
the released models have been trained and fine-tuned using the same data as
Llama 2 with different weights (see Section 2 and Table 1 in the [research
paper](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/)
for details).\n\n## Evaluation Results\n\nSee evaluations for the main models
and detailed ablations in Section 3 and safety evaluations in Section 4 of the
research paper.\n\n## Ethical Considerations and Limitations\n\nCode Llama and
its variants are a new technology that carries risks with use. Testing conducted
to date has been in English, and has not covered, nor could it cover all
scenarios. For these reasons, as with all LLMs, Code Llama’s potential outputs
cannot be predicted in advance, and the model may in some instances produce
inaccurate or objectionable responses to user prompts. Therefore, before
deploying any applications of Code Llama, developers should perform safety
testing and tuning tailored to their specific applications of the
model.\n\nPlease see the Responsible Use Guide available available at
[https://ai.meta.com/llama/responsible-use-guide](https://ai.meta.com/llama/responsible-use-guide).","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":0,"views":56,"requests":"0","sort":null,"version":"v0-Wed
Jan 10 07:41:03 GMT 2024","tags":["Code
Generation"],"playground":false,"createTime":"2024-01-10T07:41:56","updateBy":"meta","updateTime":"2024-01-10T07:42:40"},{"id":468,"versionId":3526,"owner":"meta","applicationName":"CodeLlama-34b-Instruct","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1704872479897CodeLlama.webp","description":"#
**Code Llama**\n\nCode Llama is a collection of pretrained and fine-tuned
generative text models ranging in scale from 7 billion to 34 billion parameters.
This is the repository for the 7B instruct-tuned version in the Hugging Face
Transformers format. This model is designed for general code synthesis and
understanding. Links to other models can be found in the index at the
bottom.\n\n| | Base Model | Python | Instruct
|\n|-----|---------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|\n|
7B |
[codellama/CodeLlama-7b-hf](https://huggingface.co/codellama/CodeLlama-7b-hf) |
[codellama/CodeLlama-7b-Python-hf](https://huggingface.co/codellama/CodeLlama-7b-Python-hf)
|
[codellama/CodeLlama-7b-Instruct-hf](https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf)
|\n| 13B |
[codellama/CodeLlama-13b-hf](https://huggingface.co/codellama/CodeLlama-13b-hf)
|
[codellama/CodeLlama-13b-Python-hf](https://huggingface.co/codellama/CodeLlama-13b-Python-hf)
|
[codellama/CodeLlama-13b-Instruct-hf](https://huggingface.co/codellama/CodeLlama-13b-Instruct-hf)
|\n| 34B |
[codellama/CodeLlama-34b-hf](https://huggingface.co/codellama/CodeLlama-34b-hf)
|
[codellama/CodeLlama-34b-Python-hf](https://huggingface.co/codellama/CodeLlama-34b-Python-hf)
|
[codellama/CodeLlama-34b-Instruct-hf](https://huggingface.co/codellama/CodeLlama-34b-Instruct-hf)
|\n\n## Model Use\n\nTo use this model, please make sure to install transformers
from `main` until the next version is released:\n\n```bash\npip install
git+https://github.com/huggingface/transformers.git@main
accelerate\n```\n\nModel capabilities:\n\n- [x] Code completion.\n- [x]
Infilling.\n- [x] Instructions / chat.\n- [ ] Python specialist.\n\n## Model
Details\n\n*Note: Use of this model is governed by the Meta license. Meta
developed and publicly released the Code Llama family of large language models
(LLMs).\n\n**Model Developers** Meta\n\n**Variations** Code Llama comes in three
model sizes, and three variants:\n\n* Code Llama: base models designed for
general code synthesis and understanding\n* Code Llama - Python: designed
specifically for Python\n* Code Llama - Instruct: for instruction following and
safer deployment\n\nAll variants are available in sizes of 7B, 13B and 34B
parameters.\n\n**This repository contains the Instruct version of the 7B
parameters model.**\n\n**Input** Models input text only.\n\n**Output** Models
generate text only.\n\n**Model Architecture** Code Llama is an auto-regressive
language model that uses an optimized transformer architecture.\n\n**Model
Dates** Code Llama and its variants have been trained between January 2023 and
July 2023.\n\n**Status** This is a static model trained on an offline dataset.
Future versions of Code Llama - Instruct will be released as we improve model
safety with community feedback.\n\n**License** A custom commercial license is
available at:
[https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)\n\n**Research
Paper** More information can be found in the paper \"[Code Llama: Open
Foundation Models for
Code](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/)\"
or its [arXiv page](https://arxiv.org/abs/2308.12950).\n\n## Intended
Use\n\n**Intended Use Cases** Code Llama and its variants is intended for
commercial and research use in English and relevant programming languages. The
base model Code Llama can be adapted for a variety of code synthesis and
understanding tasks, Code Llama - Python is designed specifically to handle the
Python programming language, and Code Llama - Instruct is intended to be safer
to use for code assistant and generation applications.\n\n**Out-of-Scope Uses**
Use in any manner that violates applicable laws or regulations (including trade
compliance laws). Use in languages other than English. Use in any other way that
is prohibited by the Acceptable Use Policy and Licensing Agreement for Code
Llama and its variants.\n\n## Hardware and Software\n\n**Training Factors** We
used custom training libraries. The training and fine-tuning of the released
models have been performed Meta’s Research Super Cluster.\n\n**Carbon
Footprint** In aggregate, training all 9 Code Llama models required 400K GPU
hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated
total emissions were 65.3 tCO2eq, 100% of which were offset by Meta’s
sustainability program.\n\n## Training Data\n\nAll experiments reported here and
the released models have been trained and fine-tuned using the same data as
Llama 2 with different weights (see Section 2 and Table 1 in the [research
paper](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/)
for details).\n\n## Evaluation Results\n\nSee evaluations for the main models
and detailed ablations in Section 3 and safety evaluations in Section 4 of the
research paper.\n\n## Ethical Considerations and Limitations\n\nCode Llama and
its variants are a new technology that carries risks with use. Testing conducted
to date has been in English, and has not covered, nor could it cover all
scenarios. For these reasons, as with all LLMs, Code Llama’s potential outputs
cannot be predicted in advance, and the model may in some instances produce
inaccurate or objectionable responses to user prompts. Therefore, before
deploying any applications of Code Llama, developers should perform safety
testing and tuning tailored to their specific applications of the
model.\n\nPlease see the Responsible Use Guide available available at
[https://ai.meta.com/llama/responsible-use-guide](https://ai.meta.com/llama/responsible-use-guide).","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":1,"views":36,"requests":"0","sort":null,"version":"v0-Wed
Jan 10 07:41:43 GMT 2024","tags":["Code
Generation"],"playground":false,"createTime":"2024-01-10T07:41:58","updateBy":"meta","updateTime":"2024-01-10T07:42:42"},{"id":466,"versionId":3525,"owner":"meta","applicationName":"CodeLlama-13b-Instruct","jobType":2,"jobSubType":null,"accessPermission":0,"pictureUrl":"https://fedml.s3.us-west-1.amazonaws.com/1704872375846CodeLlama.webp","description":"#
**Code Llama**\n\nCode Llama is a collection of pretrained and fine-tuned
generative text models ranging in scale from 7 billion to 34 billion parameters.
This is the repository for the 7B instruct-tuned version in the Hugging Face
Transformers format. This model is designed for general code synthesis and
understanding. Links to other models can be found in the index at the
bottom.\n\n| | Base Model | Python | Instruct
|\n|-----|---------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------|\n|
7B |
[codellama/CodeLlama-7b-hf](https://huggingface.co/codellama/CodeLlama-7b-hf) |
[codellama/CodeLlama-7b-Python-hf](https://huggingface.co/codellama/CodeLlama-7b-Python-hf)
|
[codellama/CodeLlama-7b-Instruct-hf](https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf)
|\n| 13B |
[codellama/CodeLlama-13b-hf](https://huggingface.co/codellama/CodeLlama-13b-hf)
|
[codellama/CodeLlama-13b-Python-hf](https://huggingface.co/codellama/CodeLlama-13b-Python-hf)
|
[codellama/CodeLlama-13b-Instruct-hf](https://huggingface.co/codellama/CodeLlama-13b-Instruct-hf)
|\n| 34B |
[codellama/CodeLlama-34b-hf](https://huggingface.co/codellama/CodeLlama-34b-hf)
|
[codellama/CodeLlama-34b-Python-hf](https://huggingface.co/codellama/CodeLlama-34b-Python-hf)
|
[codellama/CodeLlama-34b-Instruct-hf](https://huggingface.co/codellama/CodeLlama-34b-Instruct-hf)
|\n\n## Model Use\n\nTo use this model, please make sure to install transformers
from `main` until the next version is released:\n\n```bash\npip install
git+https://github.com/huggingface/transformers.git@main
accelerate\n```\n\nModel capabilities:\n\n- [x] Code completion.\n- [x]
Infilling.\n- [x] Instructions / chat.\n- [ ] Python specialist.\n\n## Model
Details\n\n*Note: Use of this model is governed by the Meta license. Meta
developed and publicly released the Code Llama family of large language models
(LLMs).\n\n**Model Developers** Meta\n\n**Variations** Code Llama comes in three
model sizes, and three variants:\n\n* Code Llama: base models designed for
general code synthesis and understanding\n* Code Llama - Python: designed
specifically for Python\n* Code Llama - Instruct: for instruction following and
safer deployment\n\nAll variants are available in sizes of 7B, 13B and 34B
parameters.\n\n**This repository contains the Instruct version of the 7B
parameters model.**\n\n**Input** Models input text only.\n\n**Output** Models
generate text only.\n\n**Model Architecture** Code Llama is an auto-regressive
language model that uses an optimized transformer architecture.\n\n**Model
Dates** Code Llama and its variants have been trained between January 2023 and
July 2023.\n\n**Status** This is a static model trained on an offline dataset.
Future versions of Code Llama - Instruct will be released as we improve model
safety with community feedback.\n\n**License** A custom commercial license is
available at:
[https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)\n\n**Research
Paper** More information can be found in the paper \"[Code Llama: Open
Foundation Models for
Code](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/)\"
or its [arXiv page](https://arxiv.org/abs/2308.12950).\n\n## Intended
Use\n\n**Intended Use Cases** Code Llama and its variants is intended for
commercial and research use in English and relevant programming languages. The
base model Code Llama can be adapted for a variety of code synthesis and
understanding tasks, Code Llama - Python is designed specifically to handle the
Python programming language, and Code Llama - Instruct is intended to be safer
to use for code assistant and generation applications.\n\n**Out-of-Scope Uses**
Use in any manner that violates applicable laws or regulations (including trade
compliance laws). Use in languages other than English. Use in any other way that
is prohibited by the Acceptable Use Policy and Licensing Agreement for Code
Llama and its variants.\n\n## Hardware and Software\n\n**Training Factors** We
used custom training libraries. The training and fine-tuning of the released
models have been performed Meta’s Research Super Cluster.\n\n**Carbon
Footprint** In aggregate, training all 9 Code Llama models required 400K GPU
hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated
total emissions were 65.3 tCO2eq, 100% of which were offset by Meta’s
sustainability program.\n\n## Training Data\n\nAll experiments reported here and
the released models have been trained and fine-tuned using the same data as
Llama 2 with different weights (see Section 2 and Table 1 in the [research
paper](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/)
for details).\n\n## Evaluation Results\n\nSee evaluations for the main models
and detailed ablations in Section 3 and safety evaluations in Section 4 of the
research paper.\n\n## Ethical Considerations and Limitations\n\nCode Llama and
its variants are a new technology that carries risks with use. Testing conducted
to date has been in English, and has not covered, nor could it cover all
scenarios. For these reasons, as with all LLMs, Code Llama’s potential outputs
cannot be predicted in advance, and the model may in some instances produce
inaccurate or objectionable responses to user prompts. Therefore, before
deploying any applications of Code Llama, developers should perform safety
testing and tuning tailored to their specific applications of the
model.\n\nPlease see the Responsible Use Guide available available at
[https://ai.meta.com/llama/responsible-use-guide](https://ai.meta.com/llama/responsible-use-guide).","modelId":1,"dataId":1,"platformId":-1,"users":0,"favorites":1,"views":20,"requests":"0","sort":null,"version":"v0-Wed
Jan 10 07:40:15 GMT 2024","tags":["Code
Generation"],"playground":false,"createTime":"2024-01-10T07:41:57","updateBy":"meta","updateTime":"2024-01-10T07:42:39"}]}}
